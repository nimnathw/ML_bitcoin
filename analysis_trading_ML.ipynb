{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1exZQwZvPofP4SLF4-r93am29lMQmFVTa",
      "authorship_tag": "ABX9TyNl60lauUhfoT4/e0QqUTcJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimnathw/bitcoin-price-prediction/blob/master/analysis_trading_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqvLmQGwPuR6",
        "outputId": "236532d8-d55e-4b06-a1f5-5438fb756899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks')\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "metadata": {
        "id": "-SluWrI7RjoZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the cleaned data into a DataFrame\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df[df['date'] > '2022-01-01']\n",
        "df.set_index('date', inplace=True)\n",
        "\n",
        "\n",
        "# view data\n",
        "df.describe()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "Jq9zLMMFRrSE",
        "outputId": "88e8664b-a65c-4732-eb00-3002ed8e9e87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             ACDGNO      AWHMAN       DGS10      DGS1MO      DGS3MO  \\\n",
              "count    209.000000  209.000000  209.000000  209.000000  209.000000   \n",
              "mean   44176.349282   41.187081    2.799043    1.247464    1.653158   \n",
              "std     1751.369590    0.197765    0.675100    1.157003    1.269436   \n",
              "min    41200.000000   40.900000    1.630000    0.020000    0.080000   \n",
              "25%    43202.000000   41.000000    2.200000    0.180000    0.460000   \n",
              "50%    44667.000000   41.100000    2.860000    0.850000    1.170000   \n",
              "75%    45188.000000   41.200000    3.150000    2.240000    2.710000   \n",
              "max    46602.000000   41.600000    4.250000    3.760000    4.230000   \n",
              "\n",
              "             DGS5      DGS6MO         IC4WSA          M2SL     NASDAQ100  \\\n",
              "count  209.000000  209.000000     209.000000    209.000000    209.000000   \n",
              "mean     2.827560    2.100239  213889.952153  21618.207656  13021.991914   \n",
              "std      0.790562    1.320076   22126.938626     91.395895   1352.836783   \n",
              "min      1.370000    0.220000  170500.000000  21351.600000  10692.060000   \n",
              "25%      2.180000    0.860000  196750.000000  21607.400000  11875.630000   \n",
              "50%      2.910000    1.640000  215750.000000  21636.100000  12881.790000   \n",
              "75%      3.250000    3.150000  231750.000000  21649.600000  14149.120000   \n",
              "max      4.450000    4.580000  249500.000000  21739.700000  16501.770000   \n",
              "\n",
              "       PCUADLVWRADLVWR       PERMIT     RETAILIMSA        SP500      T10YFF  \\\n",
              "count       209.000000   209.000000     209.000000   209.000000  209.000000   \n",
              "mean        189.768967  1707.662201  711141.215311  4136.572488    1.551627   \n",
              "std           2.833958   131.002072   27618.289319   302.386048    0.623636   \n",
              "min         185.559000  1512.000000  660783.000000  3577.030000    0.270000   \n",
              "25%         186.557000  1564.000000  692095.000000  3900.110000    1.000000   \n",
              "50%         189.901000  1695.000000  723116.000000  4131.930000    1.680000   \n",
              "75%         192.286000  1841.000000  738147.000000  4392.590000    2.020000   \n",
              "max         194.187000  1879.000000  741260.000000  4796.560000    2.660000   \n",
              "\n",
              "          UMCSENT   market_caps        prices  total_volumes  \n",
              "count  209.000000  2.090000e+02    209.000000   2.090000e+02  \n",
              "mean    59.032536  5.811936e+11  30534.719520   2.893393e+10  \n",
              "std      5.030722  1.859799e+11   9871.561187   1.061728e+10  \n",
              "min     50.000000  3.551945e+11  18539.635238   1.180719e+10  \n",
              "25%     58.200000  3.931043e+11  20574.840592   2.207497e+10  \n",
              "50%     58.600000  5.648852e+11  29655.026132   2.685591e+10  \n",
              "75%     62.800000  7.700967e+11  40488.877918   3.213122e+10  \n",
              "max     67.200000  9.014300e+11  47459.261238   6.598570e+10  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-335d834b-8536-4cdc-b32c-58f631d144ce\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ACDGNO</th>\n",
              "      <th>AWHMAN</th>\n",
              "      <th>DGS10</th>\n",
              "      <th>DGS1MO</th>\n",
              "      <th>DGS3MO</th>\n",
              "      <th>DGS5</th>\n",
              "      <th>DGS6MO</th>\n",
              "      <th>IC4WSA</th>\n",
              "      <th>M2SL</th>\n",
              "      <th>NASDAQ100</th>\n",
              "      <th>PCUADLVWRADLVWR</th>\n",
              "      <th>PERMIT</th>\n",
              "      <th>RETAILIMSA</th>\n",
              "      <th>SP500</th>\n",
              "      <th>T10YFF</th>\n",
              "      <th>UMCSENT</th>\n",
              "      <th>market_caps</th>\n",
              "      <th>prices</th>\n",
              "      <th>total_volumes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>2.090000e+02</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>2.090000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>44176.349282</td>\n",
              "      <td>41.187081</td>\n",
              "      <td>2.799043</td>\n",
              "      <td>1.247464</td>\n",
              "      <td>1.653158</td>\n",
              "      <td>2.827560</td>\n",
              "      <td>2.100239</td>\n",
              "      <td>213889.952153</td>\n",
              "      <td>21618.207656</td>\n",
              "      <td>13021.991914</td>\n",
              "      <td>189.768967</td>\n",
              "      <td>1707.662201</td>\n",
              "      <td>711141.215311</td>\n",
              "      <td>4136.572488</td>\n",
              "      <td>1.551627</td>\n",
              "      <td>59.032536</td>\n",
              "      <td>5.811936e+11</td>\n",
              "      <td>30534.719520</td>\n",
              "      <td>2.893393e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1751.369590</td>\n",
              "      <td>0.197765</td>\n",
              "      <td>0.675100</td>\n",
              "      <td>1.157003</td>\n",
              "      <td>1.269436</td>\n",
              "      <td>0.790562</td>\n",
              "      <td>1.320076</td>\n",
              "      <td>22126.938626</td>\n",
              "      <td>91.395895</td>\n",
              "      <td>1352.836783</td>\n",
              "      <td>2.833958</td>\n",
              "      <td>131.002072</td>\n",
              "      <td>27618.289319</td>\n",
              "      <td>302.386048</td>\n",
              "      <td>0.623636</td>\n",
              "      <td>5.030722</td>\n",
              "      <td>1.859799e+11</td>\n",
              "      <td>9871.561187</td>\n",
              "      <td>1.061728e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>41200.000000</td>\n",
              "      <td>40.900000</td>\n",
              "      <td>1.630000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>1.370000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>170500.000000</td>\n",
              "      <td>21351.600000</td>\n",
              "      <td>10692.060000</td>\n",
              "      <td>185.559000</td>\n",
              "      <td>1512.000000</td>\n",
              "      <td>660783.000000</td>\n",
              "      <td>3577.030000</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>3.551945e+11</td>\n",
              "      <td>18539.635238</td>\n",
              "      <td>1.180719e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>43202.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>2.200000</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>2.180000</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>196750.000000</td>\n",
              "      <td>21607.400000</td>\n",
              "      <td>11875.630000</td>\n",
              "      <td>186.557000</td>\n",
              "      <td>1564.000000</td>\n",
              "      <td>692095.000000</td>\n",
              "      <td>3900.110000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>58.200000</td>\n",
              "      <td>3.931043e+11</td>\n",
              "      <td>20574.840592</td>\n",
              "      <td>2.207497e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>44667.000000</td>\n",
              "      <td>41.100000</td>\n",
              "      <td>2.860000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>1.170000</td>\n",
              "      <td>2.910000</td>\n",
              "      <td>1.640000</td>\n",
              "      <td>215750.000000</td>\n",
              "      <td>21636.100000</td>\n",
              "      <td>12881.790000</td>\n",
              "      <td>189.901000</td>\n",
              "      <td>1695.000000</td>\n",
              "      <td>723116.000000</td>\n",
              "      <td>4131.930000</td>\n",
              "      <td>1.680000</td>\n",
              "      <td>58.600000</td>\n",
              "      <td>5.648852e+11</td>\n",
              "      <td>29655.026132</td>\n",
              "      <td>2.685591e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>45188.000000</td>\n",
              "      <td>41.200000</td>\n",
              "      <td>3.150000</td>\n",
              "      <td>2.240000</td>\n",
              "      <td>2.710000</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>3.150000</td>\n",
              "      <td>231750.000000</td>\n",
              "      <td>21649.600000</td>\n",
              "      <td>14149.120000</td>\n",
              "      <td>192.286000</td>\n",
              "      <td>1841.000000</td>\n",
              "      <td>738147.000000</td>\n",
              "      <td>4392.590000</td>\n",
              "      <td>2.020000</td>\n",
              "      <td>62.800000</td>\n",
              "      <td>7.700967e+11</td>\n",
              "      <td>40488.877918</td>\n",
              "      <td>3.213122e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>46602.000000</td>\n",
              "      <td>41.600000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>3.760000</td>\n",
              "      <td>4.230000</td>\n",
              "      <td>4.450000</td>\n",
              "      <td>4.580000</td>\n",
              "      <td>249500.000000</td>\n",
              "      <td>21739.700000</td>\n",
              "      <td>16501.770000</td>\n",
              "      <td>194.187000</td>\n",
              "      <td>1879.000000</td>\n",
              "      <td>741260.000000</td>\n",
              "      <td>4796.560000</td>\n",
              "      <td>2.660000</td>\n",
              "      <td>67.200000</td>\n",
              "      <td>9.014300e+11</td>\n",
              "      <td>47459.261238</td>\n",
              "      <td>6.598570e+10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-335d834b-8536-4cdc-b32c-58f631d144ce')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-335d834b-8536-4cdc-b32c-58f631d144ce button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-335d834b-8536-4cdc-b32c-58f631d144ce');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of lags\n",
        "n_lags = 7\n",
        "\n",
        "# Add lagged values of the dependent variable\n",
        "for i in range(1, n_lags+1):\n",
        "    df[f'prices_lag{i}'] = df['prices'].shift(i)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "R5DO0SN_48Re",
        "outputId": "2d166a0c-9e6f-4395-e1ba-72cdc307536d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             ACDGNO  AWHMAN  DGS10  DGS1MO  DGS3MO  DGS5  DGS6MO    IC4WSA  \\\n",
              "date                                                                         \n",
              "2022-02-01  41497.0    41.6   1.81    0.04    0.19  1.63    0.48  228500.0   \n",
              "2022-02-02  41497.0    41.6   1.78    0.04    0.19  1.60    0.45  228500.0   \n",
              "2022-02-03  41497.0    41.6   1.82    0.03    0.20  1.66    0.48  228500.0   \n",
              "2022-02-04  41497.0    41.6   1.93    0.05    0.23  1.78    0.56  228500.0   \n",
              "2022-02-07  41497.0    41.6   1.92    0.03    0.27  1.76    0.58  216750.0   \n",
              "\n",
              "               M2SL  NASDAQ100  ...   prices_lag1   prices_lag2   prices_lag3  \\\n",
              "date                            ...                                             \n",
              "2022-02-01  21708.4   15019.68  ...  37983.151499  37276.839558  36870.440167   \n",
              "2022-02-02  21708.4   15139.74  ...  38555.534461  37983.151499  37276.839558   \n",
              "2022-02-03  21708.4   14501.11  ...  38835.694943  38555.534461  37983.151499   \n",
              "2022-02-04  21708.4   14694.35  ...  37000.982499  38835.694943  38555.534461   \n",
              "2022-02-07  21708.4   14571.25  ...  37101.351594  37000.982499  38835.694943   \n",
              "\n",
              "             prices_lag4   prices_lag5   prices_lag6   prices_lag7  \\\n",
              "date                                                                 \n",
              "2022-02-01  36988.928511  36774.007142  36306.409440  40707.682414   \n",
              "2022-02-02  36870.440167  36988.928511  36774.007142  36306.409440   \n",
              "2022-02-03  37276.839558  36870.440167  36988.928511  36774.007142   \n",
              "2022-02-04  37983.151499  37276.839558  36870.440167  36988.928511   \n",
              "2022-02-07  38555.534461  37983.151499  37276.839558  36870.440167   \n",
              "\n",
              "             prices_lag8   prices_lag9  prices_lag10  \n",
              "date                                                  \n",
              "2022-02-01  41749.551431  42395.458792  42298.341117  \n",
              "2022-02-02  40707.682414  41749.551431  42395.458792  \n",
              "2022-02-03  36306.409440  40707.682414  41749.551431  \n",
              "2022-02-04  36774.007142  36306.409440  40707.682414  \n",
              "2022-02-07  36988.928511  36774.007142  36306.409440  \n",
              "\n",
              "[5 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85de0b2e-fcd4-41e9-8cfd-8ae8adbdf986\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ACDGNO</th>\n",
              "      <th>AWHMAN</th>\n",
              "      <th>DGS10</th>\n",
              "      <th>DGS1MO</th>\n",
              "      <th>DGS3MO</th>\n",
              "      <th>DGS5</th>\n",
              "      <th>DGS6MO</th>\n",
              "      <th>IC4WSA</th>\n",
              "      <th>M2SL</th>\n",
              "      <th>NASDAQ100</th>\n",
              "      <th>...</th>\n",
              "      <th>prices_lag1</th>\n",
              "      <th>prices_lag2</th>\n",
              "      <th>prices_lag3</th>\n",
              "      <th>prices_lag4</th>\n",
              "      <th>prices_lag5</th>\n",
              "      <th>prices_lag6</th>\n",
              "      <th>prices_lag7</th>\n",
              "      <th>prices_lag8</th>\n",
              "      <th>prices_lag9</th>\n",
              "      <th>prices_lag10</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2022-02-01</th>\n",
              "      <td>41497.0</td>\n",
              "      <td>41.6</td>\n",
              "      <td>1.81</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.63</td>\n",
              "      <td>0.48</td>\n",
              "      <td>228500.0</td>\n",
              "      <td>21708.4</td>\n",
              "      <td>15019.68</td>\n",
              "      <td>...</td>\n",
              "      <td>37983.151499</td>\n",
              "      <td>37276.839558</td>\n",
              "      <td>36870.440167</td>\n",
              "      <td>36988.928511</td>\n",
              "      <td>36774.007142</td>\n",
              "      <td>36306.409440</td>\n",
              "      <td>40707.682414</td>\n",
              "      <td>41749.551431</td>\n",
              "      <td>42395.458792</td>\n",
              "      <td>42298.341117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-02</th>\n",
              "      <td>41497.0</td>\n",
              "      <td>41.6</td>\n",
              "      <td>1.78</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.45</td>\n",
              "      <td>228500.0</td>\n",
              "      <td>21708.4</td>\n",
              "      <td>15139.74</td>\n",
              "      <td>...</td>\n",
              "      <td>38555.534461</td>\n",
              "      <td>37983.151499</td>\n",
              "      <td>37276.839558</td>\n",
              "      <td>36870.440167</td>\n",
              "      <td>36988.928511</td>\n",
              "      <td>36774.007142</td>\n",
              "      <td>36306.409440</td>\n",
              "      <td>40707.682414</td>\n",
              "      <td>41749.551431</td>\n",
              "      <td>42395.458792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-03</th>\n",
              "      <td>41497.0</td>\n",
              "      <td>41.6</td>\n",
              "      <td>1.82</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.66</td>\n",
              "      <td>0.48</td>\n",
              "      <td>228500.0</td>\n",
              "      <td>21708.4</td>\n",
              "      <td>14501.11</td>\n",
              "      <td>...</td>\n",
              "      <td>38835.694943</td>\n",
              "      <td>38555.534461</td>\n",
              "      <td>37983.151499</td>\n",
              "      <td>37276.839558</td>\n",
              "      <td>36870.440167</td>\n",
              "      <td>36988.928511</td>\n",
              "      <td>36774.007142</td>\n",
              "      <td>36306.409440</td>\n",
              "      <td>40707.682414</td>\n",
              "      <td>41749.551431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-04</th>\n",
              "      <td>41497.0</td>\n",
              "      <td>41.6</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.23</td>\n",
              "      <td>1.78</td>\n",
              "      <td>0.56</td>\n",
              "      <td>228500.0</td>\n",
              "      <td>21708.4</td>\n",
              "      <td>14694.35</td>\n",
              "      <td>...</td>\n",
              "      <td>37000.982499</td>\n",
              "      <td>38835.694943</td>\n",
              "      <td>38555.534461</td>\n",
              "      <td>37983.151499</td>\n",
              "      <td>37276.839558</td>\n",
              "      <td>36870.440167</td>\n",
              "      <td>36988.928511</td>\n",
              "      <td>36774.007142</td>\n",
              "      <td>36306.409440</td>\n",
              "      <td>40707.682414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-07</th>\n",
              "      <td>41497.0</td>\n",
              "      <td>41.6</td>\n",
              "      <td>1.92</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.27</td>\n",
              "      <td>1.76</td>\n",
              "      <td>0.58</td>\n",
              "      <td>216750.0</td>\n",
              "      <td>21708.4</td>\n",
              "      <td>14571.25</td>\n",
              "      <td>...</td>\n",
              "      <td>37101.351594</td>\n",
              "      <td>37000.982499</td>\n",
              "      <td>38835.694943</td>\n",
              "      <td>38555.534461</td>\n",
              "      <td>37983.151499</td>\n",
              "      <td>37276.839558</td>\n",
              "      <td>36870.440167</td>\n",
              "      <td>36988.928511</td>\n",
              "      <td>36774.007142</td>\n",
              "      <td>36306.409440</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85de0b2e-fcd4-41e9-8cfd-8ae8adbdf986')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-85de0b2e-fcd4-41e9-8cfd-8ae8adbdf986 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-85de0b2e-fcd4-41e9-8cfd-8ae8adbdf986');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "\n",
        "# Get the dependent variable 'prices'\n",
        "y = df['prices']\n",
        "\n",
        "# Get the independent variables\n",
        "X = df.drop(columns=['prices']) \n",
        "\n",
        "# Define the number of splits\n",
        "n_splits = 3\n",
        "\n",
        "# Create the TimeSeriesSplit object\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "for train_index, test_index in tscv.split(df):\n",
        "    X_train, X_test = df.iloc[train_index, :], df.iloc[test_index, :]\n",
        "    y_train, y_test = df.iloc[train_index, -1], df.iloc[test_index, -1]\n",
        "\n",
        "# Split the data into training and test sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "8vKenvPmSLI4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "# Add first LSTM layer with 64 units and input shape defined as the number of features in the X_train dataset\n",
        "model.add(LSTM(units=64, input_shape=(X_train.shape[1],1), return_sequences=True))\n",
        "# Add a second LSTM layer with 64 units\n",
        "model.add(LSTM(units=64))\n",
        "# Add a dropout layer to help prevent overfitting\n",
        "model.add(Dropout(0.2))\n",
        "# Add a fully connected Dense layer with 64 units\n",
        "model.add(Dense(64))\n",
        "# Add fully connected Dense layer with one unit to produce the output\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Define early stopping and checkpoint callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "# Train the model on the training data\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, batch_size=32, callbacks=[early_stopping, checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU_E030gSTM9",
        "outputId": "382410b8-5a20-4494-d53e-50ef76cfac99"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "4/4 [==============================] - 5s 271ms/step - loss: 1398607360.0000 - val_loss: 479842720.0000\n",
            "Epoch 2/1000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 1398572544.0000 - val_loss: 479857920.0000\n",
            "Epoch 3/1000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 1398499072.0000 - val_loss: 479851712.0000\n",
            "Epoch 4/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1398326272.0000 - val_loss: 479616672.0000\n",
            "Epoch 5/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1397971328.0000 - val_loss: 479272480.0000\n",
            "Epoch 6/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1397546752.0000 - val_loss: 479016672.0000\n",
            "Epoch 7/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1397122816.0000 - val_loss: 478781664.0000\n",
            "Epoch 8/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1396728192.0000 - val_loss: 478548032.0000\n",
            "Epoch 9/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1396361472.0000 - val_loss: 478312320.0000\n",
            "Epoch 10/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1395943296.0000 - val_loss: 478072960.0000\n",
            "Epoch 11/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1395534336.0000 - val_loss: 477827456.0000\n",
            "Epoch 12/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1395125120.0000 - val_loss: 477573568.0000\n",
            "Epoch 13/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1394678144.0000 - val_loss: 477310208.0000\n",
            "Epoch 14/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1394233600.0000 - val_loss: 477037120.0000\n",
            "Epoch 15/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1393745536.0000 - val_loss: 476752704.0000\n",
            "Epoch 16/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1393266176.0000 - val_loss: 476455328.0000\n",
            "Epoch 17/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1392772224.0000 - val_loss: 476147360.0000\n",
            "Epoch 18/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1392183040.0000 - val_loss: 475825888.0000\n",
            "Epoch 19/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1391714560.0000 - val_loss: 475492544.0000\n",
            "Epoch 20/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1391137920.0000 - val_loss: 475147584.0000\n",
            "Epoch 21/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1390612480.0000 - val_loss: 474790240.0000\n",
            "Epoch 22/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1389870080.0000 - val_loss: 474418368.0000\n",
            "Epoch 23/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1389364992.0000 - val_loss: 474034432.0000\n",
            "Epoch 24/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1388657280.0000 - val_loss: 473637504.0000\n",
            "Epoch 25/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1387980544.0000 - val_loss: 473226912.0000\n",
            "Epoch 26/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 1387342208.0000 - val_loss: 472801696.0000\n",
            "Epoch 27/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1386525184.0000 - val_loss: 472364064.0000\n",
            "Epoch 28/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1385895040.0000 - val_loss: 471913664.0000\n",
            "Epoch 29/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1385043456.0000 - val_loss: 471449120.0000\n",
            "Epoch 30/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1384253568.0000 - val_loss: 470971328.0000\n",
            "Epoch 31/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1383417344.0000 - val_loss: 470479200.0000\n",
            "Epoch 32/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1382588672.0000 - val_loss: 469974784.0000\n",
            "Epoch 33/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1381764608.0000 - val_loss: 469453920.0000\n",
            "Epoch 34/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1380943872.0000 - val_loss: 468920320.0000\n",
            "Epoch 35/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1379926784.0000 - val_loss: 468371840.0000\n",
            "Epoch 36/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1379243264.0000 - val_loss: 467812928.0000\n",
            "Epoch 37/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1377988608.0000 - val_loss: 467240576.0000\n",
            "Epoch 38/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1377129344.0000 - val_loss: 466653440.0000\n",
            "Epoch 39/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 1376229760.0000 - val_loss: 466052672.0000\n",
            "Epoch 40/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 1375196800.0000 - val_loss: 465437984.0000\n",
            "Epoch 41/1000\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 1373939584.0000 - val_loss: 464807584.0000\n",
            "Epoch 42/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 1373056000.0000 - val_loss: 464165504.0000\n",
            "Epoch 43/1000\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 1372155904.0000 - val_loss: 463512224.0000\n",
            "Epoch 44/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 1370704640.0000 - val_loss: 462844352.0000\n",
            "Epoch 45/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 1369596416.0000 - val_loss: 462161120.0000\n",
            "Epoch 46/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 1368407936.0000 - val_loss: 461459040.0000\n",
            "Epoch 47/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 1367182208.0000 - val_loss: 460747712.0000\n",
            "Epoch 48/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 1366265728.0000 - val_loss: 460025312.0000\n",
            "Epoch 49/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 1365050624.0000 - val_loss: 459289216.0000\n",
            "Epoch 50/1000\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 1363209984.0000 - val_loss: 458538528.0000\n",
            "Epoch 51/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 1362659456.0000 - val_loss: 457771072.0000\n",
            "Epoch 52/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 1360955136.0000 - val_loss: 456989088.0000\n",
            "Epoch 53/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 1359736320.0000 - val_loss: 456194816.0000\n",
            "Epoch 54/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 1358602880.0000 - val_loss: 455388288.0000\n",
            "Epoch 55/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 1356501376.0000 - val_loss: 454562848.0000\n",
            "Epoch 56/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 1355365120.0000 - val_loss: 453723328.0000\n",
            "Epoch 57/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 1354124928.0000 - val_loss: 452874720.0000\n",
            "Epoch 58/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 1352737152.0000 - val_loss: 452011936.0000\n",
            "Epoch 59/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 1351317632.0000 - val_loss: 451134208.0000\n",
            "Epoch 60/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 1350104960.0000 - val_loss: 450246784.0000\n",
            "Epoch 61/1000\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 1347904128.0000 - val_loss: 449345728.0000\n",
            "Epoch 62/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 1346656256.0000 - val_loss: 448430208.0000\n",
            "Epoch 63/1000\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 1344617344.0000 - val_loss: 447503232.0000\n",
            "Epoch 64/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1343701632.0000 - val_loss: 446564448.0000\n",
            "Epoch 65/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1341541504.0000 - val_loss: 445611136.0000\n",
            "Epoch 66/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 1339433728.0000 - val_loss: 444642912.0000\n",
            "Epoch 67/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1338754176.0000 - val_loss: 443662432.0000\n",
            "Epoch 68/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1336789760.0000 - val_loss: 442671776.0000\n",
            "Epoch 69/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1335086976.0000 - val_loss: 441663616.0000\n",
            "Epoch 70/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1332704896.0000 - val_loss: 440639904.0000\n",
            "Epoch 71/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1332237952.0000 - val_loss: 439611360.0000\n",
            "Epoch 72/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1329385728.0000 - val_loss: 438566816.0000\n",
            "Epoch 73/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1328870272.0000 - val_loss: 437510976.0000\n",
            "Epoch 74/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1326823296.0000 - val_loss: 436449024.0000\n",
            "Epoch 75/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1324773504.0000 - val_loss: 435375552.0000\n",
            "Epoch 76/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1322651648.0000 - val_loss: 434291328.0000\n",
            "Epoch 77/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1321390336.0000 - val_loss: 433193344.0000\n",
            "Epoch 78/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1317984256.0000 - val_loss: 432078560.0000\n",
            "Epoch 79/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1315569024.0000 - val_loss: 430946912.0000\n",
            "Epoch 80/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1315329664.0000 - val_loss: 429804160.0000\n",
            "Epoch 81/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1313225216.0000 - val_loss: 428649856.0000\n",
            "Epoch 82/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1311079296.0000 - val_loss: 427490208.0000\n",
            "Epoch 83/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 1309416192.0000 - val_loss: 426316672.0000\n",
            "Epoch 84/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1306353024.0000 - val_loss: 425127616.0000\n",
            "Epoch 85/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1304725248.0000 - val_loss: 423924672.0000\n",
            "Epoch 86/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1302683264.0000 - val_loss: 422710592.0000\n",
            "Epoch 87/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1301347712.0000 - val_loss: 421489216.0000\n",
            "Epoch 88/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 1298827776.0000 - val_loss: 420258208.0000\n",
            "Epoch 89/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1297555968.0000 - val_loss: 419018336.0000\n",
            "Epoch 90/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1294980352.0000 - val_loss: 417768640.0000\n",
            "Epoch 91/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 1293149568.0000 - val_loss: 416509632.0000\n",
            "Epoch 92/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1289981184.0000 - val_loss: 415242816.0000\n",
            "Epoch 93/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1287103104.0000 - val_loss: 413963296.0000\n",
            "Epoch 94/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1284800640.0000 - val_loss: 412671520.0000\n",
            "Epoch 95/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 1284168320.0000 - val_loss: 411366688.0000\n",
            "Epoch 96/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 1281002624.0000 - val_loss: 410052704.0000\n",
            "Epoch 97/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1277826176.0000 - val_loss: 408724256.0000\n",
            "Epoch 98/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1276194688.0000 - val_loss: 407382208.0000\n",
            "Epoch 99/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1274418688.0000 - val_loss: 406030752.0000\n",
            "Epoch 100/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 1270714624.0000 - val_loss: 404669184.0000\n",
            "Epoch 101/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1269395456.0000 - val_loss: 403296800.0000\n",
            "Epoch 102/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1267191680.0000 - val_loss: 401914432.0000\n",
            "Epoch 103/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1263617408.0000 - val_loss: 400511808.0000\n",
            "Epoch 104/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1262514688.0000 - val_loss: 399106656.0000\n",
            "Epoch 105/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1261245056.0000 - val_loss: 397697856.0000\n",
            "Epoch 106/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1257709440.0000 - val_loss: 396279680.0000\n",
            "Epoch 107/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1256055168.0000 - val_loss: 394862240.0000\n",
            "Epoch 108/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1252012160.0000 - val_loss: 393433824.0000\n",
            "Epoch 109/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1249594880.0000 - val_loss: 391989664.0000\n",
            "Epoch 110/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1247611008.0000 - val_loss: 390537120.0000\n",
            "Epoch 111/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1243675520.0000 - val_loss: 389073024.0000\n",
            "Epoch 112/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1240750720.0000 - val_loss: 387595712.0000\n",
            "Epoch 113/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1237921152.0000 - val_loss: 386114592.0000\n",
            "Epoch 114/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1238832128.0000 - val_loss: 384621376.0000\n",
            "Epoch 115/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1233753600.0000 - val_loss: 383117440.0000\n",
            "Epoch 116/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1233033856.0000 - val_loss: 381608544.0000\n",
            "Epoch 117/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1230765440.0000 - val_loss: 380099456.0000\n",
            "Epoch 118/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1226893184.0000 - val_loss: 378584352.0000\n",
            "Epoch 119/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1222851968.0000 - val_loss: 377052704.0000\n",
            "Epoch 120/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1220750848.0000 - val_loss: 375516288.0000\n",
            "Epoch 121/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1218455168.0000 - val_loss: 373968768.0000\n",
            "Epoch 122/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1214149632.0000 - val_loss: 372412352.0000\n",
            "Epoch 123/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1210621568.0000 - val_loss: 370842720.0000\n",
            "Epoch 124/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1211810304.0000 - val_loss: 369265568.0000\n",
            "Epoch 125/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1206043520.0000 - val_loss: 367687232.0000\n",
            "Epoch 126/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1203535744.0000 - val_loss: 366096288.0000\n",
            "Epoch 127/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1200382848.0000 - val_loss: 364496992.0000\n",
            "Epoch 128/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1198128896.0000 - val_loss: 362893888.0000\n",
            "Epoch 129/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1196092544.0000 - val_loss: 361284928.0000\n",
            "Epoch 130/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1191913728.0000 - val_loss: 359671936.0000\n",
            "Epoch 131/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1191138816.0000 - val_loss: 358053536.0000\n",
            "Epoch 132/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1189630080.0000 - val_loss: 356433504.0000\n",
            "Epoch 133/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1185939584.0000 - val_loss: 354810208.0000\n",
            "Epoch 134/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1182572544.0000 - val_loss: 353176992.0000\n",
            "Epoch 135/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1181041280.0000 - val_loss: 351546400.0000\n",
            "Epoch 136/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1174896128.0000 - val_loss: 349904320.0000\n",
            "Epoch 137/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 1174207488.0000 - val_loss: 348252640.0000\n",
            "Epoch 138/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1168918528.0000 - val_loss: 346602144.0000\n",
            "Epoch 139/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1166568832.0000 - val_loss: 344938528.0000\n",
            "Epoch 140/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1164477312.0000 - val_loss: 343273056.0000\n",
            "Epoch 141/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1161377024.0000 - val_loss: 341604448.0000\n",
            "Epoch 142/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1158272896.0000 - val_loss: 339922240.0000\n",
            "Epoch 143/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1151225984.0000 - val_loss: 338226112.0000\n",
            "Epoch 144/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1150512896.0000 - val_loss: 336528736.0000\n",
            "Epoch 145/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1149858688.0000 - val_loss: 334820320.0000\n",
            "Epoch 146/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 1149523584.0000 - val_loss: 333110400.0000\n",
            "Epoch 147/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 1142398208.0000 - val_loss: 331400896.0000\n",
            "Epoch 148/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 1138519680.0000 - val_loss: 329686272.0000\n",
            "Epoch 149/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 1134039552.0000 - val_loss: 327957600.0000\n",
            "Epoch 150/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 1132978944.0000 - val_loss: 326220768.0000\n",
            "Epoch 151/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 1130915328.0000 - val_loss: 324488512.0000\n",
            "Epoch 152/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 1126382720.0000 - val_loss: 322748768.0000\n",
            "Epoch 153/1000\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 1123263232.0000 - val_loss: 321011968.0000\n",
            "Epoch 154/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 1118685056.0000 - val_loss: 319262432.0000\n",
            "Epoch 155/1000\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 1116210816.0000 - val_loss: 317509632.0000\n",
            "Epoch 156/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 1113535616.0000 - val_loss: 315761632.0000\n",
            "Epoch 157/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 1110796288.0000 - val_loss: 314012576.0000\n",
            "Epoch 158/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 1108839808.0000 - val_loss: 312262432.0000\n",
            "Epoch 159/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 1104861184.0000 - val_loss: 310502016.0000\n",
            "Epoch 160/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 1102775552.0000 - val_loss: 308740384.0000\n",
            "Epoch 161/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 1096279680.0000 - val_loss: 306975136.0000\n",
            "Epoch 162/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 1094295936.0000 - val_loss: 305201088.0000\n",
            "Epoch 163/1000\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 1090544384.0000 - val_loss: 303425824.0000\n",
            "Epoch 164/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 1086719360.0000 - val_loss: 301644064.0000\n",
            "Epoch 165/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 1083263488.0000 - val_loss: 299866016.0000\n",
            "Epoch 166/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 1079646336.0000 - val_loss: 298087328.0000\n",
            "Epoch 167/1000\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 1074479232.0000 - val_loss: 296281696.0000\n",
            "Epoch 168/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 1077138432.0000 - val_loss: 294493440.0000\n",
            "Epoch 169/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 1072209856.0000 - val_loss: 292694848.0000\n",
            "Epoch 170/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 1067959808.0000 - val_loss: 290902112.0000\n",
            "Epoch 171/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 1064259840.0000 - val_loss: 289105504.0000\n",
            "Epoch 172/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 1059272576.0000 - val_loss: 287304224.0000\n",
            "Epoch 173/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 1059097600.0000 - val_loss: 285493952.0000\n",
            "Epoch 174/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1057284480.0000 - val_loss: 283694496.0000\n",
            "Epoch 175/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1048714240.0000 - val_loss: 281886816.0000\n",
            "Epoch 176/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1047166592.0000 - val_loss: 280077280.0000\n",
            "Epoch 177/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1045017600.0000 - val_loss: 278274784.0000\n",
            "Epoch 178/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1039826944.0000 - val_loss: 276461760.0000\n",
            "Epoch 179/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1038334976.0000 - val_loss: 274654880.0000\n",
            "Epoch 180/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1030142400.0000 - val_loss: 272836064.0000\n",
            "Epoch 181/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1033229696.0000 - val_loss: 271021760.0000\n",
            "Epoch 182/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1026704384.0000 - val_loss: 269206880.0000\n",
            "Epoch 183/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1020941312.0000 - val_loss: 267389312.0000\n",
            "Epoch 184/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1016202944.0000 - val_loss: 265568752.0000\n",
            "Epoch 185/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1015231616.0000 - val_loss: 263743216.0000\n",
            "Epoch 186/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1013102400.0000 - val_loss: 261919280.0000\n",
            "Epoch 187/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1011542656.0000 - val_loss: 260098800.0000\n",
            "Epoch 188/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1004014720.0000 - val_loss: 258286768.0000\n",
            "Epoch 189/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1003033856.0000 - val_loss: 256470192.0000\n",
            "Epoch 190/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1000563328.0000 - val_loss: 254656448.0000\n",
            "Epoch 191/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 996011264.0000 - val_loss: 252845616.0000\n",
            "Epoch 192/1000\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 990783104.0000 - val_loss: 251025520.0000\n",
            "Epoch 193/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 989980736.0000 - val_loss: 249204848.0000\n",
            "Epoch 194/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 980997248.0000 - val_loss: 247377808.0000\n",
            "Epoch 195/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 979946176.0000 - val_loss: 245553472.0000\n",
            "Epoch 196/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 977104960.0000 - val_loss: 243727888.0000\n",
            "Epoch 197/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 974670912.0000 - val_loss: 241903920.0000\n",
            "Epoch 198/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 971441600.0000 - val_loss: 240089584.0000\n",
            "Epoch 199/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 965813440.0000 - val_loss: 238275472.0000\n",
            "Epoch 200/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 964434368.0000 - val_loss: 236457696.0000\n",
            "Epoch 201/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 962357440.0000 - val_loss: 234644016.0000\n",
            "Epoch 202/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 958337152.0000 - val_loss: 232818752.0000\n",
            "Epoch 203/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 950510272.0000 - val_loss: 230999824.0000\n",
            "Epoch 204/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 947789760.0000 - val_loss: 229181968.0000\n",
            "Epoch 205/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 943365376.0000 - val_loss: 227365856.0000\n",
            "Epoch 206/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 942313920.0000 - val_loss: 225548816.0000\n",
            "Epoch 207/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 939831680.0000 - val_loss: 223743616.0000\n",
            "Epoch 208/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 935203072.0000 - val_loss: 221938016.0000\n",
            "Epoch 209/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 932759360.0000 - val_loss: 220134016.0000\n",
            "Epoch 210/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 930182400.0000 - val_loss: 218338160.0000\n",
            "Epoch 211/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 924668800.0000 - val_loss: 216543360.0000\n",
            "Epoch 212/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 917186112.0000 - val_loss: 214755856.0000\n",
            "Epoch 213/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 918809792.0000 - val_loss: 212970560.0000\n",
            "Epoch 214/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 919277952.0000 - val_loss: 211186000.0000\n",
            "Epoch 215/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 908069696.0000 - val_loss: 209391904.0000\n",
            "Epoch 216/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 910438720.0000 - val_loss: 207610672.0000\n",
            "Epoch 217/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 899692736.0000 - val_loss: 205824496.0000\n",
            "Epoch 218/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 896991360.0000 - val_loss: 204039984.0000\n",
            "Epoch 219/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 895307840.0000 - val_loss: 202252992.0000\n",
            "Epoch 220/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 891961792.0000 - val_loss: 200473184.0000\n",
            "Epoch 221/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 884088896.0000 - val_loss: 198701840.0000\n",
            "Epoch 222/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 888477376.0000 - val_loss: 196935264.0000\n",
            "Epoch 223/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 879182848.0000 - val_loss: 195169184.0000\n",
            "Epoch 224/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 881460480.0000 - val_loss: 193399712.0000\n",
            "Epoch 225/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 875500416.0000 - val_loss: 191639440.0000\n",
            "Epoch 226/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 867711296.0000 - val_loss: 189876096.0000\n",
            "Epoch 227/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 864369024.0000 - val_loss: 188129424.0000\n",
            "Epoch 228/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 866025920.0000 - val_loss: 186378416.0000\n",
            "Epoch 229/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 854552704.0000 - val_loss: 184631600.0000\n",
            "Epoch 230/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 851625344.0000 - val_loss: 182884512.0000\n",
            "Epoch 231/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 849938560.0000 - val_loss: 181148624.0000\n",
            "Epoch 232/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 843876800.0000 - val_loss: 179430784.0000\n",
            "Epoch 233/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 844604160.0000 - val_loss: 177703424.0000\n",
            "Epoch 234/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 836270336.0000 - val_loss: 175972000.0000\n",
            "Epoch 235/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 837140032.0000 - val_loss: 174245824.0000\n",
            "Epoch 236/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 830171840.0000 - val_loss: 172521152.0000\n",
            "Epoch 237/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 825432640.0000 - val_loss: 170800592.0000\n",
            "Epoch 238/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 831388800.0000 - val_loss: 169084816.0000\n",
            "Epoch 239/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 821406592.0000 - val_loss: 167377280.0000\n",
            "Epoch 240/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 818620608.0000 - val_loss: 165667680.0000\n",
            "Epoch 241/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 816978176.0000 - val_loss: 163968384.0000\n",
            "Epoch 242/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 806023616.0000 - val_loss: 162271520.0000\n",
            "Epoch 243/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 811277952.0000 - val_loss: 160589824.0000\n",
            "Epoch 244/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 810297408.0000 - val_loss: 158914944.0000\n",
            "Epoch 245/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 801847040.0000 - val_loss: 157237456.0000\n",
            "Epoch 246/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 793749760.0000 - val_loss: 155583008.0000\n",
            "Epoch 247/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 796348224.0000 - val_loss: 153915568.0000\n",
            "Epoch 248/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 793322752.0000 - val_loss: 152256512.0000\n",
            "Epoch 249/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 783898368.0000 - val_loss: 150607216.0000\n",
            "Epoch 250/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 777658176.0000 - val_loss: 148960576.0000\n",
            "Epoch 251/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 781029120.0000 - val_loss: 147339616.0000\n",
            "Epoch 252/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 771583744.0000 - val_loss: 145713360.0000\n",
            "Epoch 253/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 773635392.0000 - val_loss: 144084608.0000\n",
            "Epoch 254/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 762282624.0000 - val_loss: 142473008.0000\n",
            "Epoch 255/1000\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 766136960.0000 - val_loss: 140863072.0000\n",
            "Epoch 256/1000\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 763518336.0000 - val_loss: 139262576.0000\n",
            "Epoch 257/1000\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 756868352.0000 - val_loss: 137664992.0000\n",
            "Epoch 258/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 750064576.0000 - val_loss: 136066880.0000\n",
            "Epoch 259/1000\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 747502528.0000 - val_loss: 134471104.0000\n",
            "Epoch 260/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 745616768.0000 - val_loss: 132898056.0000\n",
            "Epoch 261/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 743868032.0000 - val_loss: 131322848.0000\n",
            "Epoch 262/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 740414592.0000 - val_loss: 129755032.0000\n",
            "Epoch 263/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 729238592.0000 - val_loss: 128193392.0000\n",
            "Epoch 264/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 734734656.0000 - val_loss: 126640232.0000\n",
            "Epoch 265/1000\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 726216064.0000 - val_loss: 125093256.0000\n",
            "Epoch 266/1000\n",
            "4/4 [==============================] - 0s 61ms/step - loss: 721961856.0000 - val_loss: 123552376.0000\n",
            "Epoch 267/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 721006464.0000 - val_loss: 122021376.0000\n",
            "Epoch 268/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 711657920.0000 - val_loss: 120492088.0000\n",
            "Epoch 269/1000\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 714583808.0000 - val_loss: 118976952.0000\n",
            "Epoch 270/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 708213056.0000 - val_loss: 117457000.0000\n",
            "Epoch 271/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 704606080.0000 - val_loss: 115950504.0000\n",
            "Epoch 272/1000\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 702684736.0000 - val_loss: 114448472.0000\n",
            "Epoch 273/1000\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 698257856.0000 - val_loss: 112953104.0000\n",
            "Epoch 274/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 691446336.0000 - val_loss: 111472608.0000\n",
            "Epoch 275/1000\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 684972928.0000 - val_loss: 110001384.0000\n",
            "Epoch 276/1000\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 688696960.0000 - val_loss: 108534064.0000\n",
            "Epoch 277/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 683978368.0000 - val_loss: 107066568.0000\n",
            "Epoch 278/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 683956480.0000 - val_loss: 105615792.0000\n",
            "Epoch 279/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 683439360.0000 - val_loss: 104171448.0000\n",
            "Epoch 280/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 675799488.0000 - val_loss: 102742408.0000\n",
            "Epoch 281/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 674600192.0000 - val_loss: 101328952.0000\n",
            "Epoch 282/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 668971200.0000 - val_loss: 99923808.0000\n",
            "Epoch 283/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 662786496.0000 - val_loss: 98524024.0000\n",
            "Epoch 284/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 663742144.0000 - val_loss: 97129832.0000\n",
            "Epoch 285/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 649546624.0000 - val_loss: 95744152.0000\n",
            "Epoch 286/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 648721344.0000 - val_loss: 94352032.0000\n",
            "Epoch 287/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 649579904.0000 - val_loss: 92976336.0000\n",
            "Epoch 288/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 649868608.0000 - val_loss: 91613280.0000\n",
            "Epoch 289/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 636061632.0000 - val_loss: 90248720.0000\n",
            "Epoch 290/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 638802368.0000 - val_loss: 88892736.0000\n",
            "Epoch 291/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 638276288.0000 - val_loss: 87550160.0000\n",
            "Epoch 292/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 624908160.0000 - val_loss: 86215360.0000\n",
            "Epoch 293/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 628350016.0000 - val_loss: 84891064.0000\n",
            "Epoch 294/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 626580288.0000 - val_loss: 83574696.0000\n",
            "Epoch 295/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 618512512.0000 - val_loss: 82272080.0000\n",
            "Epoch 296/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 618235712.0000 - val_loss: 80978528.0000\n",
            "Epoch 297/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 617229952.0000 - val_loss: 79701136.0000\n",
            "Epoch 298/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 610921856.0000 - val_loss: 78425552.0000\n",
            "Epoch 299/1000\n",
            "4/4 [==============================] - 0s 65ms/step - loss: 606807488.0000 - val_loss: 77153272.0000\n",
            "Epoch 300/1000\n",
            "4/4 [==============================] - 0s 67ms/step - loss: 611807040.0000 - val_loss: 75898928.0000\n",
            "Epoch 301/1000\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 604889280.0000 - val_loss: 74657800.0000\n",
            "Epoch 302/1000\n",
            "4/4 [==============================] - 0s 76ms/step - loss: 599771072.0000 - val_loss: 73418728.0000\n",
            "Epoch 303/1000\n",
            "4/4 [==============================] - 0s 75ms/step - loss: 600879424.0000 - val_loss: 72187320.0000\n",
            "Epoch 304/1000\n",
            "4/4 [==============================] - 0s 102ms/step - loss: 590286976.0000 - val_loss: 70971752.0000\n",
            "Epoch 305/1000\n",
            "4/4 [==============================] - 0s 84ms/step - loss: 587002240.0000 - val_loss: 69762832.0000\n",
            "Epoch 306/1000\n",
            "4/4 [==============================] - 0s 60ms/step - loss: 589579840.0000 - val_loss: 68560832.0000\n",
            "Epoch 307/1000\n",
            "4/4 [==============================] - 0s 74ms/step - loss: 581153280.0000 - val_loss: 67373768.0000\n",
            "Epoch 308/1000\n",
            "4/4 [==============================] - 0s 77ms/step - loss: 579642112.0000 - val_loss: 66189752.0000\n",
            "Epoch 309/1000\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 574011840.0000 - val_loss: 65023644.0000\n",
            "Epoch 310/1000\n",
            "4/4 [==============================] - 0s 77ms/step - loss: 569859712.0000 - val_loss: 63865972.0000\n",
            "Epoch 311/1000\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 557887936.0000 - val_loss: 62712832.0000\n",
            "Epoch 312/1000\n",
            "4/4 [==============================] - 0s 61ms/step - loss: 566165248.0000 - val_loss: 61567620.0000\n",
            "Epoch 313/1000\n",
            "4/4 [==============================] - 0s 87ms/step - loss: 557784960.0000 - val_loss: 60434336.0000\n",
            "Epoch 314/1000\n",
            "4/4 [==============================] - 0s 109ms/step - loss: 562042560.0000 - val_loss: 59313504.0000\n",
            "Epoch 315/1000\n",
            "4/4 [==============================] - 0s 75ms/step - loss: 555043456.0000 - val_loss: 58203568.0000\n",
            "Epoch 316/1000\n",
            "4/4 [==============================] - 0s 88ms/step - loss: 548026944.0000 - val_loss: 57109644.0000\n",
            "Epoch 317/1000\n",
            "4/4 [==============================] - 0s 85ms/step - loss: 547375232.0000 - val_loss: 56017980.0000\n",
            "Epoch 318/1000\n",
            "4/4 [==============================] - 0s 87ms/step - loss: 546052416.0000 - val_loss: 54937096.0000\n",
            "Epoch 319/1000\n",
            "4/4 [==============================] - 0s 85ms/step - loss: 539435392.0000 - val_loss: 53865088.0000\n",
            "Epoch 320/1000\n",
            "4/4 [==============================] - 0s 83ms/step - loss: 537108032.0000 - val_loss: 52809632.0000\n",
            "Epoch 321/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 535316096.0000 - val_loss: 51759200.0000\n",
            "Epoch 322/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 528448992.0000 - val_loss: 50720068.0000\n",
            "Epoch 323/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 526981024.0000 - val_loss: 49688372.0000\n",
            "Epoch 324/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 528062912.0000 - val_loss: 48674576.0000\n",
            "Epoch 325/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 521903488.0000 - val_loss: 47664260.0000\n",
            "Epoch 326/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 522892544.0000 - val_loss: 46673068.0000\n",
            "Epoch 327/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 522121472.0000 - val_loss: 45687324.0000\n",
            "Epoch 328/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 514586688.0000 - val_loss: 44710472.0000\n",
            "Epoch 329/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 513020416.0000 - val_loss: 43745976.0000\n",
            "Epoch 330/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 506128320.0000 - val_loss: 42786328.0000\n",
            "Epoch 331/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 504910080.0000 - val_loss: 41842276.0000\n",
            "Epoch 332/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 496570912.0000 - val_loss: 40911192.0000\n",
            "Epoch 333/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 497530400.0000 - val_loss: 39984136.0000\n",
            "Epoch 334/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 498009792.0000 - val_loss: 39069664.0000\n",
            "Epoch 335/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 491654720.0000 - val_loss: 38168376.0000\n",
            "Epoch 336/1000\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 483300448.0000 - val_loss: 37283640.0000\n",
            "Epoch 337/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 486330144.0000 - val_loss: 36405520.0000\n",
            "Epoch 338/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 475045440.0000 - val_loss: 35539696.0000\n",
            "Epoch 339/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 468333696.0000 - val_loss: 34682940.0000\n",
            "Epoch 340/1000\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 472609536.0000 - val_loss: 33833160.0000\n",
            "Epoch 341/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 473856256.0000 - val_loss: 32996596.0000\n",
            "Epoch 342/1000\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 464677984.0000 - val_loss: 32170838.0000\n",
            "Epoch 343/1000\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 466919616.0000 - val_loss: 31359688.0000\n",
            "Epoch 344/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 463534016.0000 - val_loss: 30558060.0000\n",
            "Epoch 345/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 458573856.0000 - val_loss: 29762522.0000\n",
            "Epoch 346/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 451182400.0000 - val_loss: 28979774.0000\n",
            "Epoch 347/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 454063520.0000 - val_loss: 28211270.0000\n",
            "Epoch 348/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 453224672.0000 - val_loss: 27449864.0000\n",
            "Epoch 349/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 444050336.0000 - val_loss: 26701228.0000\n",
            "Epoch 350/1000\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 440096352.0000 - val_loss: 25959104.0000\n",
            "Epoch 351/1000\n",
            "4/4 [==============================] - 0s 55ms/step - loss: 445551712.0000 - val_loss: 25233298.0000\n",
            "Epoch 352/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 439485056.0000 - val_loss: 24511910.0000\n",
            "Epoch 353/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 441539840.0000 - val_loss: 23804502.0000\n",
            "Epoch 354/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 434011808.0000 - val_loss: 23117320.0000\n",
            "Epoch 355/1000\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 438123232.0000 - val_loss: 22435312.0000\n",
            "Epoch 356/1000\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 427139552.0000 - val_loss: 21763772.0000\n",
            "Epoch 357/1000\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 426357504.0000 - val_loss: 21105740.0000\n",
            "Epoch 358/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 415921504.0000 - val_loss: 20456616.0000\n",
            "Epoch 359/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 413259168.0000 - val_loss: 19817942.0000\n",
            "Epoch 360/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 417376864.0000 - val_loss: 19194368.0000\n",
            "Epoch 361/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 409633280.0000 - val_loss: 18584878.0000\n",
            "Epoch 362/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 402532512.0000 - val_loss: 17982616.0000\n",
            "Epoch 363/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 401864352.0000 - val_loss: 17393320.0000\n",
            "Epoch 364/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 404987072.0000 - val_loss: 16813030.0000\n",
            "Epoch 365/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 391522144.0000 - val_loss: 16242231.0000\n",
            "Epoch 366/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 397441728.0000 - val_loss: 15684031.0000\n",
            "Epoch 367/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 401514208.0000 - val_loss: 15138089.0000\n",
            "Epoch 368/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 398493952.0000 - val_loss: 14604825.0000\n",
            "Epoch 369/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 384635520.0000 - val_loss: 14079218.0000\n",
            "Epoch 370/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 389918912.0000 - val_loss: 13569598.0000\n",
            "Epoch 371/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 381248544.0000 - val_loss: 13068084.0000\n",
            "Epoch 372/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 384070976.0000 - val_loss: 12577485.0000\n",
            "Epoch 373/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 371029536.0000 - val_loss: 12100271.0000\n",
            "Epoch 374/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 373555168.0000 - val_loss: 11634143.0000\n",
            "Epoch 375/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 369205696.0000 - val_loss: 11177255.0000\n",
            "Epoch 376/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 373575936.0000 - val_loss: 10731618.0000\n",
            "Epoch 377/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 372508576.0000 - val_loss: 10303403.0000\n",
            "Epoch 378/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 362476224.0000 - val_loss: 9880533.0000\n",
            "Epoch 379/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 362739200.0000 - val_loss: 9468651.0000\n",
            "Epoch 380/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 367658976.0000 - val_loss: 9068193.0000\n",
            "Epoch 381/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 364844992.0000 - val_loss: 8681918.0000\n",
            "Epoch 382/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 352018368.0000 - val_loss: 8308682.0000\n",
            "Epoch 383/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 353689920.0000 - val_loss: 7942311.5000\n",
            "Epoch 384/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 352117472.0000 - val_loss: 7591654.5000\n",
            "Epoch 385/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 342904352.0000 - val_loss: 7253524.0000\n",
            "Epoch 386/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 348337056.0000 - val_loss: 6924290.0000\n",
            "Epoch 387/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 346404160.0000 - val_loss: 6603997.0000\n",
            "Epoch 388/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 339468256.0000 - val_loss: 6298282.5000\n",
            "Epoch 389/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 339171968.0000 - val_loss: 6000282.5000\n",
            "Epoch 390/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 334076960.0000 - val_loss: 5712898.0000\n",
            "Epoch 391/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 329226048.0000 - val_loss: 5437022.0000\n",
            "Epoch 392/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 328613888.0000 - val_loss: 5171399.5000\n",
            "Epoch 393/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 333066880.0000 - val_loss: 4916623.0000\n",
            "Epoch 394/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 329479392.0000 - val_loss: 4672938.5000\n",
            "Epoch 395/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 322796512.0000 - val_loss: 4440086.0000\n",
            "Epoch 396/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 328042752.0000 - val_loss: 4217349.5000\n",
            "Epoch 397/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 316348576.0000 - val_loss: 4005793.7500\n",
            "Epoch 398/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 324982688.0000 - val_loss: 3805204.2500\n",
            "Epoch 399/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 323423264.0000 - val_loss: 3617683.2500\n",
            "Epoch 400/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 314644512.0000 - val_loss: 3439185.0000\n",
            "Epoch 401/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 307343712.0000 - val_loss: 3270144.5000\n",
            "Epoch 402/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 306348640.0000 - val_loss: 3113798.0000\n",
            "Epoch 403/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 299855968.0000 - val_loss: 2967383.2500\n",
            "Epoch 404/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 307264768.0000 - val_loss: 2830857.7500\n",
            "Epoch 405/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 303027040.0000 - val_loss: 2705858.7500\n",
            "Epoch 406/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 303133280.0000 - val_loss: 2591495.0000\n",
            "Epoch 407/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 289515872.0000 - val_loss: 2487387.5000\n",
            "Epoch 408/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 296453824.0000 - val_loss: 2394509.7500\n",
            "Epoch 409/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 285623008.0000 - val_loss: 2312893.2500\n",
            "Epoch 410/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 286970048.0000 - val_loss: 2240916.2500\n",
            "Epoch 411/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 288700672.0000 - val_loss: 2180152.2500\n",
            "Epoch 412/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 283083680.0000 - val_loss: 2129647.0000\n",
            "Epoch 413/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 282717760.0000 - val_loss: 2089821.7500\n",
            "Epoch 414/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 285590400.0000 - val_loss: 2060419.8750\n",
            "Epoch 415/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 275539712.0000 - val_loss: 2041448.1250\n",
            "Epoch 416/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 272825664.0000 - val_loss: 2032824.2500\n",
            "Epoch 417/1000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 269399712.0000 - val_loss: 2034361.7500\n",
            "Epoch 418/1000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 272947840.0000 - val_loss: 2045988.3750\n",
            "Epoch 419/1000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 268817920.0000 - val_loss: 2067822.6250\n",
            "Epoch 420/1000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 268319968.0000 - val_loss: 2099480.2500\n",
            "Epoch 421/1000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 267120656.0000 - val_loss: 2141085.5000\n",
            "Epoch 422/1000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 259049600.0000 - val_loss: 2192940.0000\n",
            "Epoch 423/1000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 260092432.0000 - val_loss: 2255132.2500\n",
            "Epoch 424/1000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 260738848.0000 - val_loss: 2327927.7500\n",
            "Epoch 425/1000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 256806400.0000 - val_loss: 2410761.5000\n",
            "Epoch 426/1000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 250168336.0000 - val_loss: 2503123.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load the best model\n",
        "model = load_model('best_model.h5')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss = model.evaluate(X_test, y_test)\n",
        "print('Test Loss:', test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7FJd-Lj8yW9",
        "outputId": "4a81d34f-e0a6-4561-dd7d-f192f7fd2900"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 9ms/step - loss: 4559591.0000\n",
            "Test Loss: 4559591.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the training, validation, and test loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(test_loss)\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation', 'Test'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "LEQRzjePduvT",
        "outputId": "74786094-fe01-4cea-e890-c30abfd84c20"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print('Mean Absolute Error:', mae)\n",
        "\n",
        "# Calculate variance of transformed y_test\n",
        "y_test_transformed_var = np.var(y_test)\n",
        "print('Variance of y_test_transformed:', y_test_transformed_var)\n",
        "\n",
        "\n",
        "# Calculate the RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"rmse: {rmse}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzy7Hf65MK4u",
        "outputId": "e369aa4a-6e0e-4cae-8f53-25504cfae891"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9465c2fe50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 9ms/step\n",
            "Mean Absolute Error: 1972.1610977893479\n",
            "Variance of y_test_transformed: 2732281.081671216\n",
            "rmse: 2135.3200076731255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pmdarima\n",
        "\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from pmdarima.arima import auto_arima\n",
        "\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "X = df.drop(\"prices\", axis=1) # drop prices column to create X\n",
        "y = df[\"prices\"] # select prices column to create y\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "stepwise_model = auto_arima(y_train, start_p=1, start_q=1,\n",
        "                           max_p=3, max_q=3, m=12,\n",
        "                           start_P=0, seasonal=True,\n",
        "                           d=1, D=1, trace=True,\n",
        "                           error_action='ignore',  \n",
        "                           suppress_warnings=True, \n",
        "                           stepwise=True)\n",
        "\n",
        "print(stepwise_model.order)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jSuJyBmCHWc",
        "outputId": "c310da5f-cea8-49cf-8288-4082f5e4bc6d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pmdarima\n",
            "  Downloading pmdarima-2.0.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (0.29.34)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (1.5.3)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (67.6.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (1.2.0)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (0.13.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.19->pmdarima) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22->pmdarima) (3.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.9/dist-packages (from statsmodels>=0.13.2->pmdarima) (23.1)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.9/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy>=0.5.2->statsmodels>=0.13.2->pmdarima) (1.16.0)\n",
            "Installing collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.3\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,1,1)(0,1,1)[12]             : AIC=7548.534, Time=0.53 sec\n",
            " ARIMA(0,1,0)(0,1,0)[12]             : AIC=7885.289, Time=0.04 sec\n",
            " ARIMA(1,1,0)(1,1,0)[12]             : AIC=7674.864, Time=0.22 sec\n",
            " ARIMA(0,1,1)(0,1,1)[12]             : AIC=inf, Time=1.37 sec\n",
            " ARIMA(1,1,1)(0,1,0)[12]             : AIC=7683.612, Time=0.43 sec\n",
            " ARIMA(1,1,1)(1,1,1)[12]             : AIC=7548.275, Time=1.32 sec\n",
            " ARIMA(1,1,1)(1,1,0)[12]             : AIC=7604.800, Time=0.96 sec\n",
            " ARIMA(1,1,1)(2,1,1)[12]             : AIC=7549.380, Time=1.86 sec\n",
            " ARIMA(1,1,1)(1,1,2)[12]             : AIC=7548.206, Time=2.10 sec\n",
            " ARIMA(1,1,1)(0,1,2)[12]             : AIC=7547.809, Time=1.82 sec\n",
            " ARIMA(0,1,1)(0,1,2)[12]             : AIC=7548.577, Time=1.28 sec\n",
            " ARIMA(1,1,0)(0,1,2)[12]             : AIC=7608.185, Time=1.18 sec\n",
            " ARIMA(2,1,1)(0,1,2)[12]             : AIC=7544.185, Time=2.16 sec\n",
            " ARIMA(2,1,1)(0,1,1)[12]             : AIC=7544.523, Time=0.90 sec\n",
            " ARIMA(2,1,1)(1,1,2)[12]             : AIC=7544.632, Time=5.40 sec\n",
            " ARIMA(2,1,1)(1,1,1)[12]             : AIC=7544.587, Time=1.05 sec\n",
            " ARIMA(2,1,0)(0,1,2)[12]             : AIC=7583.896, Time=1.38 sec\n",
            " ARIMA(3,1,1)(0,1,2)[12]             : AIC=7546.160, Time=2.71 sec\n",
            " ARIMA(2,1,2)(0,1,2)[12]             : AIC=7538.309, Time=2.64 sec\n",
            " ARIMA(2,1,2)(0,1,1)[12]             : AIC=inf, Time=3.22 sec\n",
            " ARIMA(2,1,2)(1,1,2)[12]             : AIC=7538.713, Time=3.98 sec\n",
            " ARIMA(2,1,2)(1,1,1)[12]             : AIC=inf, Time=2.78 sec\n",
            " ARIMA(1,1,2)(0,1,2)[12]             : AIC=7537.057, Time=3.21 sec\n",
            " ARIMA(1,1,2)(0,1,1)[12]             : AIC=7537.773, Time=0.61 sec\n",
            " ARIMA(1,1,2)(1,1,2)[12]             : AIC=7537.212, Time=2.50 sec\n",
            " ARIMA(1,1,2)(1,1,1)[12]             : AIC=7537.558, Time=1.96 sec\n",
            " ARIMA(0,1,2)(0,1,2)[12]             : AIC=7544.992, Time=3.25 sec\n",
            " ARIMA(1,1,3)(0,1,2)[12]             : AIC=7536.133, Time=3.20 sec\n",
            " ARIMA(1,1,3)(0,1,1)[12]             : AIC=7536.639, Time=1.22 sec\n",
            " ARIMA(1,1,3)(1,1,2)[12]             : AIC=7536.367, Time=4.42 sec\n",
            " ARIMA(1,1,3)(1,1,1)[12]             : AIC=7536.578, Time=2.03 sec\n",
            " ARIMA(0,1,3)(0,1,2)[12]             : AIC=7542.185, Time=3.52 sec\n",
            " ARIMA(2,1,3)(0,1,2)[12]             : AIC=7534.546, Time=5.15 sec\n",
            " ARIMA(2,1,3)(0,1,1)[12]             : AIC=7534.558, Time=2.01 sec\n",
            " ARIMA(2,1,3)(1,1,2)[12]             : AIC=7534.418, Time=8.48 sec\n",
            " ARIMA(2,1,3)(1,1,1)[12]             : AIC=7535.019, Time=2.66 sec\n",
            " ARIMA(2,1,3)(2,1,2)[12]             : AIC=7538.000, Time=7.17 sec\n",
            " ARIMA(2,1,3)(2,1,1)[12]             : AIC=7535.361, Time=6.84 sec\n",
            " ARIMA(3,1,3)(1,1,2)[12]             : AIC=inf, Time=6.96 sec\n",
            " ARIMA(3,1,2)(1,1,2)[12]             : AIC=7539.913, Time=6.43 sec\n",
            " ARIMA(2,1,3)(1,1,2)[12] intercept   : AIC=7539.245, Time=7.06 sec\n",
            "\n",
            "Best model:  ARIMA(2,1,3)(1,1,2)[12]          \n",
            "Total fit time: 118.073 seconds\n",
            "(2, 1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit an ARIMA model to the time series data\n",
        "(p,d,q) = (2, 1, 3)\n",
        "model = ARIMA(y_train, order=(p, d, q))\n",
        "model_fit = model.fit()\n",
        "\n"
      ],
      "metadata": {
        "id": "UeIRPJ3-K9nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c66e578-012f-4224-bfdd-9d310469d8fc"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.9/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.9/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.9/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        }
      ]
    }
  ]
}