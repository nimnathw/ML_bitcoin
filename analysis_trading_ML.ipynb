{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1exZQwZvPofP4SLF4-r93am29lMQmFVTa",
      "authorship_tag": "ABX9TyN3i8iI3Rih78yvLPsvhlyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimnathw/bitcoin-price-prediction/blob/master/analysis_trading_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqvLmQGwPuR6",
        "outputId": "236532d8-d55e-4b06-a1f5-5438fb756899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks')\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "metadata": {
        "id": "-SluWrI7RjoZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the cleaned data into a DataFrame\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df[df['date'] > '2022-01-01']\n",
        "df.set_index('date', inplace=True)\n",
        "\n",
        "\n",
        "# view data\n",
        "df.describe()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "Jq9zLMMFRrSE",
        "outputId": "e0711642-94b7-4215-e81d-f38d09fe0303"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             ACDGNO      AWHMAN       DGS10      DGS1MO      DGS3MO  \\\n",
              "count    209.000000  209.000000  209.000000  209.000000  209.000000   \n",
              "mean   44176.349282   41.187081    2.799043    1.247464    1.653158   \n",
              "std     1751.369590    0.197765    0.675100    1.157003    1.269436   \n",
              "min    41200.000000   40.900000    1.630000    0.020000    0.080000   \n",
              "25%    43202.000000   41.000000    2.200000    0.180000    0.460000   \n",
              "50%    44667.000000   41.100000    2.860000    0.850000    1.170000   \n",
              "75%    45188.000000   41.200000    3.150000    2.240000    2.710000   \n",
              "max    46602.000000   41.600000    4.250000    3.760000    4.230000   \n",
              "\n",
              "             DGS5      DGS6MO         IC4WSA          M2SL     NASDAQ100  \\\n",
              "count  209.000000  209.000000     209.000000    209.000000    209.000000   \n",
              "mean     2.827560    2.100239  213889.952153  21618.207656  13021.991914   \n",
              "std      0.790562    1.320076   22126.938626     91.395895   1352.836783   \n",
              "min      1.370000    0.220000  170500.000000  21351.600000  10692.060000   \n",
              "25%      2.180000    0.860000  196750.000000  21607.400000  11875.630000   \n",
              "50%      2.910000    1.640000  215750.000000  21636.100000  12881.790000   \n",
              "75%      3.250000    3.150000  231750.000000  21649.600000  14149.120000   \n",
              "max      4.450000    4.580000  249500.000000  21739.700000  16501.770000   \n",
              "\n",
              "       PCUADLVWRADLVWR       PERMIT     RETAILIMSA        SP500      T10YFF  \\\n",
              "count       209.000000   209.000000     209.000000   209.000000  209.000000   \n",
              "mean        189.768967  1707.662201  711141.215311  4136.572488    1.551627   \n",
              "std           2.833958   131.002072   27618.289319   302.386048    0.623636   \n",
              "min         185.559000  1512.000000  660783.000000  3577.030000    0.270000   \n",
              "25%         186.557000  1564.000000  692095.000000  3900.110000    1.000000   \n",
              "50%         189.901000  1695.000000  723116.000000  4131.930000    1.680000   \n",
              "75%         192.286000  1841.000000  738147.000000  4392.590000    2.020000   \n",
              "max         194.187000  1879.000000  741260.000000  4796.560000    2.660000   \n",
              "\n",
              "          UMCSENT   market_caps        prices  total_volumes  \n",
              "count  209.000000  2.090000e+02    209.000000   2.090000e+02  \n",
              "mean    59.032536  5.811936e+11  30534.719520   2.893393e+10  \n",
              "std      5.030722  1.859799e+11   9871.561187   1.061728e+10  \n",
              "min     50.000000  3.551945e+11  18539.635238   1.180719e+10  \n",
              "25%     58.200000  3.931043e+11  20574.840592   2.207497e+10  \n",
              "50%     58.600000  5.648852e+11  29655.026132   2.685591e+10  \n",
              "75%     62.800000  7.700967e+11  40488.877918   3.213122e+10  \n",
              "max     67.200000  9.014300e+11  47459.261238   6.598570e+10  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-001b9937-a3e4-4f9a-8688-900c6f592640\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ACDGNO</th>\n",
              "      <th>AWHMAN</th>\n",
              "      <th>DGS10</th>\n",
              "      <th>DGS1MO</th>\n",
              "      <th>DGS3MO</th>\n",
              "      <th>DGS5</th>\n",
              "      <th>DGS6MO</th>\n",
              "      <th>IC4WSA</th>\n",
              "      <th>M2SL</th>\n",
              "      <th>NASDAQ100</th>\n",
              "      <th>PCUADLVWRADLVWR</th>\n",
              "      <th>PERMIT</th>\n",
              "      <th>RETAILIMSA</th>\n",
              "      <th>SP500</th>\n",
              "      <th>T10YFF</th>\n",
              "      <th>UMCSENT</th>\n",
              "      <th>market_caps</th>\n",
              "      <th>prices</th>\n",
              "      <th>total_volumes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>2.090000e+02</td>\n",
              "      <td>209.000000</td>\n",
              "      <td>2.090000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>44176.349282</td>\n",
              "      <td>41.187081</td>\n",
              "      <td>2.799043</td>\n",
              "      <td>1.247464</td>\n",
              "      <td>1.653158</td>\n",
              "      <td>2.827560</td>\n",
              "      <td>2.100239</td>\n",
              "      <td>213889.952153</td>\n",
              "      <td>21618.207656</td>\n",
              "      <td>13021.991914</td>\n",
              "      <td>189.768967</td>\n",
              "      <td>1707.662201</td>\n",
              "      <td>711141.215311</td>\n",
              "      <td>4136.572488</td>\n",
              "      <td>1.551627</td>\n",
              "      <td>59.032536</td>\n",
              "      <td>5.811936e+11</td>\n",
              "      <td>30534.719520</td>\n",
              "      <td>2.893393e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1751.369590</td>\n",
              "      <td>0.197765</td>\n",
              "      <td>0.675100</td>\n",
              "      <td>1.157003</td>\n",
              "      <td>1.269436</td>\n",
              "      <td>0.790562</td>\n",
              "      <td>1.320076</td>\n",
              "      <td>22126.938626</td>\n",
              "      <td>91.395895</td>\n",
              "      <td>1352.836783</td>\n",
              "      <td>2.833958</td>\n",
              "      <td>131.002072</td>\n",
              "      <td>27618.289319</td>\n",
              "      <td>302.386048</td>\n",
              "      <td>0.623636</td>\n",
              "      <td>5.030722</td>\n",
              "      <td>1.859799e+11</td>\n",
              "      <td>9871.561187</td>\n",
              "      <td>1.061728e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>41200.000000</td>\n",
              "      <td>40.900000</td>\n",
              "      <td>1.630000</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>1.370000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>170500.000000</td>\n",
              "      <td>21351.600000</td>\n",
              "      <td>10692.060000</td>\n",
              "      <td>185.559000</td>\n",
              "      <td>1512.000000</td>\n",
              "      <td>660783.000000</td>\n",
              "      <td>3577.030000</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>3.551945e+11</td>\n",
              "      <td>18539.635238</td>\n",
              "      <td>1.180719e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>43202.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>2.200000</td>\n",
              "      <td>0.180000</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>2.180000</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>196750.000000</td>\n",
              "      <td>21607.400000</td>\n",
              "      <td>11875.630000</td>\n",
              "      <td>186.557000</td>\n",
              "      <td>1564.000000</td>\n",
              "      <td>692095.000000</td>\n",
              "      <td>3900.110000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>58.200000</td>\n",
              "      <td>3.931043e+11</td>\n",
              "      <td>20574.840592</td>\n",
              "      <td>2.207497e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>44667.000000</td>\n",
              "      <td>41.100000</td>\n",
              "      <td>2.860000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>1.170000</td>\n",
              "      <td>2.910000</td>\n",
              "      <td>1.640000</td>\n",
              "      <td>215750.000000</td>\n",
              "      <td>21636.100000</td>\n",
              "      <td>12881.790000</td>\n",
              "      <td>189.901000</td>\n",
              "      <td>1695.000000</td>\n",
              "      <td>723116.000000</td>\n",
              "      <td>4131.930000</td>\n",
              "      <td>1.680000</td>\n",
              "      <td>58.600000</td>\n",
              "      <td>5.648852e+11</td>\n",
              "      <td>29655.026132</td>\n",
              "      <td>2.685591e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>45188.000000</td>\n",
              "      <td>41.200000</td>\n",
              "      <td>3.150000</td>\n",
              "      <td>2.240000</td>\n",
              "      <td>2.710000</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>3.150000</td>\n",
              "      <td>231750.000000</td>\n",
              "      <td>21649.600000</td>\n",
              "      <td>14149.120000</td>\n",
              "      <td>192.286000</td>\n",
              "      <td>1841.000000</td>\n",
              "      <td>738147.000000</td>\n",
              "      <td>4392.590000</td>\n",
              "      <td>2.020000</td>\n",
              "      <td>62.800000</td>\n",
              "      <td>7.700967e+11</td>\n",
              "      <td>40488.877918</td>\n",
              "      <td>3.213122e+10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>46602.000000</td>\n",
              "      <td>41.600000</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>3.760000</td>\n",
              "      <td>4.230000</td>\n",
              "      <td>4.450000</td>\n",
              "      <td>4.580000</td>\n",
              "      <td>249500.000000</td>\n",
              "      <td>21739.700000</td>\n",
              "      <td>16501.770000</td>\n",
              "      <td>194.187000</td>\n",
              "      <td>1879.000000</td>\n",
              "      <td>741260.000000</td>\n",
              "      <td>4796.560000</td>\n",
              "      <td>2.660000</td>\n",
              "      <td>67.200000</td>\n",
              "      <td>9.014300e+11</td>\n",
              "      <td>47459.261238</td>\n",
              "      <td>6.598570e+10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-001b9937-a3e4-4f9a-8688-900c6f592640')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-001b9937-a3e4-4f9a-8688-900c6f592640 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-001b9937-a3e4-4f9a-8688-900c6f592640');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of lags\n",
        "n_lags = 7\n",
        "\n",
        "# Add lagged values of the dependent variable\n",
        "for i in range(1, n_lags+1):\n",
        "    df[f'prices_lag{i}'] = df['prices'].shift(i)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "R5DO0SN_48Re",
        "outputId": "bf001962-0809-4b2a-fe86-58dfa733431b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             ACDGNO  AWHMAN  DGS10  DGS1MO  DGS3MO  DGS5  DGS6MO    IC4WSA  \\\n",
              "date                                                                         \n",
              "2022-02-02  41497.0    41.6   1.78    0.04    0.19  1.60    0.45  228500.0   \n",
              "2022-02-03  41497.0    41.6   1.82    0.03    0.20  1.66    0.48  228500.0   \n",
              "2022-02-04  41497.0    41.6   1.93    0.05    0.23  1.78    0.56  228500.0   \n",
              "2022-02-07  41497.0    41.6   1.92    0.03    0.27  1.76    0.58  216750.0   \n",
              "2022-02-08  41497.0    41.6   1.96    0.03    0.25  1.81    0.59  216750.0   \n",
              "\n",
              "               M2SL  NASDAQ100  ...   market_caps        prices  \\\n",
              "date                            ...                               \n",
              "2022-02-02  21708.4   15139.74  ...  7.361023e+11  38835.694943   \n",
              "2022-02-03  21708.4   14501.11  ...  7.006456e+11  37000.982499   \n",
              "2022-02-04  21708.4   14694.35  ...  7.030033e+11  37101.351594   \n",
              "2022-02-07  21708.4   14571.25  ...  8.049420e+11  42475.543221   \n",
              "2022-02-08  21708.4   14747.03  ...  8.333925e+11  43910.929986   \n",
              "\n",
              "            total_volumes   prices_lag1   prices_lag2   prices_lag3  \\\n",
              "date                                                                  \n",
              "2022-02-02   1.710912e+10  38555.534461  37983.151499  37276.839558   \n",
              "2022-02-03   1.670991e+10  38835.694943  38555.534461  37983.151499   \n",
              "2022-02-04   1.613630e+10  37000.982499  38835.694943  38555.534461   \n",
              "2022-02-07   1.320649e+10  37101.351594  37000.982499  38835.694943   \n",
              "2022-02-08   2.417186e+10  42475.543221  37101.351594  37000.982499   \n",
              "\n",
              "             prices_lag4   prices_lag5   prices_lag6   prices_lag7  \n",
              "date                                                                \n",
              "2022-02-02  36870.440167  36988.928511  36774.007142  36306.409440  \n",
              "2022-02-03  37276.839558  36870.440167  36988.928511  36774.007142  \n",
              "2022-02-04  37983.151499  37276.839558  36870.440167  36988.928511  \n",
              "2022-02-07  38555.534461  37983.151499  37276.839558  36870.440167  \n",
              "2022-02-08  38835.694943  38555.534461  37983.151499  37276.839558  \n",
              "\n",
              "[5 rows x 26 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4151079-8eb0-422c-9916-97f49c0474d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ACDGNO</th>\n",
              "      <th>AWHMAN</th>\n",
              "      <th>DGS10</th>\n",
              "      <th>DGS1MO</th>\n",
              "      <th>DGS3MO</th>\n",
              "      <th>DGS5</th>\n",
              "      <th>DGS6MO</th>\n",
              "      <th>IC4WSA</th>\n",
              "      <th>M2SL</th>\n",
              "      <th>NASDAQ100</th>\n",
              "      <th>...</th>\n",
              "      <th>market_caps</th>\n",
              "      <th>prices</th>\n",
              "      <th>total_volumes</th>\n",
              "      <th>prices_lag1</th>\n",
              "      <th>prices_lag2</th>\n",
              "      <th>prices_lag3</th>\n",
              "      <th>prices_lag4</th>\n",
              "      <th>prices_lag5</th>\n",
              "      <th>prices_lag6</th>\n",
              "      <th>prices_lag7</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2022-02-02</th>\n",
              "      <td>41497.0</td>\n",
              "      <td>41.6</td>\n",
              "      <td>1.78</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.45</td>\n",
              "      <td>228500.0</td>\n",
              "      <td>21708.4</td>\n",
              "      <td>15139.74</td>\n",
              "      <td>...</td>\n",
              "      <td>7.361023e+11</td>\n",
              "      <td>38835.694943</td>\n",
              "      <td>1.710912e+10</td>\n",
              "      <td>38555.534461</td>\n",
              "      <td>37983.151499</td>\n",
              "      <td>37276.839558</td>\n",
              "      <td>36870.440167</td>\n",
              "      <td>36988.928511</td>\n",
              "      <td>36774.007142</td>\n",
              "      <td>36306.409440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-03</th>\n",
              "      <td>41497.0</td>\n",
              "      <td>41.6</td>\n",
              "      <td>1.82</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.20</td>\n",
              "      <td>1.66</td>\n",
              "      <td>0.48</td>\n",
              "      <td>228500.0</td>\n",
              "      <td>21708.4</td>\n",
              "      <td>14501.11</td>\n",
              "      <td>...</td>\n",
              "      <td>7.006456e+11</td>\n",
              "      <td>37000.982499</td>\n",
              "      <td>1.670991e+10</td>\n",
              "      <td>38835.694943</td>\n",
              "      <td>38555.534461</td>\n",
              "      <td>37983.151499</td>\n",
              "      <td>37276.839558</td>\n",
              "      <td>36870.440167</td>\n",
              "      <td>36988.928511</td>\n",
              "      <td>36774.007142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-04</th>\n",
              "      <td>41497.0</td>\n",
              "      <td>41.6</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.23</td>\n",
              "      <td>1.78</td>\n",
              "      <td>0.56</td>\n",
              "      <td>228500.0</td>\n",
              "      <td>21708.4</td>\n",
              "      <td>14694.35</td>\n",
              "      <td>...</td>\n",
              "      <td>7.030033e+11</td>\n",
              "      <td>37101.351594</td>\n",
              "      <td>1.613630e+10</td>\n",
              "      <td>37000.982499</td>\n",
              "      <td>38835.694943</td>\n",
              "      <td>38555.534461</td>\n",
              "      <td>37983.151499</td>\n",
              "      <td>37276.839558</td>\n",
              "      <td>36870.440167</td>\n",
              "      <td>36988.928511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-07</th>\n",
              "      <td>41497.0</td>\n",
              "      <td>41.6</td>\n",
              "      <td>1.92</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.27</td>\n",
              "      <td>1.76</td>\n",
              "      <td>0.58</td>\n",
              "      <td>216750.0</td>\n",
              "      <td>21708.4</td>\n",
              "      <td>14571.25</td>\n",
              "      <td>...</td>\n",
              "      <td>8.049420e+11</td>\n",
              "      <td>42475.543221</td>\n",
              "      <td>1.320649e+10</td>\n",
              "      <td>37101.351594</td>\n",
              "      <td>37000.982499</td>\n",
              "      <td>38835.694943</td>\n",
              "      <td>38555.534461</td>\n",
              "      <td>37983.151499</td>\n",
              "      <td>37276.839558</td>\n",
              "      <td>36870.440167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-08</th>\n",
              "      <td>41497.0</td>\n",
              "      <td>41.6</td>\n",
              "      <td>1.96</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.81</td>\n",
              "      <td>0.59</td>\n",
              "      <td>216750.0</td>\n",
              "      <td>21708.4</td>\n",
              "      <td>14747.03</td>\n",
              "      <td>...</td>\n",
              "      <td>8.333925e+11</td>\n",
              "      <td>43910.929986</td>\n",
              "      <td>2.417186e+10</td>\n",
              "      <td>42475.543221</td>\n",
              "      <td>37101.351594</td>\n",
              "      <td>37000.982499</td>\n",
              "      <td>38835.694943</td>\n",
              "      <td>38555.534461</td>\n",
              "      <td>37983.151499</td>\n",
              "      <td>37276.839558</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 26 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4151079-8eb0-422c-9916-97f49c0474d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a4151079-8eb0-422c-9916-97f49c0474d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a4151079-8eb0-422c-9916-97f49c0474d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "\n",
        "# Get the dependent variable 'prices'\n",
        "y = df['prices']\n",
        "\n",
        "# Get the independent variables\n",
        "X = df.drop(columns=['prices']) \n",
        "\n",
        "# Define the number of splits\n",
        "n_splits = 3\n",
        "\n",
        "# Create the TimeSeriesSplit object\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "for train_index, test_index in tscv.split(df):\n",
        "    X_train, X_test = df.iloc[train_index, :], df.iloc[test_index, :]\n",
        "    y_train, y_test = df.iloc[train_index, -1], df.iloc[test_index, -1]\n",
        "\n",
        "# Split the data into training and test sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "print(scaler.scale_.shape)"
      ],
      "metadata": {
        "id": "8vKenvPmSLI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "349d800a-2f44-4990-d1b3-c8652a1cdbb2"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(26,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "# Add first LSTM layer with 64 units and input shape defined as the number of features in the X_train dataset\n",
        "model.add(LSTM(units=64, input_shape=(X_train.shape[1],1), return_sequences=True))\n",
        "# Add a second LSTM layer with 64 units\n",
        "model.add(LSTM(units=64))\n",
        "# Add a dropout layer to help prevent overfitting\n",
        "model.add(Dropout(0.2))\n",
        "# Add a fully connected Dense layer with 64 units\n",
        "model.add(Dense(64))\n",
        "# Add fully connected Dense layer with one unit to produce the output\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Define early stopping and checkpoint callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "# Train the model on the training data\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, batch_size=32, callbacks=[early_stopping, checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU_E030gSTM9",
        "outputId": "d143aba1-6858-41c0-af8d-4a1f264d52ed"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "4/4 [==============================] - 7s 593ms/step - loss: 1459336704.0000 - val_loss: 476044448.0000\n",
            "Epoch 2/1000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 1459305728.0000 - val_loss: 476055424.0000\n",
            "Epoch 3/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1459247488.0000 - val_loss: 476043264.0000\n",
            "Epoch 4/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1459102336.0000 - val_loss: 475864800.0000\n",
            "Epoch 5/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1458778368.0000 - val_loss: 475565888.0000\n",
            "Epoch 6/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1458355840.0000 - val_loss: 475275200.0000\n",
            "Epoch 7/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1457893376.0000 - val_loss: 475022976.0000\n",
            "Epoch 8/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1457459328.0000 - val_loss: 474781312.0000\n",
            "Epoch 9/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1457038976.0000 - val_loss: 474537312.0000\n",
            "Epoch 10/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1456604416.0000 - val_loss: 474288032.0000\n",
            "Epoch 11/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1456239488.0000 - val_loss: 474033184.0000\n",
            "Epoch 12/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1455753344.0000 - val_loss: 473771328.0000\n",
            "Epoch 13/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1455286272.0000 - val_loss: 473501056.0000\n",
            "Epoch 14/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1454812800.0000 - val_loss: 473221664.0000\n",
            "Epoch 15/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 1454287488.0000 - val_loss: 472930400.0000\n",
            "Epoch 16/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1453843456.0000 - val_loss: 472628448.0000\n",
            "Epoch 17/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1453245056.0000 - val_loss: 472315360.0000\n",
            "Epoch 18/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1452673664.0000 - val_loss: 471988416.0000\n",
            "Epoch 19/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1452194304.0000 - val_loss: 471650112.0000\n",
            "Epoch 20/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1451612800.0000 - val_loss: 471300352.0000\n",
            "Epoch 21/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1450957312.0000 - val_loss: 470938464.0000\n",
            "Epoch 22/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1450321664.0000 - val_loss: 470564576.0000\n",
            "Epoch 23/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1449673088.0000 - val_loss: 470175712.0000\n",
            "Epoch 24/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1448888704.0000 - val_loss: 469773568.0000\n",
            "Epoch 25/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1448238592.0000 - val_loss: 469358400.0000\n",
            "Epoch 26/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1447504256.0000 - val_loss: 468929568.0000\n",
            "Epoch 27/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 1446843648.0000 - val_loss: 468487968.0000\n",
            "Epoch 28/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1445950720.0000 - val_loss: 468033280.0000\n",
            "Epoch 29/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1445132160.0000 - val_loss: 467563744.0000\n",
            "Epoch 30/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1444385024.0000 - val_loss: 467080960.0000\n",
            "Epoch 31/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1443478272.0000 - val_loss: 466585824.0000\n",
            "Epoch 32/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 1442817920.0000 - val_loss: 466079488.0000\n",
            "Epoch 33/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1441597824.0000 - val_loss: 465554688.0000\n",
            "Epoch 34/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1440954368.0000 - val_loss: 465020032.0000\n",
            "Epoch 35/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1440062592.0000 - val_loss: 464469408.0000\n",
            "Epoch 36/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1439056000.0000 - val_loss: 463907744.0000\n",
            "Epoch 37/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1437936512.0000 - val_loss: 463330432.0000\n",
            "Epoch 38/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1437009792.0000 - val_loss: 462739232.0000\n",
            "Epoch 39/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1436145664.0000 - val_loss: 462137792.0000\n",
            "Epoch 40/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1434739840.0000 - val_loss: 461518400.0000\n",
            "Epoch 41/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1433863936.0000 - val_loss: 460887392.0000\n",
            "Epoch 42/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1432739584.0000 - val_loss: 460242080.0000\n",
            "Epoch 43/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1431857024.0000 - val_loss: 459585728.0000\n",
            "Epoch 44/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1430519168.0000 - val_loss: 458916448.0000\n",
            "Epoch 45/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1429361408.0000 - val_loss: 458231520.0000\n",
            "Epoch 46/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 1428135424.0000 - val_loss: 457532832.0000\n",
            "Epoch 47/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1426866048.0000 - val_loss: 456819488.0000\n",
            "Epoch 48/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1425482240.0000 - val_loss: 456093248.0000\n",
            "Epoch 49/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1424615040.0000 - val_loss: 455353248.0000\n",
            "Epoch 50/1000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 1423029760.0000 - val_loss: 454598912.0000\n",
            "Epoch 51/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1421347328.0000 - val_loss: 453830336.0000\n",
            "Epoch 52/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1420675072.0000 - val_loss: 453048928.0000\n",
            "Epoch 53/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1418851456.0000 - val_loss: 452253216.0000\n",
            "Epoch 54/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1418065024.0000 - val_loss: 451446592.0000\n",
            "Epoch 55/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1416789120.0000 - val_loss: 450629216.0000\n",
            "Epoch 56/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1414786048.0000 - val_loss: 449802560.0000\n",
            "Epoch 57/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1413799808.0000 - val_loss: 448960128.0000\n",
            "Epoch 58/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1412000768.0000 - val_loss: 448105472.0000\n",
            "Epoch 59/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1410266496.0000 - val_loss: 447234976.0000\n",
            "Epoch 60/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1408633472.0000 - val_loss: 446347072.0000\n",
            "Epoch 61/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1407213952.0000 - val_loss: 445447456.0000\n",
            "Epoch 62/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1406292992.0000 - val_loss: 444536032.0000\n",
            "Epoch 63/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1404165888.0000 - val_loss: 443612640.0000\n",
            "Epoch 64/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 1402185856.0000 - val_loss: 442674976.0000\n",
            "Epoch 65/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1401075456.0000 - val_loss: 441724864.0000\n",
            "Epoch 66/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1399255040.0000 - val_loss: 440764224.0000\n",
            "Epoch 67/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1396912384.0000 - val_loss: 439786976.0000\n",
            "Epoch 68/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1395540864.0000 - val_loss: 438795200.0000\n",
            "Epoch 69/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1393841280.0000 - val_loss: 437790720.0000\n",
            "Epoch 70/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1392529408.0000 - val_loss: 436775360.0000\n",
            "Epoch 71/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1390667520.0000 - val_loss: 435748160.0000\n",
            "Epoch 72/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1388946688.0000 - val_loss: 434710080.0000\n",
            "Epoch 73/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1386598144.0000 - val_loss: 433660960.0000\n",
            "Epoch 74/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1384860928.0000 - val_loss: 432597824.0000\n",
            "Epoch 75/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1383631488.0000 - val_loss: 431524992.0000\n",
            "Epoch 76/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1381182208.0000 - val_loss: 430435808.0000\n",
            "Epoch 77/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1379631744.0000 - val_loss: 429338272.0000\n",
            "Epoch 78/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1377468032.0000 - val_loss: 428228480.0000\n",
            "Epoch 79/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1374298624.0000 - val_loss: 427106144.0000\n",
            "Epoch 80/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1373126144.0000 - val_loss: 425967552.0000\n",
            "Epoch 81/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1371228672.0000 - val_loss: 424819488.0000\n",
            "Epoch 82/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1368892928.0000 - val_loss: 423662688.0000\n",
            "Epoch 83/1000\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 1367253248.0000 - val_loss: 422491776.0000\n",
            "Epoch 84/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 1364926976.0000 - val_loss: 421307296.0000\n",
            "Epoch 85/1000\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 1363061888.0000 - val_loss: 420111584.0000\n",
            "Epoch 86/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 1361046016.0000 - val_loss: 418907200.0000\n",
            "Epoch 87/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 1359232384.0000 - val_loss: 417696352.0000\n",
            "Epoch 88/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 1356360704.0000 - val_loss: 416470176.0000\n",
            "Epoch 89/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 1354006656.0000 - val_loss: 415234048.0000\n",
            "Epoch 90/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 1352783872.0000 - val_loss: 413980768.0000\n",
            "Epoch 91/1000\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 1350699648.0000 - val_loss: 412724192.0000\n",
            "Epoch 92/1000\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 1346545536.0000 - val_loss: 411456192.0000\n",
            "Epoch 93/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 1346085504.0000 - val_loss: 410173440.0000\n",
            "Epoch 94/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 1342457984.0000 - val_loss: 408881632.0000\n",
            "Epoch 95/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 1339884928.0000 - val_loss: 407576704.0000\n",
            "Epoch 96/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 1338247040.0000 - val_loss: 406258656.0000\n",
            "Epoch 97/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 1336229248.0000 - val_loss: 404937504.0000\n",
            "Epoch 98/1000\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 1333100800.0000 - val_loss: 403602752.0000\n",
            "Epoch 99/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 1331662464.0000 - val_loss: 402256608.0000\n",
            "Epoch 100/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 1328645376.0000 - val_loss: 400903200.0000\n",
            "Epoch 101/1000\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 1326604544.0000 - val_loss: 399539232.0000\n",
            "Epoch 102/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 1324197376.0000 - val_loss: 398164064.0000\n",
            "Epoch 103/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 1320482560.0000 - val_loss: 396777792.0000\n",
            "Epoch 104/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 1320304128.0000 - val_loss: 395386240.0000\n",
            "Epoch 105/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 1315974016.0000 - val_loss: 393981216.0000\n",
            "Epoch 106/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 1313383424.0000 - val_loss: 392569120.0000\n",
            "Epoch 107/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 1312467456.0000 - val_loss: 391148544.0000\n",
            "Epoch 108/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1309337728.0000 - val_loss: 389714528.0000\n",
            "Epoch 109/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1307038464.0000 - val_loss: 388275488.0000\n",
            "Epoch 110/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1303755008.0000 - val_loss: 386829600.0000\n",
            "Epoch 111/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1299256832.0000 - val_loss: 385374016.0000\n",
            "Epoch 112/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1298415232.0000 - val_loss: 383905792.0000\n",
            "Epoch 113/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1296464896.0000 - val_loss: 382429728.0000\n",
            "Epoch 114/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1294785792.0000 - val_loss: 380947040.0000\n",
            "Epoch 115/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1290592000.0000 - val_loss: 379459616.0000\n",
            "Epoch 116/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1288731264.0000 - val_loss: 377967264.0000\n",
            "Epoch 117/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1284251392.0000 - val_loss: 376459808.0000\n",
            "Epoch 118/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1283386112.0000 - val_loss: 374944192.0000\n",
            "Epoch 119/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 1278341376.0000 - val_loss: 373424512.0000\n",
            "Epoch 120/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1277909632.0000 - val_loss: 371895648.0000\n",
            "Epoch 121/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1276023168.0000 - val_loss: 370359872.0000\n",
            "Epoch 122/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1273249664.0000 - val_loss: 368818368.0000\n",
            "Epoch 123/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1267175680.0000 - val_loss: 367272256.0000\n",
            "Epoch 124/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 1265190144.0000 - val_loss: 365715008.0000\n",
            "Epoch 125/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1262149888.0000 - val_loss: 364142912.0000\n",
            "Epoch 126/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1261263360.0000 - val_loss: 362569312.0000\n",
            "Epoch 127/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 1256619264.0000 - val_loss: 360989280.0000\n",
            "Epoch 128/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1254052224.0000 - val_loss: 359394144.0000\n",
            "Epoch 129/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1249580928.0000 - val_loss: 357796448.0000\n",
            "Epoch 130/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1250356992.0000 - val_loss: 356188928.0000\n",
            "Epoch 131/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1244838016.0000 - val_loss: 354580928.0000\n",
            "Epoch 132/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1242412544.0000 - val_loss: 352960064.0000\n",
            "Epoch 133/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1239664512.0000 - val_loss: 351330688.0000\n",
            "Epoch 134/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1235297664.0000 - val_loss: 349693088.0000\n",
            "Epoch 135/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1231301888.0000 - val_loss: 348052384.0000\n",
            "Epoch 136/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1229806848.0000 - val_loss: 346405600.0000\n",
            "Epoch 137/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1229205120.0000 - val_loss: 344753600.0000\n",
            "Epoch 138/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 1223938816.0000 - val_loss: 343097664.0000\n",
            "Epoch 139/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1219580544.0000 - val_loss: 341437760.0000\n",
            "Epoch 140/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1216764672.0000 - val_loss: 339766816.0000\n",
            "Epoch 141/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1214521600.0000 - val_loss: 338089440.0000\n",
            "Epoch 142/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1209297792.0000 - val_loss: 336401280.0000\n",
            "Epoch 143/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1207665152.0000 - val_loss: 334705696.0000\n",
            "Epoch 144/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1203986304.0000 - val_loss: 333015072.0000\n",
            "Epoch 145/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1202087552.0000 - val_loss: 331308256.0000\n",
            "Epoch 146/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1197863680.0000 - val_loss: 329599968.0000\n",
            "Epoch 147/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1195691648.0000 - val_loss: 327888832.0000\n",
            "Epoch 148/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1189362048.0000 - val_loss: 326168608.0000\n",
            "Epoch 149/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 1187724032.0000 - val_loss: 324438496.0000\n",
            "Epoch 150/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1185397248.0000 - val_loss: 322700512.0000\n",
            "Epoch 151/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1184884864.0000 - val_loss: 320965728.0000\n",
            "Epoch 152/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1183069696.0000 - val_loss: 319236416.0000\n",
            "Epoch 153/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1176089472.0000 - val_loss: 317504832.0000\n",
            "Epoch 154/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1173824640.0000 - val_loss: 315770592.0000\n",
            "Epoch 155/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1169475456.0000 - val_loss: 314027072.0000\n",
            "Epoch 156/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 1166817152.0000 - val_loss: 312279840.0000\n",
            "Epoch 157/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1162202624.0000 - val_loss: 310536960.0000\n",
            "Epoch 158/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1160466944.0000 - val_loss: 308788864.0000\n",
            "Epoch 159/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1153602688.0000 - val_loss: 307021696.0000\n",
            "Epoch 160/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1152909824.0000 - val_loss: 305257888.0000\n",
            "Epoch 161/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1151498496.0000 - val_loss: 303499200.0000\n",
            "Epoch 162/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1145916160.0000 - val_loss: 301731648.0000\n",
            "Epoch 163/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1142508032.0000 - val_loss: 299954400.0000\n",
            "Epoch 164/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1139668864.0000 - val_loss: 298175584.0000\n",
            "Epoch 165/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1137835136.0000 - val_loss: 296399776.0000\n",
            "Epoch 166/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 1137333376.0000 - val_loss: 294632032.0000\n",
            "Epoch 167/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1130301056.0000 - val_loss: 292860032.0000\n",
            "Epoch 168/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1123830144.0000 - val_loss: 291074240.0000\n",
            "Epoch 169/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1122767360.0000 - val_loss: 289290240.0000\n",
            "Epoch 170/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1118460544.0000 - val_loss: 287503264.0000\n",
            "Epoch 171/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1117796224.0000 - val_loss: 285713376.0000\n",
            "Epoch 172/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1110764032.0000 - val_loss: 283924512.0000\n",
            "Epoch 173/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1106289280.0000 - val_loss: 282123104.0000\n",
            "Epoch 174/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 1102759936.0000 - val_loss: 280324640.0000\n",
            "Epoch 175/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1103974656.0000 - val_loss: 278524512.0000\n",
            "Epoch 176/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 1096865152.0000 - val_loss: 276719872.0000\n",
            "Epoch 177/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1098625408.0000 - val_loss: 274918880.0000\n",
            "Epoch 178/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1091232000.0000 - val_loss: 273118240.0000\n",
            "Epoch 179/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 1086036992.0000 - val_loss: 271307904.0000\n",
            "Epoch 180/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1088985856.0000 - val_loss: 269502560.0000\n",
            "Epoch 181/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1085790336.0000 - val_loss: 267702352.0000\n",
            "Epoch 182/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 1074695424.0000 - val_loss: 265898912.0000\n",
            "Epoch 183/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1073131200.0000 - val_loss: 264085696.0000\n",
            "Epoch 184/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 1073113216.0000 - val_loss: 262283264.0000\n",
            "Epoch 185/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 1068871424.0000 - val_loss: 260478928.0000\n",
            "Epoch 186/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 1063225728.0000 - val_loss: 258672208.0000\n",
            "Epoch 187/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 1058752576.0000 - val_loss: 256856112.0000\n",
            "Epoch 188/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 1054238912.0000 - val_loss: 255041888.0000\n",
            "Epoch 189/1000\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 1053105088.0000 - val_loss: 253225536.0000\n",
            "Epoch 190/1000\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 1046390336.0000 - val_loss: 251408752.0000\n",
            "Epoch 191/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 1044724928.0000 - val_loss: 249595376.0000\n",
            "Epoch 192/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 1040441536.0000 - val_loss: 247770912.0000\n",
            "Epoch 193/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 1037894336.0000 - val_loss: 245944544.0000\n",
            "Epoch 194/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 1038175936.0000 - val_loss: 244129168.0000\n",
            "Epoch 195/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 1034571456.0000 - val_loss: 242318688.0000\n",
            "Epoch 196/1000\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 1030072704.0000 - val_loss: 240515968.0000\n",
            "Epoch 197/1000\n",
            "4/4 [==============================] - 0s 52ms/step - loss: 1023595328.0000 - val_loss: 238712672.0000\n",
            "Epoch 198/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 1014223296.0000 - val_loss: 236900928.0000\n",
            "Epoch 199/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 1013677184.0000 - val_loss: 235082288.0000\n",
            "Epoch 200/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 1013601280.0000 - val_loss: 233270112.0000\n",
            "Epoch 201/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 1010622336.0000 - val_loss: 231461280.0000\n",
            "Epoch 202/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 999643776.0000 - val_loss: 229657632.0000\n",
            "Epoch 203/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 998317888.0000 - val_loss: 227847200.0000\n",
            "Epoch 204/1000\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 997015296.0000 - val_loss: 226036592.0000\n",
            "Epoch 205/1000\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 994402240.0000 - val_loss: 224224592.0000\n",
            "Epoch 206/1000\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 991369344.0000 - val_loss: 222422064.0000\n",
            "Epoch 207/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 987718272.0000 - val_loss: 220620608.0000\n",
            "Epoch 208/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 983851328.0000 - val_loss: 218815920.0000\n",
            "Epoch 209/1000\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 975568384.0000 - val_loss: 217013648.0000\n",
            "Epoch 210/1000\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 974097344.0000 - val_loss: 215215328.0000\n",
            "Epoch 211/1000\n",
            "4/4 [==============================] - 0s 53ms/step - loss: 968851904.0000 - val_loss: 213418544.0000\n",
            "Epoch 212/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 965868352.0000 - val_loss: 211618688.0000\n",
            "Epoch 213/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 960195328.0000 - val_loss: 209828944.0000\n",
            "Epoch 214/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 956494400.0000 - val_loss: 208034448.0000\n",
            "Epoch 215/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 955256064.0000 - val_loss: 206243728.0000\n",
            "Epoch 216/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 949480832.0000 - val_loss: 204457952.0000\n",
            "Epoch 217/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 951224960.0000 - val_loss: 202678992.0000\n",
            "Epoch 218/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 944158848.0000 - val_loss: 200895920.0000\n",
            "Epoch 219/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 939204672.0000 - val_loss: 199117792.0000\n",
            "Epoch 220/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 942486080.0000 - val_loss: 197347136.0000\n",
            "Epoch 221/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 931439552.0000 - val_loss: 195579152.0000\n",
            "Epoch 222/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 929162880.0000 - val_loss: 193810768.0000\n",
            "Epoch 223/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 926820416.0000 - val_loss: 192053312.0000\n",
            "Epoch 224/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 920681792.0000 - val_loss: 190300992.0000\n",
            "Epoch 225/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 919555072.0000 - val_loss: 188546640.0000\n",
            "Epoch 226/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 916498752.0000 - val_loss: 186793328.0000\n",
            "Epoch 227/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 910167104.0000 - val_loss: 185043792.0000\n",
            "Epoch 228/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 898778560.0000 - val_loss: 183293680.0000\n",
            "Epoch 229/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 904976576.0000 - val_loss: 181543376.0000\n",
            "Epoch 230/1000\n",
            "4/4 [==============================] - 0s 44ms/step - loss: 902046208.0000 - val_loss: 179806304.0000\n",
            "Epoch 231/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 893057472.0000 - val_loss: 178072480.0000\n",
            "Epoch 232/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 894918720.0000 - val_loss: 176342080.0000\n",
            "Epoch 233/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 890567744.0000 - val_loss: 174617264.0000\n",
            "Epoch 234/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 884244672.0000 - val_loss: 172902768.0000\n",
            "Epoch 235/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 880064512.0000 - val_loss: 171189536.0000\n",
            "Epoch 236/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 876035904.0000 - val_loss: 169474416.0000\n",
            "Epoch 237/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 868442112.0000 - val_loss: 167764192.0000\n",
            "Epoch 238/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 866307904.0000 - val_loss: 166056656.0000\n",
            "Epoch 239/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 859438336.0000 - val_loss: 164360624.0000\n",
            "Epoch 240/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 861683520.0000 - val_loss: 162666704.0000\n",
            "Epoch 241/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 858375744.0000 - val_loss: 160974768.0000\n",
            "Epoch 242/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 858455360.0000 - val_loss: 159297008.0000\n",
            "Epoch 243/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 845343744.0000 - val_loss: 157626176.0000\n",
            "Epoch 244/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 845753472.0000 - val_loss: 155949184.0000\n",
            "Epoch 245/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 839175232.0000 - val_loss: 154280768.0000\n",
            "Epoch 246/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 836621312.0000 - val_loss: 152621392.0000\n",
            "Epoch 247/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 838070720.0000 - val_loss: 150968768.0000\n",
            "Epoch 248/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 831326144.0000 - val_loss: 149318352.0000\n",
            "Epoch 249/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 823947392.0000 - val_loss: 147680560.0000\n",
            "Epoch 250/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 826550656.0000 - val_loss: 146047936.0000\n",
            "Epoch 251/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 824094080.0000 - val_loss: 144419120.0000\n",
            "Epoch 252/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 821779456.0000 - val_loss: 142797232.0000\n",
            "Epoch 253/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 815661760.0000 - val_loss: 141185808.0000\n",
            "Epoch 254/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 810901760.0000 - val_loss: 139577616.0000\n",
            "Epoch 255/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 805967232.0000 - val_loss: 137976144.0000\n",
            "Epoch 256/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 797570752.0000 - val_loss: 136371136.0000\n",
            "Epoch 257/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 799579008.0000 - val_loss: 134782608.0000\n",
            "Epoch 258/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 791870528.0000 - val_loss: 133194976.0000\n",
            "Epoch 259/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 792854912.0000 - val_loss: 131622280.0000\n",
            "Epoch 260/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 787761536.0000 - val_loss: 130052616.0000\n",
            "Epoch 261/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 782452480.0000 - val_loss: 128482576.0000\n",
            "Epoch 262/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 772879552.0000 - val_loss: 126924040.0000\n",
            "Epoch 263/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 777810240.0000 - val_loss: 125374888.0000\n",
            "Epoch 264/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 776527104.0000 - val_loss: 123836704.0000\n",
            "Epoch 265/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 765478080.0000 - val_loss: 122293952.0000\n",
            "Epoch 266/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 762144960.0000 - val_loss: 120771056.0000\n",
            "Epoch 267/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 754969152.0000 - val_loss: 119242664.0000\n",
            "Epoch 268/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 759028736.0000 - val_loss: 117724352.0000\n",
            "Epoch 269/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 751638016.0000 - val_loss: 116211416.0000\n",
            "Epoch 270/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 751841536.0000 - val_loss: 114708560.0000\n",
            "Epoch 271/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 745669568.0000 - val_loss: 113213424.0000\n",
            "Epoch 272/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 741569536.0000 - val_loss: 111725104.0000\n",
            "Epoch 273/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 733993408.0000 - val_loss: 110246960.0000\n",
            "Epoch 274/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 738073216.0000 - val_loss: 108777816.0000\n",
            "Epoch 275/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 729727232.0000 - val_loss: 107314440.0000\n",
            "Epoch 276/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 724291776.0000 - val_loss: 105856200.0000\n",
            "Epoch 277/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 718240576.0000 - val_loss: 104395736.0000\n",
            "Epoch 278/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 720467328.0000 - val_loss: 102959560.0000\n",
            "Epoch 279/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 718707904.0000 - val_loss: 101528016.0000\n",
            "Epoch 280/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 710353152.0000 - val_loss: 100112928.0000\n",
            "Epoch 281/1000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 712673408.0000 - val_loss: 98694256.0000\n",
            "Epoch 282/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 707207360.0000 - val_loss: 97288920.0000\n",
            "Epoch 283/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 694695744.0000 - val_loss: 95892824.0000\n",
            "Epoch 284/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 695901568.0000 - val_loss: 94502336.0000\n",
            "Epoch 285/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 692619136.0000 - val_loss: 93118936.0000\n",
            "Epoch 286/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 688237184.0000 - val_loss: 91741560.0000\n",
            "Epoch 287/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 690374656.0000 - val_loss: 90379696.0000\n",
            "Epoch 288/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 680149440.0000 - val_loss: 89018696.0000\n",
            "Epoch 289/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 678122752.0000 - val_loss: 87669712.0000\n",
            "Epoch 290/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 677067648.0000 - val_loss: 86320936.0000\n",
            "Epoch 291/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 670234112.0000 - val_loss: 84992416.0000\n",
            "Epoch 292/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 663222272.0000 - val_loss: 83668976.0000\n",
            "Epoch 293/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 670577024.0000 - val_loss: 82355792.0000\n",
            "Epoch 294/1000\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 663178240.0000 - val_loss: 81057104.0000\n",
            "Epoch 295/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 658927488.0000 - val_loss: 79761080.0000\n",
            "Epoch 296/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 651249792.0000 - val_loss: 78474904.0000\n",
            "Epoch 297/1000\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 651482112.0000 - val_loss: 77197920.0000\n",
            "Epoch 298/1000\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 645658176.0000 - val_loss: 75924064.0000\n",
            "Epoch 299/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 642836288.0000 - val_loss: 74670512.0000\n",
            "Epoch 300/1000\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 633502848.0000 - val_loss: 73428216.0000\n",
            "Epoch 301/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 637822400.0000 - val_loss: 72190136.0000\n",
            "Epoch 302/1000\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 628202240.0000 - val_loss: 70965712.0000\n",
            "Epoch 303/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 621486144.0000 - val_loss: 69753176.0000\n",
            "Epoch 304/1000\n",
            "4/4 [==============================] - 0s 59ms/step - loss: 622584960.0000 - val_loss: 68545504.0000\n",
            "Epoch 305/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 624213568.0000 - val_loss: 67346584.0000\n",
            "Epoch 306/1000\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 610338176.0000 - val_loss: 66159588.0000\n",
            "Epoch 307/1000\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 608150336.0000 - val_loss: 64980232.0000\n",
            "Epoch 308/1000\n",
            "4/4 [==============================] - 0s 51ms/step - loss: 611967168.0000 - val_loss: 63807728.0000\n",
            "Epoch 309/1000\n",
            "4/4 [==============================] - 0s 54ms/step - loss: 607217664.0000 - val_loss: 62644952.0000\n",
            "Epoch 310/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 596571456.0000 - val_loss: 61498068.0000\n",
            "Epoch 311/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 595982080.0000 - val_loss: 60356552.0000\n",
            "Epoch 312/1000\n",
            "4/4 [==============================] - 0s 50ms/step - loss: 600256704.0000 - val_loss: 59229568.0000\n",
            "Epoch 313/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 590961408.0000 - val_loss: 58108408.0000\n",
            "Epoch 314/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 590664512.0000 - val_loss: 57001636.0000\n",
            "Epoch 315/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 588293248.0000 - val_loss: 55902892.0000\n",
            "Epoch 316/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 582802432.0000 - val_loss: 54813660.0000\n",
            "Epoch 317/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 581442496.0000 - val_loss: 53733888.0000\n",
            "Epoch 318/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 572131072.0000 - val_loss: 52664904.0000\n",
            "Epoch 319/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 571462272.0000 - val_loss: 51606652.0000\n",
            "Epoch 320/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 570540800.0000 - val_loss: 50556888.0000\n",
            "Epoch 321/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 561960768.0000 - val_loss: 49522620.0000\n",
            "Epoch 322/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 563911104.0000 - val_loss: 48497072.0000\n",
            "Epoch 323/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 556448192.0000 - val_loss: 47482684.0000\n",
            "Epoch 324/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 561013248.0000 - val_loss: 46479600.0000\n",
            "Epoch 325/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 551180160.0000 - val_loss: 45480804.0000\n",
            "Epoch 326/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 548329408.0000 - val_loss: 44500280.0000\n",
            "Epoch 327/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 546063296.0000 - val_loss: 43529052.0000\n",
            "Epoch 328/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 547564416.0000 - val_loss: 42566632.0000\n",
            "Epoch 329/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 535890848.0000 - val_loss: 41614600.0000\n",
            "Epoch 330/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 532521728.0000 - val_loss: 40674280.0000\n",
            "Epoch 331/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 533494976.0000 - val_loss: 39747104.0000\n",
            "Epoch 332/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 530617248.0000 - val_loss: 38830964.0000\n",
            "Epoch 333/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 526204448.0000 - val_loss: 37923220.0000\n",
            "Epoch 334/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 530112608.0000 - val_loss: 37026572.0000\n",
            "Epoch 335/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 516362880.0000 - val_loss: 36141684.0000\n",
            "Epoch 336/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 515285952.0000 - val_loss: 35270220.0000\n",
            "Epoch 337/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 510116032.0000 - val_loss: 34400504.0000\n",
            "Epoch 338/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 508307040.0000 - val_loss: 33547998.0000\n",
            "Epoch 339/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 502657248.0000 - val_loss: 32707052.0000\n",
            "Epoch 340/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 504934848.0000 - val_loss: 31875714.0000\n",
            "Epoch 341/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 493914880.0000 - val_loss: 31051758.0000\n",
            "Epoch 342/1000\n",
            "4/4 [==============================] - 0s 40ms/step - loss: 492959488.0000 - val_loss: 30243832.0000\n",
            "Epoch 343/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 496064992.0000 - val_loss: 29452082.0000\n",
            "Epoch 344/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 488422816.0000 - val_loss: 28666326.0000\n",
            "Epoch 345/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 489500704.0000 - val_loss: 27896780.0000\n",
            "Epoch 346/1000\n",
            "4/4 [==============================] - 0s 38ms/step - loss: 482484768.0000 - val_loss: 27136938.0000\n",
            "Epoch 347/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 484425632.0000 - val_loss: 26385502.0000\n",
            "Epoch 348/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 479329056.0000 - val_loss: 25646594.0000\n",
            "Epoch 349/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 480295776.0000 - val_loss: 24917286.0000\n",
            "Epoch 350/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 469382176.0000 - val_loss: 24203678.0000\n",
            "Epoch 351/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 468440416.0000 - val_loss: 23499732.0000\n",
            "Epoch 352/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 456446048.0000 - val_loss: 22803340.0000\n",
            "Epoch 353/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 472012672.0000 - val_loss: 22120504.0000\n",
            "Epoch 354/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 465370176.0000 - val_loss: 21447312.0000\n",
            "Epoch 355/1000\n",
            "4/4 [==============================] - 0s 92ms/step - loss: 451924480.0000 - val_loss: 20789510.0000\n",
            "Epoch 356/1000\n",
            "4/4 [==============================] - 0s 96ms/step - loss: 458816128.0000 - val_loss: 20142682.0000\n",
            "Epoch 357/1000\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 445328768.0000 - val_loss: 19502848.0000\n",
            "Epoch 358/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 435665952.0000 - val_loss: 18877300.0000\n",
            "Epoch 359/1000\n",
            "4/4 [==============================] - 0s 39ms/step - loss: 442641664.0000 - val_loss: 18258308.0000\n",
            "Epoch 360/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 440859072.0000 - val_loss: 17656758.0000\n",
            "Epoch 361/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 435653280.0000 - val_loss: 17065378.0000\n",
            "Epoch 362/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 436608384.0000 - val_loss: 16482940.0000\n",
            "Epoch 363/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 430136928.0000 - val_loss: 15914090.0000\n",
            "Epoch 364/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 433013728.0000 - val_loss: 15356419.0000\n",
            "Epoch 365/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 430030912.0000 - val_loss: 14810083.0000\n",
            "Epoch 366/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 414875296.0000 - val_loss: 14277544.0000\n",
            "Epoch 367/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 419737376.0000 - val_loss: 13754636.0000\n",
            "Epoch 368/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 419700480.0000 - val_loss: 13242637.0000\n",
            "Epoch 369/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 413105440.0000 - val_loss: 12746543.0000\n",
            "Epoch 370/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 416795008.0000 - val_loss: 12258441.0000\n",
            "Epoch 371/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 407804736.0000 - val_loss: 11781980.0000\n",
            "Epoch 372/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 408981088.0000 - val_loss: 11317207.0000\n",
            "Epoch 373/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 400856352.0000 - val_loss: 10861148.0000\n",
            "Epoch 374/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 392567904.0000 - val_loss: 10420581.0000\n",
            "Epoch 375/1000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 393416512.0000 - val_loss: 9990901.0000\n",
            "Epoch 376/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 394817856.0000 - val_loss: 9573564.0000\n",
            "Epoch 377/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 397100224.0000 - val_loss: 9165662.0000\n",
            "Epoch 378/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 383941728.0000 - val_loss: 8765529.0000\n",
            "Epoch 379/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 384586784.0000 - val_loss: 8380430.5000\n",
            "Epoch 380/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 388196960.0000 - val_loss: 8008542.0000\n",
            "Epoch 381/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 377807552.0000 - val_loss: 7643903.5000\n",
            "Epoch 382/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 377804480.0000 - val_loss: 7294230.5000\n",
            "Epoch 383/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 372824352.0000 - val_loss: 6956305.5000\n",
            "Epoch 384/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 380170208.0000 - val_loss: 6631440.5000\n",
            "Epoch 385/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 374933472.0000 - val_loss: 6316848.0000\n",
            "Epoch 386/1000\n",
            "4/4 [==============================] - 0s 37ms/step - loss: 365562112.0000 - val_loss: 6011937.0000\n",
            "Epoch 387/1000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 366488928.0000 - val_loss: 5720273.5000\n",
            "Epoch 388/1000\n",
            "4/4 [==============================] - 0s 34ms/step - loss: 365544512.0000 - val_loss: 5437940.5000\n",
            "Epoch 389/1000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 357805632.0000 - val_loss: 5166976.0000\n",
            "Epoch 390/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 355333344.0000 - val_loss: 4906460.0000\n",
            "Epoch 391/1000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 352134688.0000 - val_loss: 4657884.5000\n",
            "Epoch 392/1000\n",
            "4/4 [==============================] - 0s 42ms/step - loss: 353222720.0000 - val_loss: 4421914.5000\n",
            "Epoch 393/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 347846752.0000 - val_loss: 4195751.5000\n",
            "Epoch 394/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 354601952.0000 - val_loss: 3981695.7500\n",
            "Epoch 395/1000\n",
            "4/4 [==============================] - 0s 47ms/step - loss: 340251136.0000 - val_loss: 3777606.2500\n",
            "Epoch 396/1000\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 348785088.0000 - val_loss: 3584429.0000\n",
            "Epoch 397/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 337438016.0000 - val_loss: 3402840.7500\n",
            "Epoch 398/1000\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 341074048.0000 - val_loss: 3232434.0000\n",
            "Epoch 399/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 342264672.0000 - val_loss: 3072950.0000\n",
            "Epoch 400/1000\n",
            "4/4 [==============================] - 0s 56ms/step - loss: 331743328.0000 - val_loss: 2926043.0000\n",
            "Epoch 401/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 331471456.0000 - val_loss: 2790341.2500\n",
            "Epoch 402/1000\n",
            "4/4 [==============================] - 0s 49ms/step - loss: 319620448.0000 - val_loss: 2664936.0000\n",
            "Epoch 403/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 324783616.0000 - val_loss: 2551109.0000\n",
            "Epoch 404/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 323967936.0000 - val_loss: 2448101.5000\n",
            "Epoch 405/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 323633600.0000 - val_loss: 2356447.2500\n",
            "Epoch 406/1000\n",
            "4/4 [==============================] - 0s 58ms/step - loss: 318601248.0000 - val_loss: 2275760.2500\n",
            "Epoch 407/1000\n",
            "4/4 [==============================] - 0s 48ms/step - loss: 306217728.0000 - val_loss: 2206397.0000\n",
            "Epoch 408/1000\n",
            "4/4 [==============================] - 0s 46ms/step - loss: 309296192.0000 - val_loss: 2147172.7500\n",
            "Epoch 409/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 307436832.0000 - val_loss: 2098916.2500\n",
            "Epoch 410/1000\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 309865312.0000 - val_loss: 2061534.2500\n",
            "Epoch 411/1000\n",
            "4/4 [==============================] - 0s 45ms/step - loss: 305294624.0000 - val_loss: 2034724.8750\n",
            "Epoch 412/1000\n",
            "4/4 [==============================] - 0s 57ms/step - loss: 304087488.0000 - val_loss: 2018571.1250\n",
            "Epoch 413/1000\n",
            "4/4 [==============================] - 0s 43ms/step - loss: 300667520.0000 - val_loss: 2013263.7500\n",
            "Epoch 414/1000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 298963840.0000 - val_loss: 2018523.1250\n",
            "Epoch 415/1000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 296639360.0000 - val_loss: 2034435.5000\n",
            "Epoch 416/1000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 289746464.0000 - val_loss: 2061021.8750\n",
            "Epoch 417/1000\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 299630816.0000 - val_loss: 2098436.5000\n",
            "Epoch 418/1000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 294646688.0000 - val_loss: 2146393.0000\n",
            "Epoch 419/1000\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 283888064.0000 - val_loss: 2204683.2500\n",
            "Epoch 420/1000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 283007872.0000 - val_loss: 2273556.7500\n",
            "Epoch 421/1000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 280467904.0000 - val_loss: 2352879.0000\n",
            "Epoch 422/1000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 278513504.0000 - val_loss: 2442994.5000\n",
            "Epoch 423/1000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 281582560.0000 - val_loss: 2544080.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load the best model\n",
        "model = load_model('best_model.h5')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss = model.evaluate(X_test, y_test)\n",
        "print('Test Loss:', test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7FJd-Lj8yW9",
        "outputId": "b43c8da4-3835-4b22-f72b-b4f263a93a91"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 14ms/step - loss: 4539013.0000\n",
            "Test Loss: 4539013.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the training, validation, and test loss\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(test_loss)\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation', 'Test'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "LEQRzjePduvT",
        "outputId": "8dbabe4b-52e7-4f57-c3a1-13b3829946f6"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIjCAYAAAA0vUuxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACIZklEQVR4nOzdd3hUZfrG8e/MJJn0RnohoYQOoYeiAoqiICo2rKBY1roq+ltFV2yr2JdVLLuugroibQVdQVAQlCpdQSHUkBBSCel9Zn5/TEiMQGghJ+X+XNe5JnPKzDMYIXfe9zyvyeFwOBAREREREZETMhtdgIiIiIiISGOn4CQiIiIiInISCk4iIiIiIiInoeAkIiIiIiJyEgpOIiIiIiIiJ6HgJCIiIiIichIKTiIiIiIiIieh4CQiIiIiInISCk4iIiIiIiInoeAkIiLNhslk4tlnnz3t65KSkjCZTMyYMaPO81asWIHJZGLFihVnVJ+IiDRdCk4iIlKvZsyYgclkwmQysWrVqmOOOxwOoqOjMZlMXH755QZUKCIicvoUnERE5Jxwd3dn5syZx+z/4YcfOHjwIFar1YCqREREzoyCk4iInBMjR45k7ty5VFZW1to/c+ZM+vTpQ1hYmEGViYiInD4FJxEROSduvPFGDh8+zHfffVe9r7y8nHnz5nHTTTcd95qioiIeffRRoqOjsVqtdOzYkddffx2Hw1HrvLKyMh555BGCg4Px8fHhiiuu4ODBg8d9zdTUVCZMmEBoaChWq5WuXbvy0Ucf1d8HBebOnUufPn3w8PAgKCiIW265hdTU1FrnpKenc/vttxMVFYXVaiU8PJwrr7ySpKSk6nM2btzIiBEjCAoKwsPDgzZt2jBhwoR6rVVERM6Mi9EFiIhI8xQbG8vAgQP5/PPPueyyywD45ptvyMvL44YbbuCtt96qdb7D4eCKK65g+fLl3HHHHfTs2ZMlS5bwf//3f6SmpvL3v/+9+tw777yT//znP9x0000MGjSI77//nlGjRh1TQ0ZGBgMGDMBkMvHAAw8QHBzMN998wx133EF+fj4PP/zwWX/OGTNmcPvtt9OvXz+mTJlCRkYG//jHP1i9ejVbtmzB398fgGuuuYZff/2VBx98kNjYWDIzM/nuu+9ITk6ufn7JJZcQHBzME088gb+/P0lJSXzxxRdnXaOIiNQDh4iISD2aPn26A3Bs2LDBMW3aNIePj4+juLjY4XA4HNddd51j2LBhDofD4YiJiXGMGjWq+roFCxY4AMff/va3Wq937bXXOkwmk2PPnj0Oh8Ph2Lp1qwNw3HfffbXOu+mmmxyA45lnnqned8cddzjCw8Md2dnZtc694YYbHH5+ftV17d+/3wE4pk+fXudnW758uQNwLF++3OFwOBzl5eWOkJAQR7du3RwlJSXV53399dcOwDF58mSHw+FwHDlyxAE4XnvttRO+9vz586v/3EREpPHRVD0RETlnrr/+ekpKSvj6668pKCjg66+/PuE0vUWLFmGxWPjzn/9ca/+jjz6Kw+Hgm2++qT4POOa8P44eORwO/vvf/zJ69GgcDgfZ2dnV24gRI8jLy2Pz5s1n9fk2btxIZmYm9913H+7u7tX7R40aRadOnVi4cCEAHh4euLm5sWLFCo4cOXLc1zo6MvX1119TUVFxVnWJiEj9a9HB6ccff2T06NFERERgMplYsGDBab/GnDlz6NmzJ56ensTExPDaa6/Vf6EiIk1UcHAww4cPZ+bMmXzxxRfYbDauvfba45574MABIiIi8PHxqbW/c+fO1cePPprNZtq1a1frvI4dO9Z6npWVRW5uLv/6178IDg6utd1+++0AZGZmntXnO1rTH98boFOnTtXHrVYrr7zyCt988w2hoaFccMEFvPrqq6Snp1efP2TIEK655hqee+45goKCuPLKK5k+fTplZWVnVaOIiNSPFn2PU1FREfHx8UyYMIGrr776tK//5ptvuPnmm3n77be55JJL2LFjB3fddRceHh488MAD56BiEZGm56abbuKuu+4iPT2dyy67rHpk5Vyz2+0A3HLLLYwfP/645/To0aNBagHniNjo0aNZsGABS5Ys4emnn2bKlCl8//339OrVC5PJxLx581i3bh3/+9//WLJkCRMmTOCNN95g3bp1eHt7N1itIiJyrBY94nTZZZfxt7/9jTFjxhz3eFlZGY899hiRkZF4eXmRkJBQa7X4Tz/9lKuuuop77rmHtm3bMmrUKCZNmsQrr7xyTAcoEZGWasyYMZjNZtatW3fCaXoAMTExHDp0iIKCglr7d+7cWX386KPdbmfv3r21zktMTKz1/GjHPZvNxvDhw4+7hYSEnNVnO1rTH9/76L6jx49q164djz76KN9++y3bt2+nvLycN954o9Y5AwYM4MUXX2Tjxo189tln/Prrr8yaNeus6hQRkbPXooPTyTzwwAOsXbuWWbNm8csvv3Dddddx6aWXsnv3bsAZrH4/px2c89gPHjxYPT1DRKSl8/b25r333uPZZ59l9OjRJzxv5MiR2Gw2pk2bVmv/3//+d0wmU3VnvqOPf+zKN3Xq1FrPLRYL11xzDf/973/Zvn37Me+XlZV1Jh+nlr59+xISEsL7779fa0rdN998w44dO6o7/RUXF1NaWlrr2nbt2uHj41N93ZEjR475pVvPnj0BNF1PRKQRaNFT9eqSnJzM9OnTSU5OJiIiAoDHHnuMxYsXM336dF566SVGjBjBI488wm233cawYcPYs2dP9W8O09LSiI2NNfATiIg0HieaKvd7o0ePZtiwYTz11FMkJSURHx/Pt99+y5dffsnDDz9cfU9Tz549ufHGG3n33XfJy8tj0KBBLFu2jD179hzzmi+//DLLly8nISGBu+66iy5dupCTk8PmzZtZunQpOTk5Z/W5XF1deeWVV7j99tsZMmQIN954Y3U78tjYWB555BEAdu3axUUXXcT1119Ply5dcHFxYf78+WRkZHDDDTcA8PHHH/Puu+8yZswY2rVrR0FBAR988AG+vr6MHDnyrOoUEZGzp+B0Atu2bcNms9GhQ4da+8vKymjVqhUAd911F3v37uXyyy+noqICX19fHnroIZ599lnMZg3miYicDrPZzFdffcXkyZOZPXs206dPJzY2ltdee41HH3201rkfffQRwcHBfPbZZyxYsIALL7yQhQsXEh0dXeu80NBQ1q9fz/PPP88XX3zBu+++S6tWrejatSuvvPJKvdR922234enpycsvv8zjjz+Ol5cXY8aM4ZVXXqm+nys6Opobb7yRZcuW8emnn+Li4kKnTp2YM2cO11xzDeBsDrF+/XpmzZpFRkYGfn5+9O/fn88++4w2bdrUS60iInLmTA7djAOAyWRi/vz5XHXVVQDMnj2bm2++mV9//RWLxVLrXG9vb8LCwqqf22w20tPTCQ4OZtmyZYwcOZLMzEyCg4Mb8iOIiIiIiMg5ohGnE+jVqxc2m43MzEzOP//8Os+1WCxERkYC8PnnnzNw4ECFJhERERGRZqRFB6fCwsJac+L379/P1q1bCQwMpEOHDtx8882MGzeON954g169epGVlcWyZcvo0aMHo0aNIjs7m3nz5jF06FBKS0uZPn06c+fO5YcffjDwU4mIiIiISH1r0VP1VqxYwbBhw47ZP378eGbMmEFFRQV/+9vf+OSTT0hNTSUoKIgBAwbw3HPP0b17d7Kzsxk9ejTbtm3D4XAwcOBAXnzxRRISEgz4NCIiIiIicq606OAkIiIiIiJyKtT6TURERERE5CQUnERERERERE6ixTWHsNvtHDp0CB8fH0wmk9HliIiIiIiIQRwOBwUFBURERJx0HdYWF5wOHTp0zAKJIiIiIiLScqWkpBAVFVXnOS0uOPn4+ADOPxxfX1+DqxEREREREaPk5+cTHR1dnRHq0uKC09Hpeb6+vgpOIiIiIiJySrfwqDmEiIiIiIjISSg4iYiIiIiInISCk4iIiIiIyEm0uHucREREREROxOFwUFlZic1mM7oUqSeurq5YLJazfh0FJxERERERoLy8nLS0NIqLi40uReqRyWQiKioKb2/vs3odBScRERERafHsdjv79+/HYrEQERGBm5vbKXVak8bN4XCQlZXFwYMHiYuLO6uRJwUnEREREWnxysvLsdvtREdH4+npaXQ5Uo+Cg4NJSkqioqLirIKTmkOIiIiIiFQxm/XjcXNTXyOH+s4QERERERE5CQUnERERERGRk1BwEhERERGRWmJjY5k6darRZTQqCk4iIiIiIk2UyWSqc3v22WfP6HU3bNjA3XffXb/FNnHqqiciIiIi0kSlpaVVfz179mwmT55MYmJi9b7fr13kcDiw2Wy4uJw8AgQHB9dvoc2ARpxERERERI7D4XBQXF5pyOZwOE6pxrCwsOrNz88Pk8lU/Xznzp34+PjwzTff0KdPH6xWK6tWrWLv3r1ceeWVhIaG4u3tTb9+/Vi6dGmt1/3jVD2TycS///1vxowZg6enJ3FxcXz11Vf1+cfd6GnESURERETkOEoqbHSZvMSQ9/7t+RF4utXPj+pPPPEEr7/+Om3btiUgIICUlBRGjhzJiy++iNVq5ZNPPmH06NEkJibSunXrE77Oc889x6uvvsprr73G22+/zc0338yBAwcIDAyslzobO404iYiIiIg0Y88//zwXX3wx7dq1IzAwkPj4eP70pz/RrVs34uLieOGFF2jXrt1JR5Buu+02brzxRtq3b89LL71EYWEh69evb6BPYTyNOBloZ3o+SdlFmE0mXCwm56PZjMVswtfDhQBPN/w9XfFwtdTbwl0iIiIicmo8XC389vwIw967vvTt27fW88LCQp599lkWLlxIWloalZWVlJSUkJycXOfr9OjRo/prLy8vfH19yczMrLc6GzsFJwPN35zKP3/cd9Lz3Cxm/Dxd8fdwxd/TlQBPNwK93AjwciPQs+rRq/Z+H6uLwpaIiIjIWTCZTPU2Xc5IXl5etZ4/9thjfPfdd7z++uu0b98eDw8Prr32WsrLy+t8HVdX11rPTSYTdru93uttrJr+d0ITFhXgQb/YACrtDux2B5V2Bza7gwqbnfzSSnKLy6mwOSi32ckqKCOroOyUX9vVYqoJUp5uhPpaCfPzINzPnTA/9+rHIC8rZrMCloiIiEhLsXr1am677TbGjBkDOEegkpKSjC2qCVBwMtCtA2O5dWDsCY87HA5KKmzkFlc4t5JyjhRVcKS4nCNF5eRUP1Y4H4vKOVJcTnG5jQqbg8yCMjJPErZcLSZCfNz/EKhqB6xgbysuFt0OJyIiItIcxMXF8cUXXzB69GhMJhNPP/10ixo5OlMKTo3Y0eFhTzcXIvw9Tvm60gobR4rLOVxYXv2YkV9KWl4p6XmlpOWXkp5XQmZBGRU2B6m5JaTmlpzw9cwmCPV1JzrQk9aBnsQEehIb5EWbqs3Lqm8jERERkabizTffZMKECQwaNIigoCAef/xx8vPzjS6r0TM5TrVJfDORn5+Pn58feXl5+Pr6Gl2OoSqqpgBWB6q8kt8Fq6otvxSbve5vkVBfK22CvGgb7E3b3wWq6EBPXDVSJSIiIk1AaWkp+/fvp02bNri7uxtdjtSjuv7bnk420FBBC+ZqMRPh71HnaJbN7uBwYRkpR0o4eKSY5MPFJB0uJulwEfuzi8gpKicjv4yM/DLW7cupda3FbKJ1oGd1kGof4k3HMB86hvpolEpEREREmhT99Cp1sphNhPi6E+LrTp+YgGOO5xaXsz+7qHrbl13E/izn1yUVtur9fxQd6EHbIG86h/vSv00AXSP8CPGxqhOgiIiIiDRKCk5yVvw93ejV2o1erWuHKofDQUZ+GfuyC52BKquIXRkFJKYXkFlQRkpOCSk5JfywK4v3f3Be4+vuQlyoD3Eh3sSF+tA90o8eUX641+M6BiIiIiIiZ0LBSc4Jk8lEWFVnvkHtgmodyykqZ3dGAXuzivg5JZcNB3JIyi4iv7SSTQeOsOnAkepzXcwmOof70ru1P71aB9CrtT+tAz01MiUiIiIiDUrBSRpcoJcbCW1bkdC2FTcltAagrNI5rW9XRiF7MgrYkV7A1pRcsgrK2Jaax7bUPD5eewCAVl5u9GrtT9cIP9oGe9E3NpDI0+g6KCIiIiJyuhScpFGwuljoFOZLp7CabiYOh4NDeaVsPnCELcm5bE4+wq+H8jhcVM7SHZks3ZFZfW5ciDdDOgTTJyaAyAAPOoX54uaijn4iIiIiUj8UnKTRMplMRPp7EOnvwej4CMC5RtVvaflsPnCE3RmF7MwoYNvBXHZnFrI7s5B/r9oPgI/VhfM7BNE+xHnPVK/W/kQFeBr5cURERESkCVNwkibF3dVC79YB9P5dM4q84gpW7cnmh12Z7Moo5MDhIo4UV7BoWzqQXn1eh1BvLusWTo8oP7pH+RHiozUaREREROTUKDhJk+fn6cqoHuGM6hEOgN3uYEtKLuv353DgcBG/peXz26F8dmUUsitjd/V13SP9uLBTCBd1DqFbhB9msxpOiIiIiMjxGXoTyI8//sjo0aOJiIjAZDKxYMGCU7529erVuLi40LNnz3NWnzRNZrOJPjEB3Du0HS9f04OvHjiPTU9fzKvX9uCK+Ag6hvoAsC01j38s280V01aTMGUZj839mbkbU/jtUD6lFTaDP4WIiIhIwxg6dCgPP/xw9fPY2FimTp1a5zWn+7P7uX6dhmDoiFNRURHx8fFMmDCBq6+++pSvy83NZdy4cVx00UVkZGScwwqlufDzcOX6vtFc3zcagMyCUlYkZvH9jkxW7s4iq6CMeZsOMm/TQQBcLSaGdAjhql4RXNQpFA83rSUlIiIijc/o0aOpqKhg8eLFxxxbuXIlF1xwAT///DM9evQ45dfcsGEDXl5e9Vkmzz77LAsWLGDr1q219qelpREQEHD8ixoZQ4PTZZddxmWXXXba191zzz3cdNNNWCyWJpNQpXEJ8XGvDlJllTbW789hzd7DbEzKYVdGIXklFSzdkcHSHRl4W13oHRNAj0g/hnUKoVe0v6b1iYiISKNwxx13cM0113Dw4EGioqJqHZs+fTp9+/Y9rdAEEBwcXJ8l1iksLKzB3utsNbl+zdOnT2ffvn0888wzp3R+WVkZ+fn5tTaR37O6WDg/LpjHL+3E3HsGsXXyxSx5+ALuG9qOSH8PCssq+XFXFtOW7+Ga99Yw+JXvmbp0F7szCnA4HEaXLyIiIueKwwHlRcZsp/gzxuWXX05wcDAzZsyotb+wsJC5c+dy1VVXceONNxIZGYmnpyfdu3fn888/r/M1/zhVb/fu3VxwwQW4u7vTpUsXvvvuu2Ouefzxx+nQoQOenp60bduWp59+moqKCgBmzJjBc889x88//4zJZMJkMlXX+8epetu2bePCCy/Ew8ODVq1acffdd1NYWFh9/LbbbuOqq67i9ddfJzw8nFatWnH//fdXv9e51KSaQ+zevZsnnniClStX4uJyaqVPmTKF55577hxXJs2JyWSiY5gPf7m0E49d0pHth/L4+WAe6/fnsGJnJml5pUxdupupS3cT5G1lQNtABrRtxcB2rWgb5IXJpNEoERGRZqGiGF6KMOa9nzwEbiefLufi4sK4ceOYMWMGTz31VPXPIXPnzsVms3HLLbcwd+5cHn/8cXx9fVm4cCG33nor7dq1o3///id9fbvdztVXX01oaCg//fQTeXl5te6HOsrHx4cZM2YQERHBtm3buOuuu/Dx8eEvf/kLY8eOZfv27SxevJilS5cC4Ofnd8xrFBUVMWLECAYOHMiGDRvIzMzkzjvv5IEHHqgVDJcvX054eDjLly9nz549jB07lp49e3LXXXed9POcjSYTnGw2GzfddBPPPfccHTp0OOXrJk2axMSJE6uf5+fnEx0dfS5KlGbIbDbRI8qfHlH+3DoghtIKG0t+TWfuxoNsSMohu7CMr39J4+tf0gCIaeXJyO7hjOwWTrdIX4UoEREROecmTJjAa6+9xg8//MDQoUMB5yyta665hpiYGB577LHqcx988EGWLFnCnDlzTik4LV26lJ07d7JkyRIiIpwh8qWXXjrmdpu//vWv1V/Hxsby2GOPMWvWLP7yl7/g4eGBt7c3Li4udU7NmzlzJqWlpXzyySfV91hNmzaN0aNH88orrxAaGgpAQEAA06ZNw2Kx0KlTJ0aNGsWyZcsUnI4qKChg48aNbNmyhQceeABwJmCHw4GLiwvffvstF1544THXWa1WrFZrQ5crzZS7q4Ure0ZyZc9IyiptbE3OZd2+HNbuy2Zzci4HDhfz3oq9vLdiL60DPbmsexijuofTPdJPIUpERKSpcfV0jvwY9d6nqFOnTgwaNIiPPvqIoUOHsmfPHlauXMnzzz+PzWbjpZdeYs6cOaSmplJeXk5ZWRmenqf2+jt27CA6Oro6NAEMHDjwmPNmz57NW2+9xd69eyksLKSyshJfX99T/gxH3ys+Pr5WY4rBgwdjt9tJTEysDk5du3bFYqlp3BUeHs62bdtO673ORJMJTr6+vsf8gbz77rt8//33zJs3jzZt2hhUmbRUVhcLCW1bkdC2FQ8RR1FZJcsTM1m0LY3vd2aSnFPMP3/Yxz9/2Eekvwd9YwMY2LYVV/SMwNOtyfyvJyIi0nKZTKc0Xa4xuOOOO3jwwQd55513mD59Ou3atWPIkCG88sor/OMf/2Dq1Kl0794dLy8vHn74YcrLy+vtvdeuXcvNN9/Mc889x4gRI/Dz82PWrFm88cYb9fYev+fq6lrruclkwm63n5P3+j1Df3orLCxkz5491c/379/P1q1bCQwMpHXr1kyaNInU1FQ++eQTzGYz3bp1q3V9SEgI7u7ux+wXMYKX1YXLe0RweY8IissrWb4zi0Xb0/h+RyapuSWkbi3hy62HeHHRDi7uEsoFccEM7xKKt1UhSkRERM7O9ddfz0MPPcTMmTP55JNPuPfeezGZTKxevZorr7ySW265BXDO2Nq1axddunQ5pdft3LkzKSkppKWlER4eDsC6detqnbNmzRpiYmJ46qmnqvcdOHCg1jlubm7YbHWvk9m5c2dmzJhBUVFR9ajT6tWrMZvNdOzY8ZTqPZcM/Ylt48aNDBs2rPr50XuRxo8fz4wZM0hLSyM5Odmo8kTOmKebC6N6hDOqRzgl5TbWJ+WwJfkIC7akknS4mC82p/LF5lQ8XC1cER/Bn4fHEenvYXTZIiIi0kR5e3szduxYJk2aRH5+PrfddhsAcXFxzJs3jzVr1hAQEMCbb75JRkbGKQen4cOH06FDB8aPH89rr71Gfn5+rYB09D2Sk5OZNWsW/fr1Y+HChcyfP7/WObGxsdWDJFFRUfj4+BxzO83NN9/MM888w/jx43n22WfJysriwQcf5NZbb62epmckQ9uRDx06FIfDccx2tGvGjBkzWLFixQmvf/bZZ49ZREuksfFwszCkQzAPD+/A948OZeadCdwzpB1tg7woqbAxe2MKw15fwZ8+3ci7K/awS23ORURE5AzccccdHDlyhBEjRlTfk/TXv/6V3r17M2LECIYOHUpYWBhXXXXVKb+m2Wxm/vz5lJSU0L9/f+68805efPHFWudcccUVPPLIIzzwwAP07NmTNWvW8PTTT9c655prruHSSy9l2LBhBAcHH7cluqenJ0uWLCEnJ4d+/fpx7bXXctFFFzFt2rTT/8M4B0yOFvYTWn5+Pn5+fuTl5Z32DWsi9cnhcLDpwBFe/zaRdftyah3rEOrNDf1aMzo+gmAfNTcRERE510pLS9m/fz9t2rTB3d3d6HKkHtX13/Z0soGCk4jBHA4HW1Jy2ZR0hHX7DrNydzbltpobHHtE+XF1r0jG9I7Cz8O1jlcSERGRM6Xg1HzVV3DSXekiBjOZTPRuHUDv1gHcdUFb8ksr+HLrIeZuTOGXg3nV20uLdnJ+XBCXdQ/n4s6h+HkqRImIiIg0FAUnkUbG192VWwfEcOuAGLIKyvhmexozf0pmZ3oBy3ZmsmxnJi5mE4PaB3FlfARX9IzA1WLo7YoiIiIizZ6Ck0gjFuxjZdzAWMYNjGV3RgGLtqXzzfY0dqYX8OOuLH7clcXfl+7izvPacHWfKHzdNQolIiIici7oHieRJmhvViGLfknj47UHyC4sA8DqYqZHlB/9YgO5smckHcN8DK5SRESk6dA9Ts2XmkOcIQUnaU5Kym3M3ZTCf9YdYFdGYa1jQzsG8+q1PQjx0V/+IiIiJ6Pg1HypOYSI4OFmYdzAWG4dEMPerCK2puTy3W/pfL8zkxWJWVzy9x/pGuFLpzBf7h/WnkAvN6NLFhEREWmSFJxEmgGTyUT7EG/ah3hzbZ8odmcU8ODnW9iZXsDqPYdZvecwX2w+yCMXd+DaPlF4uul/fREREZHToZ+eRJqhuFAfvnrgPDYk5XAot4R/r9xPYkYBk7/8lTe+3cWIrqFc0iWM8+KCcHe1GF2uiIiISKOn4CTSTLm5mBncPgiAq3pFMvOnZD5ctZ/knGLmbDzInI0H8XSzMKZXJA9dFEeIr+Zzi4iIiJyIFn8RaQFcLWbGD4pl+WNDmXlnAuMHxhDu505xuY3PfkrmgteWc9v09cxYvZ+ScpvR5YqIiMgpMplMdW7PPvvsWb32ggUL6q3Wpk4jTiItiKVq4dxB7YN49oqurNuXw2tLdrI5OZcViVmsSMzinRV7ueO8NlzdO1Id+URERBq5tLS06q9nz57N5MmTSUxMrN7n7e1tRFnNkkacRFook8nEwHat+O+9g1j45/N4amRnogI8yCoo4+VvdjJwyvfc9clGlidm0sJWLRAREQHA4XBQXFFsyHaq//aGhYVVb35+fphMplr7Zs2aRefOnXF3d6dTp068++671deWl5fzwAMPEB4ejru7OzExMUyZMgWA2NhYAMaMGYPJZKp+3pJpxEmkhTOZTHSN8KNrhB/jB8WyYEsqszYkszk5l+9+y+C73zIY3jmUmxNaE+DlRo9IP8xmk9Fli4iInHMllSUkzEww5L1/uuknPF09z+o1PvvsMyZPnsy0adPo1asXW7Zs4a677sLLy4vx48fz1ltv8dVXXzFnzhxat25NSkoKKSkpAGzYsIGQkBCmT5/OpZdeisWiZlIKTiJSzc3FzPX9orm+XzS7MwqYuT6Z/6w7wNIdGSzdkQHAee2D+PvYngT7WA2uVkREROryzDPP8MYbb3D11VcD0KZNG3777Tf++c9/Mn78eJKTk4mLi+O8887DZDIRExNTfW1wcDAA/v7+hIWFGVJ/Y6PgJCLHFRfqwzOju3JDv9a88W0iqbkl7M0qZNWebC75+w+MHxTLqO7hxLTyws1Fs35FRKT58XDx4KebfjLsvc9GUVERe/fu5Y477uCuu+6q3l9ZWYmfnx8At912GxdffDEdO3bk0ksv5fLLL+eSSy45q/dtzhScRKROHcN8+Ne4vgDszijg/pmb2ZVRyNSlu5m6dDduFjPX94vioYs6aBRKRESaFZPJdNbT5YxSWFgIwAcffEBCQu3phken3fXu3Zv9+/fzzTffsHTpUq6//nqGDx/OvHnzGrzepkDBSUROWVyoD4v+fD6Ltqfzn3UH+DU1j6JyG/9Zl8zcjQcZ2T2cCYPb0D3Kz+hSRUREWrTQ0FAiIiLYt28fN9988wnP8/X1ZezYsYwdO5Zrr72WSy+9lJycHAIDA3F1dcVm0zIlRyk4ichpcbGYuSI+giviI3A4HKzbl8Mri3eyNSWX+VtSmb8llat7R/LUyM608tYIlIiIiFGee+45/vznP+Pn58ell15KWVkZGzdu5MiRI0ycOJE333yT8PBwevXqhdlsZu7cuYSFheHv7w84O+stW7aMwYMHY7VaCQgIMPYDGUw3JojIGTva0nz+fYOYf98gruwZAcAXm1MZ9dYq1u49TIXNbnCVIiIiLdOdd97Jv//9b6ZPn0737t0ZMmQIM2bMoE2bNgD4+Pjw6quv0rdvX/r160dSUhKLFi3CbHZGhDfeeIPvvvuO6OhoevXqZeRHaRRMjha2QEt+fj5+fn7k5eXh6+trdDkizc7PKbk8Mmcr+7KKAOeiu90j/bikayiXdAmjfYgW4hMRkcantLSU/fv306ZNG9zdtQB8c1LXf9vTyQYacRKRehUf7c9XD5zH1b0j8XC1YLM72JqSy6uLExn+5g+M/MdKth3MM7pMERERkdOie5xEpN55W1148/qevHGdg0N5paxIzOTbXzNYszeb39Lyufq91dw2KJbR8RF0j3Suci4iIiLSmGnESUTOGZPJRKS/BzcnxPDxhP789ORwLu0aRoXNwQcr93PFtNU8+PkWisoqjS5VREREpE4acRKRBhPo5cZ7t/Tm298y+GrrIZb8ms7Xv6Sx6cAR4qP8ubJnBJd1Dze6TBEREZFjKDiJSIMymUyM6BrGiK5hbEzK4f6Zm0nLKyUtL53Fv6bz4IXtGTcwlgBPV1wsGhQXEZGG1cL6prUI9fXfVF31RMRQhWWVbEjK4YfELGasSare7+lmYVinEK7tHcXQjsG6D0pERM4pm83Grl27CAkJoVWrVkaXI/UoLy+PQ4cO0b59e1xdXWsdO51soBEnETGUt9WFYR1DGNYxhC7hvrz+bSJZhWUUl9tY+EsaC39Jo0u4L69e24NukX5GlysiIs2UxWLB39+fzMxMADw9PfVLu2bAbreTlZWFp6cnLi5nF3004iQijY7N7mB7ah5fbj3ErA3JFJfb8Pd0ZdbdA+gUpv9vRUTk3HA4HKSnp5Obm2t0KVKPzGYzbdq0wc3N7Zhjp5MNFJxEpFE7UlTO7TM2sDUlF39PV67tHcX1/aLpEOpjdGkiItJM2Ww2KioqjC5D6ombmxtm8/Hvm1ZwqoOCk0jTk1dcwc0frmN7aj4AJhOM7hHBsE7BxEf50zbY2+AKRUREpClScKqDgpNI01ReaWdFYib/3XyQJb9mVO83meCR4R14YFh7zGbNRRcREZFTp+BUBwUnkaZve2oen69PZkdaPpuTcwEY0DaQ+4a25/y4IN3MKyIiIqdEwakOCk4izcvsDck8veBXym12AC7uEsrr18XjZjHj4WYxuDoRERFpzBSc6qDgJNL8JB8uZvqa/Xy2Lplymx03i5lym534aH8+GNeHEB93o0sUERGRRuh0ssHx20uIiDQhrVt58szorsy7dyCR/h7Vo08/p+Qy9p/r2JGWb3CFIiIi0tRpxElEmpWyShvJh4uptDu48+ONpOaWAHB+XBB3nNeGIR2CdQ+UiIiIAJqqVycFJ5GWIy2vhL8t3ME329KwV/1N1zPan6ljexIb5GVscSIiImI4Bac6KDiJtDwpOcV8vCaJWRtSKCyrxMvNwvNXduPq3pEafRIREWnBFJzqoOAk0nKl5ZXw8Kyt/LQ/B4DLuoUxplckXSP9CPGx4mrRbZ8iIiItiYJTHRScRFo2m93Bu8v3MHXZbmz2mr/+3Cxm7hvWjocuitMolIiISAuhrnoiIidgMZt48KI45t83iFsGtKZdsBcuZhPlNjtTl+7myfnbqKjqyiciIiJylIvRBYiIGKFHlD89ovwBsNsdzFyfzOQvt/P5+hQS0wt4+6beRPp7GFukiIiINBoacRKRFs9sNnHLgBj+eWtffNxd2Jycy0VvrOC5//1K8uFio8sTERGRRkD3OImI/E7y4WImztnKxgNHqvedHxfEq9f2INxPI1AiIiLNie5xEhE5Q61beTL3noH8544ELugQjMkEK3dnc827a9iemmd0eSIiImIQQ4PTjz/+yOjRo4mIiMBkMrFgwYI6z//iiy+4+OKLCQ4OxtfXl4EDB7JkyZKGKVZEWgyTycR5cUF8MqE/3z86lLbBXhzKK+Xyt1cx7PUVfL4+Gbu9RQ3Wi4iItHiGBqeioiLi4+N55513Tun8H3/8kYsvvphFixaxadMmhg0bxujRo9myZcs5rlREWqo2QV7Mu2cQwzuH4mI2sT+7iElfbOPa99cwd2MK+aUVRpcoIiIiDaDR3ONkMpmYP38+V1111Wld17VrV8aOHcvkyZOPe7ysrIyysrLq5/n5+URHR+seJxE5bQWlFczekMKb3+2iuNwGQCsvN964Pp6hHUMMrk5EREROV4u5x8lut1NQUEBgYOAJz5kyZQp+fn7VW3R0dANWKCLNiY+7K3ee35alE4fwyPAOxLby5HBRObdN38CdH29g6W8ZNJLfRYmIiEg9a9LB6fXXX6ewsJDrr7/+hOdMmjSJvLy86i0lJaUBKxSR5ijC34OHhsex+OELGDcwBoClOzK585ONTPt+j8HViYiIyLnQZBfAnTlzJs899xxffvklISEnniJjtVqxWq0NWJmItBTurhaev7Ib4wbG8MnaA3yy9gBvfLcLm8PB9X2jidACuiIiIs1GkwxOs2bN4s4772Tu3LkMHz7c6HJEpIVrH+LD81d2w9fdlWnL9zB16W6mLt1Nv9gAxvSKIjrQgw6hPoT6uhtdqoiIiJyhJhecPv/8cyZMmMCsWbMYNWqU0eWIiFR79JIOhPpamb8lla0puWxIOsKGpJqFdOOj/Xl6VGf6xp74vkwRERFpnAwNToWFhezZU3M/wP79+9m6dSuBgYG0bt2aSZMmkZqayieffAI4p+eNHz+ef/zjHyQkJJCeng6Ah4cHfn5+hnwGEZGjTCYTtw6M5daBsWTklzJnQwpr9x0ms6CMPZmF/JySy+0zNjD/vsG0D/E2ulwRERE5DYa2I1+xYgXDhg07Zv/48eOZMWMGt912G0lJSaxYsQKAoUOH8sMPP5zw/FNxOi0HRUTqS2Z+Kfd9tpmNB44Q08qTZ0d35YIOwVjMJqNLExERabFOJxs0mnWcGoqCk4gYJbuwjCunrSY1twSAuBBvpt3Um45hPgZXJiIi0jK1mHWcRESakiBvK/PuHciEwW3w83Bld2YhV76zireX7Sa3uNzo8kRERKQOGnESETFAdmEZj8zeysrd2QB4uVl4+ZoejI6PMLgyERGRlkNT9eqg4CQijYXd7uB/vxzivRV72ZleAMB1faK4omcEg9sFYdb9TyIiIueUglMdFJxEpLGx2R28tiSR93/YW71vZPcwpt3YW+FJRETkHNI9TiIiTYjFbOKJyzox864EbugXjZvFzKJt6Uz5ZgdHisppYb/fEhERaZQ04iQi0sjM33KQR2b/XP08yNtKQptA7h3ajm6RWrNORESkvmjESUSkCRvTK4rJl3ch2McKOBtJLNyWxth/rmXdvsMGVyciItIyacRJRKQRKym3sS01j6lLd7Fm72HcXc08MrwDtw9ug5uLfvclIiJyNjTiJCLSTHi4WejfJpCPbuvHRZ1CKK2wM+WbnVz89x+YsyGFskqb0SWKiIi0CBpxEhFpIux2B//dfJBXFieSXVgGgLfVhUHtWhHh78GQjsEM6xhicJUiIiJNh9qR10HBSUSauuLySj5bl8yHq/aTnl9a69jbN/bSIroiIiKnSMGpDgpOItJc2O0Ofj6Yy+bkXDYm5fDN9nTcXMx8ftcA+sQEGF2eiIhIo6d7nEREWgCz2USv1gHccV4bpt3Um+GdQymvtHPXJxtJPlxsdHkiIiLNioKTiEgzYDGbeOvGnnSL9CWnqJzbpq9n+ur97MksNLo0ERGRZkHBSUSkmfB0c+HD8f0I93NnX3YRz/3vNy6d+iP/+nEvNnuLmpUtIiJS73SPk4hIM3Mot4RZ65NZn5TDun05ALiYTXQI9eGN6+PpHK6/+0REREDNIeqk4CQiLYXD4eDz9SlMWbSDgrJKACL9PfjygcEEeVsNrk5ERMR4ag4hIiKYTCZuSmjN1mcu4cf/G0abIC9Sc0u465ON1etAiYiIyKlRcBIRaeYsZhOtW3nywbi++Li7sCU5l8vfWsWavdlGlyYiItJkaKqeiEgLsjujgHv+s4m9WUUAnB8XRFZBGVEBHky7qTfurhaDKxQREWk4mqonIiLHFRfqw5cPnMfNCa0xmWDl7mx2phewdEcmj//3F/ZkFvLjrizs6sInIiJSi0acRERaqF8O5rJm72E8XC288PVvVP4uLN07tB2PX9rJwOpERETOvdPJBi4NVJOIiDQyPaL86RHlD4DZbOLpBdtxtZiosDl4b8Ve4qP8ubRbmLFFioiINBIKTiIiwq0DYji/fRCtvN2YunQ3H67azyOzt+Lu2huAjUlHGNsvmuhAT4MrFRERMYam6omISC0VNjt3fLyRH3dl1drvZjFz37B2PDy8g0GViYiI1C81hxARkTPmajHz73F9ubJnBABWFzPxUX6U2+xMXbqb1XvUxlxERFoeTdUTEZFjuLmY+fv1PRnbL5r2Id4Ee1uZ/OWvfLruAK8tSWRQu1aYTCajyxQREWkwGnESEZHjMptNDGoXRIiPOyaTiQcvao+Hq4WtKbl8uu4AhWWVVNrstLAZ3yIi0kLpHicRETllry7eybsr9tbaFxfizevXxRMf7W9MUSIiImdI9ziJiMg58eCFcdxxXhuiAjyq9+3OLOSa99bwn3UHDKxMRETk3NKIk4iInDaHw0FBWSVFZZW88PVvLNqWDsBTIztz1wVtDa5ORETk1GjESUREzimTyYSvuyvhfh68c1NvHhjWHoAXF+3guf/9Snml3eAKRURE6peCk4iInBWTycRjIzry2CXO9Z2mr07ixg/WkZ5XanBlIiIi9UfBSURE6sUDF8bxr1v74OPuwqYDR7j87ZX8e+U+fk7JNbo0ERGRs6Z7nEREpF4lZRdxz382sTO9oHrfI8M78NDwOAOrEhEROZbucRIREcPEBnkx/77BTLqsExd2CgHg70t3MfOnZGz2FvW7OhERaUY04iQiIufUy9/s5P0fnGs/+Xu6cvugNtw/rB0uFv3uTkREjKURJxERaTT+MqIjd1/QFl93F3KLK/j70l3c9MFPuvdJRESaFI04iYhIg6i02fnq50M8vWA7ReU2AIJ9rNjsDoZ0CGbSyE6E+LgbXKWIiLQkGnESEZFGx8Vi5ureUSx66HzG9IrExWwiq6CMnKJy5m9JZfgbP7BoW5rRZYqIiByXRpxERMQQ2YVlHMotobCskimLdrItNQ+AP13Qlicu64TJZDK4QhERae404iQiIo1ekLeVHlH+DGoXxPz7BvGnC9oC8M8f9/H6t4kGVyciIlKbgpOIiBjOxWJm0sjOvHx1dwDeWb6Xz346YHBVIiIiNRScRESk0bihf2smXtwBgBe+/o2UnGKDKxIREXFScBIRkUblwQvbk9AmkNIKO8989Sst7FZcERFppNQcQkREGp09mQVc9o+VVNgc+Fhd8PVwxcViYnD7IO4f1p5Ifw+jSxQRkWagyTSH+PHHHxk9ejQRERGYTCYWLFhw0mtWrFhB7969sVqttG/fnhkzZpzzOkVEpGG1D/Fh0mWdsZhNFJRVkppbwoHDxcz8KZlhr61g9Z5so0sUEZEWxtDgVFRURHx8PO+8884pnb9//35GjRrFsGHD2Lp1Kw8//DB33nknS5YsOceViohIQ5twXhu2Tr6YZY8OYcH9g5l+Wz/6xARQbrPz3P9+xWZvURMmRETEYI1mqp7JZGL+/PlcddVVJzzn8ccfZ+HChWzfvr163w033EBubi6LFy8+pffRVD0RkaYrr7iC81/9nvzSSv4+Np4xvaKMLklERJqwJjNV73StXbuW4cOH19o3YsQI1q5de8JrysrKyM/Pr7WJiEjT5Ofpyp+GtAPgxYU7eWnRDhZvTyeroMzgykREpLlzMbqA05Genk5oaGitfaGhoeTn51NSUoKHx7E3C0+ZMoXnnnuuoUoUEZFz7PbBsczakExKTgn/+nEfACYTXNo1jPuHtadbpJ/BFYqISHPUpEaczsSkSZPIy8ur3lJSUowuSUREzoKnmwv/e+A83rgunpsSWtMpzAeHA77Zns7lb6/ijhkb+PVQntFliohIM9OkRpzCwsLIyMiotS8jIwNfX9/jjjYBWK1WrFZrQ5QnIiINxN/TjWv6RHFNH+c9TrsyCpj2/R6+/uUQy3ZmsmJXFndf0JaHLorD3dVicLUiItIcNKkRp4EDB7Js2bJa+7777jsGDhxoUEUiItIYdAj14a0be7F04hAu6xaGze7gvRV7GfmPlWxMyjG6PBERaQYMDU6FhYVs3bqVrVu3As5241u3biU5ORlwTrMbN25c9fn33HMP+/bt4y9/+Qs7d+7k3XffZc6cOTzyyCNGlC8iIo1M22Bv3rulD+/f0odgHyv7sou44V/r2JqSa3RpIiLSxBkanDZu3EivXr3o1asXABMnTqRXr15MnjwZgLS0tOoQBdCmTRsWLlzId999R3x8PG+88Qb//ve/GTFihCH1i4hI43RptzCWThzChZ1CqLQ7eGT2VorLK40uS0REmrBGs45TQ9E6TiIiLUdecQUjpv5Ien4p3SJ9ubRrGOMGxeLr7mp0aSIi0gg023WcREREToefpytvXB+Pm8XM9tR8Xv92F9e/v5aM/FKjSxMRkSZGwUlERJq1we2DWPboEF64qhvBPlZ2phdw5bTVzNmYQqXNbnR5IiLSRGiqnoiItBgpOcWM/2g9+7KLAOgXG8BHt/XDR1P3RERaJE3VExEROY7oQE8W/vl8nhrZGR+rCxuSjnDrh+vJKSo3ujQREWnkFJxERKRF8XCzcNcFbfn87gH4e7qyNSWX4W/+wGc/HSD5cDEtbCKGiIicIk3VExGRFmtnej4Pfb6VxIyC6n0XdQrhnZt74+5qMbAyERFpCKeTDRScRESkRSuvtPPR6v18sy2NXw/lU2l30Lu1P+6uFixmE2/d0IsALzejyxQRkXNAwakOCk4iInIiG5JyGP/ReorLbdX7BrVrxccT+uNq0ex2EZHmRs0hREREzkC/2EA+uzOBK+IjeOySDni6WViz9zB/nb+dSpudpOwitqfmGV2miIgYwMXoAkRERBqTXq0D6NU6AIC4UB/u+c8mZm9MYUvKEfZkFgIw795B9K46R0REWgaNOImIiJzAiK5hvHtTb6wuZnZlFGJ3gN0Bry9JNLo0ERFpYApOIiIidbisezjz7hnEzQmtmXZTL9wsZtbsPcyq3dlGlyYiIg1IU/VEREROonuUH92jugOw6cARpq9O4p7/bOKqXhHc2L81XSP8DK5QRETONY04iYiInIYHhrWnc7gvhWWV/GddMqPeWsX1/1zL4cIyo0sTEZFzSMFJRETkNLTytrLwwfOYeWcCl/cIx9ViYv3+HMZ9tJ68kgqjyxMRkXNE6ziJiIichb1ZhYz951qyC8vxcLXQLdKXmxNiuCI+ArPZZHR5IiJSB63jJCIi0kDaBXvzyYQEwv3cKamwsSHpCA/P3soV76wis6DU6PJERKSeaMRJRESkHtjsDvZnF7F4exr//HEfBaWVdA73ZfafBuDr7mp0eSIichwacRIREWlgFrOJ9iHePHBhHF8/eB5B3lZ2pOXz0OdbjC5NRETqgYKTiIhIPYtp5cXHE/rhajGxPDGLtXsPs2ZPNh+u2k9esRpIiIg0RZqqJyIico48vWA7n647QEwrTw4eKcFmd+Dj7sJfR3VmbL/WRpcnItLiaaqeiIhII3DfsHa4uZg5cLgYm92Bv6crBaWVPDl/O9tT84wuT0REToOCk4iIyDkS7ufBuAExAFzcJZT1Tw5nZPcwbHYHf5n3C/9Yupspi3aQX6rpeyIijZ2m6omIiJxDFTY7mw8coU9MAC4WM1kFZQx/84dai+W2DvTk3Zt70y3Sz8BKRURaHk3VExERaSRcLWYS2rbCxeL8JzfYx8or1/QgxMfK8M4hRPp7kJxTzI0frOO3Q/kGVysiIieiEScRERED5RVXcOcnG9iQdIRgHyuf35VA+xAfo8sSEWkRNOIkIiLSRPh5uvLv8f3oFOZDVkEZo99ezWc/HaCF/V5TRKTRU3ASERExmJ+HK/+5M4Hz2gdRUmHjqfnbueuTTRwuLDO6NBERqaLgJCIi0ggEeVv5ZEJ//jqqM24WM0t3ZHDd+2spUMc9EZFGQcFJRESkkTCbTdx5flsW3D+YcD939mUX8fh/f9G0PRGRRkDBSUREpJHpEuHLOzf3xtViYtG2dG764CcWbUtTgBIRMZCCk4iISCPUu3UAz17RFZMJ1u47zH2fbWb66iQABSgREQOoHbmIiEgjlpJTzIer9jNjTRJmE/SI8ufXQ3lc2CmEBy+Mw8fdhQh/D1wt+l2oiMjpOp1soOAkIiLSyDkcDiZ9sY1ZG1KOe7xXa3++uHcQJpOpgSsTEWnaTicbuDRQTSIiInKGTCYTz1/ZjRBfd6wuZvrEBPDPH/ayfn8OxRU2tiTnsmpPNufHBRtdqohIs6XgJCIi0gS4uZiZeHGH6ucD2rYC4NmvfmXGmiQ+XLVfwUlE5BzShGgREZEm7PbBsZhMsCIxi2nf72bRtjTs9hY1C19EpEEoOImIiDRhMa28uLhzKACvf7uL+z7bzCuLdxpclYhI86OpeiIiIk3cpJGdcbWYKbfZ+e63DP754z7cXS38aUhbDheWU1Zpo32Ij9Fliog0aeqqJyIi0oy8s3wPry1JBMDVYqLC5vxnfurYnlzVK9LI0kREGp3TyQaaqiciItKM3De0HS+N6U5MK08qbA6Odij/y7xfWL8/x9jiRESaMI04iYiINEM2u4N9WYWE+3vwf3N/5pvt6QR4uvL1n88n0t/D6PJERBoFreMkIiLSwlnMJuJCnfc1vXl9T1KOrGF7aj53fbyRVt5uHMotYWT3cG7s35oIBSkRkZPSiJOIiEgLkJJTzOhpq8gtrqi132yCizqH8uTIzrQJ8jKoOhERY+geJxEREaklOtCT927uQ9cIXyYMbsPfx8YzoG0gdgd891sGN/xrLcmHi40uU0Sk0dKIk4iISAu2J7OA+z/bQmJGAVEBHnx6R4JGnkSkxdCIk4iIiJyS9iE+fHpHf2JbeXLwSAlXvbOaNXuzjS5LRKTRMTw4vfPOO8TGxuLu7k5CQgLr16+v8/ypU6fSsWNHPDw8iI6O5pFHHqG0tLSBqhUREWl+QnzdmXPPQOKj/ckrqWDch+v5fH2y0WWJiDQqhgan2bNnM3HiRJ555hk2b95MfHw8I0aMIDMz87jnz5w5kyeeeIJnnnmGHTt28OGHHzJ79myefPLJBq5cRESkeQnxcWf23QMYHR9Bpd3BpC+28cGP+4wuS0Sk0TD0HqeEhAT69evHtGnTALDb7URHR/Pggw/yxBNPHHP+Aw88wI4dO1i2bFn1vkcffZSffvqJVatWndJ76h4nERGRE3M4HPx96W7eWrYbV4uJBfcPJtzPA3dXM55uWsVERJqXJnGPU3l5OZs2bWL48OE1xZjNDB8+nLVr1x73mkGDBrFp06bq6Xz79u1j0aJFjBw58oTvU1ZWRn5+fq1NREREjs9kMvHI8Dgu6RJKhc3BDf9aR5+/fcfIf6wk7w+tzEVEWhLDglN2djY2m43Q0NBa+0NDQ0lPTz/uNTfddBPPP/885513Hq6urrRr146hQ4fWOVVvypQp+Pn5VW/R0dH1+jlERESaG5PJxJSruxPk7UZBaSUOByQdLuaxeT+TfLiY9DzdWywiLY/hzSFOx4oVK3jppZd499132bx5M1988QULFy7khRdeOOE1kyZNIi8vr3pLSUlpwIpFRESaplbeVj6/awAvjenOR7f1xc1i5rvfMrjgteUMmLKMd5bvoYWtaCIiLZxhk5WDgoKwWCxkZGTU2p+RkUFYWNhxr3n66ae59dZbufPOOwHo3r07RUVF3H333Tz11FOYzcfmQKvVitVqrf8PICIi0szFhfoQF+oDwDNXdOGp+dtxtZiosDl4bUkimfmlTB7dFYvZZHClIiLnnmEjTm5ubvTp06dWowe73c6yZcsYOHDgca8pLi4+JhxZLBYA/dZLRETkHLo5IYYtT1/MjucvZfLlXTCZ4OO1B3jw882UVtiMLk9E5JwztD3OxIkTGT9+PH379qV///5MnTqVoqIibr/9dgDGjRtHZGQkU6ZMAWD06NG8+eab9OrVi4SEBPbs2cPTTz/N6NGjqwOUiIiInBsBXm4ATDivDcE+Vh6d8zOLtqXjbd3Oq9fGG1ydiMi5ZWhwGjt2LFlZWUyePJn09HR69uzJ4sWLqxtGJCcn1xph+utf/4rJZOKvf/0rqampBAcHM3r0aF588UWjPoKIiEiLNDo+Al8PV8Z/tJ45Gw8yuH0QX249RFZBGb1b+3P/he0J8XE3ukwRkXpj6DpORtA6TiIiIvXn8Xm/MHvjsY2X+sYEMPeegZhMuv9JRBqvJrGOk4iIiDR9j1/WCX9PVwA6hfnwxnXxWF3MbDxwhBWJWQZXJyJSf85oql5KSgomk4moqCgA1q9fz8yZM+nSpQt33313vRYoIiIijVeglxufTOjPqj3ZjBsYi7fVhcSMAv714z5eXZLIBR2C1XVPRJqFMxpxuummm1i+fDkA6enpXHzxxaxfv56nnnqK559/vl4LFBERkcatR5Q/9w1tj7fV+fvYe4e0w9vqwo60fMa8u5qtKbnGFigiUg/OKDht376d/v37AzBnzhy6devGmjVr+Oyzz5gxY0Z91iciIiJNTICXGy9f0x0fqwu/HMzjhn+tZXtqHjvT8/nvpoNU2uxGlygictrOaKpeRUVF9aKyS5cu5YorrgCgU6dOpKWl1V91IiIi0iRd3iOChDateGT2VlbtyWb8R+vJK6mg0u7gt7R8nr68i9ElioicljMaceratSvvv/8+K1eu5LvvvuPSSy8F4NChQ7Rq1apeCxQREZGmKdjHyjs396ZNkBeHi8qptDsb+X64aj9vfpvInI0p5BVXGFyliMipOaN25CtWrGDMmDHk5+czfvx4PvroIwCefPJJdu7cyRdffFHvhdYXtSMXERFpWPuyCnnju11c3DmUxIwC3luxt/pY22Av5vxpIEHeVgMrFJGW6nSywRmv42Sz2cjPzycgIKB6X1JSEp6enoSEhJzJSzYIBScRERHj2OwOXv5mB3uzitiemkdmQRldwn2ZcXs/Qny1YK6INKxzHpxKSkpwOBx4enoCcODAAebPn0/nzp0ZMWLEmVXdQBScREREGod9WYVc9/5aDheVE+DpyuvXxXNR51CjyxKRFuScL4B75ZVX8sknnwCQm5tLQkICb7zxBldddRXvvffembykiIiItDBtg72Zc89AuoT7cqS4gj99uolNB46wP7uIb7alYbef0aQYEZFz4oyC0+bNmzn//PMBmDdvHqGhoRw4cIBPPvmEt956q14LFBERkearXbA38+8fxKVdw6i0O7j7k42MmPoj9362mbe/32N0eSIi1c4oOBUXF+Pj4wPAt99+y9VXX43ZbGbAgAEcOHCgXgsUERGR5s3qYuH16+NpG+zsvlde6Vzn6a3vd/OzFs8VkUbijIJT+/btWbBgASkpKSxZsoRLLrkEgMzMTN03JCIiIqfN2+rCv27ty0WdQnjhqm5c3iMcm93Bn2dt4VBuidHliYicWXCaPHkyjz32GLGxsfTv35+BAwcCztGnXr161WuBIiIi0jK0D/Hmw9v6ceuAGP52VTci/T04cLiY695fy/7sIqPLE5EW7ozbkaenp5OWlkZ8fDxmszN/rV+/Hl9fXzp16lSvRdYnddUTERFpGlJzS7j13z+xL7uIcD93Zt09gLS8UuwOB31jAnFzOaPf/4qIVGuQdZyOOnjwIABRUVFn8zINRsFJRESk6cgqKOPGD9axJ7MQswmONtrzcXfh+Su7MqZX0/j5Q0Qap3Pejtxut/P888/j5+dHTEwMMTEx+Pv788ILL2C328+oaBEREZE/Cvax8tmdCcS08sTuAD8PV4K83SgoreTx/25jV0YBJeU2SitsRpcqIs2cy5lc9NRTT/Hhhx/y8ssvM3jwYABWrVrFs88+S2lpKS+++GK9FikiIiItV6ivO1/eP5hfDubRv00gbhYzd3y8geWJWdz64U/kFlcQ6OXGJxP6ExfqY3S5ItJMndFUvYiICN5//32uuOKKWvu//PJL7rvvPlJTU+utwPqmqXoiIiJNX2Z+KSOm/siR4orqfQGervznzgS6RvgZWJmINCXnfKpeTk7OcRtAdOrUiZycnDN5SREREZFTFuLrzke39eNPF7Tl0zv6Ex/lx5HiCh7/7y+c5e3bIiLHdUbBKT4+nmnTph2zf9q0afTo0eOsixIRERE5mV6tA5g0sjPnxwXz0W398HC1sD01nzV7Dxtdmog0Q2d0j9Orr77KqFGjWLp0afUaTmvXriUlJYVFixbVa4EiIiIiJ9PK28rYftHMWJPEuyv24OlmIcjbSnSgp9GliUgzcUYjTkOGDGHXrl2MGTOG3NxccnNzufrqq/n111/59NNP67tGERERkZO647w2WMwmVu85zJh31zDkteVM+mIbOUXlRpcmIs3AWa/j9Hs///wzvXv3xmZrvC1B1RxCRESk+Xpq/jY++ymZAE/X6sYRob5WHr+0E78czCPMz517hrQzuEoRaSxOJxuc0VQ9ERERkcbohSu78X8jOuLn4cqGpCNM+uIX9mYVMXHOz9XndArzYWjHEAOrFJGm6Iym6omIiIg0RmazCX9PN0wmE/3bBPLVA+dxbZ8o/Dxc6Rzu/G3yiwt3UGmzG1ypiDQ1GnESERGRZsvL6sLr18UDkFdSwbDXV7A7s5An529j3MBYOoX54GLR75FF5OROKzhdffXVdR7Pzc09m1pEREREzhk/D1ceu6QjT87fxpyNB5mz8SBWFzOXdA3jtWt74O5qMbpEEWnETis4+fnVvRK3n58f48aNO6uCRERERM6VmxJaE+xjZe7GFFbvyaao3Mb/fj6Eq9nEG9fHYzKZjC5RRBqpeu2q1xSoq56IiIgA2O0Olidmcvenm7DZHfzfiI6MGxjDnz7dRKXdwb/H98XX3dXoMkXkHDqdbKBJvSIiItIimc0mLuocyrNXdAXgtSWJ3PjBOtbsPcz6/Tk8Oudn7PYW9ftlEamDgpOIiIi0aLcOiGH8wBgAtqfm42ox4WYx891vGfxNHfhEpIqCk4iIiLR4T1/ehQs7hWA2OdeCeuEq5yjUR6v3M/Zf68gpKje4QhExmtqRi4iISIvnYjHz4fi+HC4qJ8jbCjhbmU/67zY2HTjCQ7O28PHt/TGb1TxCpKXSiJOIiIgIYDKZqkMTwOU9Iph770DcXc2s3J3NY/N+5r0VezlwuMjAKkXEKApOIiIiIifQKcyXF67sBsAXm1N5ZfFOrnxnNb8eyjO4MhFpaJqqJyIiIlKH6/pGU1hWyS8H89iRls/O9AJu/vdPPHlZZ8b0jsTVot9Di7QEWsdJRERE5BTll1Yw7sP1bE3JBaBTmA//vXcQXlb9LlqkKdI6TiIiIiLngK+7K5/fNYAnR3bC39OVnekFvLVsN78dyuffK/dRUm4zukQROUc04iQiIiJyBpbtyOCOjzdiMZswm6DC5uDmhNa8OKa70aWJyCnSiJOIiIjIOXZR51Au7RqGze6gwub8PfTM9cn8XDWNT0SaFwUnERERkTP0wlXdGNMrkhfHdOOqnhE4HPDXBdspr7QbXZqI1DNN1RMRERGpB5kFpVz0xg8UlFZybZ8oXriyGz8fzGXV7mxCfa3cMiAGk0kL6Io0JqeTDdQCRkRERKQehPi489aNvbjz443M23SQ/24+yO9/Pd0t0o9erQOMK1BEzoqm6omIiIjUk2EdQ3j+yq4AOBwQ5G0ltpUnAJ+uPWBkaSJyljTiJCIiIlKPbk6IYWDbVnhZXQjxsbI1JZcx767h61/SeGpUZ1p5W7HZHVTa7VhdLEaXKyKnyPARp3feeYfY2Fjc3d1JSEhg/fr1dZ6fm5vL/fffT3h4OFarlQ4dOrBo0aIGqlZERETk5NoGexPq647JZKJntD/dI/0ot9l547tdLNqWxuCXv2fwy9+zdu9ho0sVkVNkaHCaPXs2EydO5JlnnmHz5s3Ex8czYsQIMjMzj3t+eXk5F198MUlJScybN4/ExEQ++OADIiMjG7hyERERkVNjMpm4bVAsADN/Sua+zzaTnl9KdmE5t374Ewu2pBpboIicEkO76iUkJNCvXz+mTZsGgN1uJzo6mgcffJAnnnjimPPff/99XnvtNXbu3Imrq+sZvae66omIiEhDczgczFyfzKdrD7A7s5AJg2PJyC/jq58P4WN14Ye/DCPQy83oMkVanNPJBoYFp/Lycjw9PZk3bx5XXXVV9f7x48eTm5vLl19+ecw1I0eOJDAwEE9PT7788kuCg4O56aabePzxx7FYjj9HuKysjLKysurn+fn5REdHKziJiIiIIcoqbVhdLNjtDi5/exW/peUzYXAbJo/uwoHDRbyzfA+XdAljeJdQo0sVafZOJzgZNlUvOzsbm81GaGjtvxRCQ0NJT08/7jX79u1j3rx52Gw2Fi1axNNPP80bb7zB3/72txO+z5QpU/Dz86veoqOj6/VziIiIiJyOow0hzGYTk0Z2AuDTdUk8NX8bo95axZyNB3nw8y0cPFJsZJki8geGN4c4HXa7nZCQEP71r3/Rp08fxo4dy1NPPcX7779/wmsmTZpEXl5e9ZaSktKAFYuIiIic2PlxwZwfF0SFzcFnPyVTWFaJm4uZkgobz3z5K3a7YXdUiMgfGNaOPCgoCIvFQkZGRq39GRkZhIWFHfea8PBwXF1da03L69y5M+np6ZSXl+PmduzcYKvVitVqrd/iRUREROrJP27oxde/HOJQbilRAR70iw3k8rdXsmxnJm2fXERMK0/G9ovm5v4x+Hme2T3eInL2DBtxcnNzo0+fPixbtqx6n91uZ9myZQwcOPC41wwePJg9e/Zgt9ur9+3atYvw8PDjhiYRERGRxi7Qy41xA2N54rJO3DIgho5hPjx6SUdMJufxA4eLeXVxIqOnrSIlR9P3RIxi6FS9iRMn8sEHH/Dxxx+zY8cO7r33XoqKirj99tsBGDduHJMmTao+/9577yUnJ4eHHnqIXbt2sXDhQl566SXuv/9+oz6CiIiISL27Z0g7djx/KeufuojXru1BpL8HyTnFXPf+Wt5Zvoc9mQVGlyjS4hg2VQ9g7NixZGVlMXnyZNLT0+nZsyeLFy+ubhiRnJyM2VyT7aKjo1myZAmPPPIIPXr0IDIykoceeojHH3/cqI8gIiIick64u1pwd7VwXd9ozo8L5pYPf2JPZiGvLUnkH0t3s+D+wXSJUIdgkYZi6DpORtA6TiIiItIU5ZVUMH/zQb7YksovB/Po1dqf/94zCLPZZHRpIk1Wk1jHySgKTiIiItKUpeeVMvzNHygsq+TqXpH0iQ0gJtCL7lF++HmoeYTI6VBwqoOCk4iIiDR101fv57n//VZrX6CXG1/cO4jYIC+DqhJpek4nGxh6j5OIiIiInL7bBsUS5G1l04EjJOcUsz01j8yCMv5v3s98dFs/UnJK6Bzug8mkaXwi9UUjTiIiIiJNXEpOMZdO/ZGichsuZhOVdgf3D2vHoxd35IddWXSJ8CXU193oMkUandPJBoa2IxcRERGRsxcd6MlTo7oAUGl3/k78neV7GfPeGm6fsYFr3ltDaYXNyBJFmjxN1RMRERFpBm7sH03rQE/8PV2Zt+kgM9Yk8XNKLgAHj5Tw/g97eXh4B2OLFGnCFJxEREREmgGTycR5cUEAdAj1IT2vlJQjxQzvHMo/lu3mvRV7GdoxhJ7R/sYWKtJE6R4nERERkWbM4XBw4wfrWLcvB4BhHYO5sX9ryirtJOcUc+vAGHzd1cZcWiZ11RMRERERwDkS9daNvXhp4Q6++vkQyxOzWJ6YVX08+XAxr1zbw8AKRZoGNYcQERERaeZCfNyZekMvlj06lLvOb0OkvwdxId4A/HfzQVJyig2uUKTxU3ASERERaSHaBHnx1KgurH7iQr6bOITz44KotDt4d8Ueo0sTafQ0VU9ERESkhXroojhW7s5mzsaD+Li70jXCl8OF5VzSNZSoAE+jyxNpVNQcQkRERKQFe3zeL8zemFJrn6vFxNh+0Uy6rDM5ReX8sCuLa3pH4eFmMahKkXPjdLKBgpOIiIhIC7d8ZybvrtjD0Z8KNx44AkBMK0+yCsooLrcxbmAMz1/ZzcAqReqfglMdFJxERERE6rZmTzaPzNlKRn5Z9T4vNwvrnrwIH7Uul2bkdLKBmkOIiIiISC2D2gex8M/nM25gDH+7qhvtQ7wpKrfxxeZUo0sTMYyaQ4iIiIjIMYK8rdVT8+wOB5O//JUPV+0n1NfK4PZBGnmSFkcjTiIiIiJSp6t7R+FtdSE5p5h7/rOZS6euZE9mgdFliTQoBScRERERqZO31YV/jevD9X2jCPdzJzW3hGveW8vby3azL6vQ6PJEGoSaQ4iIiIjIKcspKmfCjA1sTckFwMVsYuoNPRneOZT92UV0CvPBZDIZW6TIKVJXvTooOImIiIicndIKG19tPcQXWw6ybl8OZhP4uLuSV1LBoxd34MGL4owuUeSUqKueiIiIiJwz7q4Wru8XzWd3DuD6vlHYHZBXUgHAez/sJbuw7CSvINL0KDiJiIiIyBmxmE28fHUPpo7tyYzb+9Ejyo/ichvvLt+Lw+Fg8fY0Hpv7sxpJSLOgqXoiIiIiUi9W7s7i1g/XA+Dv6UpusXMUKqaVJ/978Dx81cJcGhlN1RMRERGRBnde+yDG9o0GILe4Ag9XC0Hebhw4XMzNH/zE9e+vZdr3uw2uUuTMaMRJREREROpVbnE5+7OLiGnlRUpOMde+v4YKW82PnC+O6Uav6ABKKirpExNoYKXS0qmrXh0UnEREREQa1orETDYk5ZBXUsF/1iXXOnbv0HY8dFEcucUVhPpa1cpcGpSCUx0UnERERESM4XA4eGjWVr76+RBuLmbKK+0AmEzgcMBfR3XmzvPbGlyltCQKTnVoVMFp+Uuw7n0wm8FkBpPF+Wh2Aas3WH1qNjef2s+rN9/j77fo5ksRERFpfGx2B1tTjtAxzJdvtqXx5Pxt1dP4fKwu/PiXYQR4uRlcpbQUp5MNXBqoJjmeimIoyzs3r+3iURWivOsOWMfb//uQ5urh/DWQiIiISD2wmE3V9zVd1zeaS7qGUVxeyYQZG9mRls97P+zlyZGdq8//fH0yG/bn8Mzorvh56hfDYhyNOBmp6DCUHAGHHRw2sNucX9sroLwIygpOsuUfu6+ypH5rNFmcAcrdF7yCwTPI+egVVLUd3fe7/S7W+q1BREREmr3liZncPn0DFrOJuBBvbk5ozZW9Iun/4lJKK+wMaBvIJxMScHNRU2ipPxpxaiq8Wjm3+mSrqB2kygtPHLKOu+93Gw5noCvNdW65ySd58ypWX/Bs5QxSPmHgE+589I2oel71aPXRaJaIiIgAMLRDMJf3COfrX9LYmV7Ac//7jcyCMkornPdBrduXwzNfbWfK1T0MrlRaKo04yfHZ7VVTCatCVGkuFGVDUZZzKz5c9XV2zf7ibLBXnvp7uHqBb3hNsPKp+to3Avyjwa+1cwRL4UpERKRFcDgcJOcU83/zfmH9/pzq/cM7h7J0RwYmE6x4bCgxrbwMrFKaE404ydkzm6vuj/IGwk/tGoejKmAdDVWZUJABBWk1W34aFKQ77+2qKILDe5zbibh4gF9UVZCKcoYp/2jwi3Y++kSARd/GIiIizYHJZCKmlRcPD4/jpg9+AsBscq77VGGz88OuLGasSaKwtJKfD+by2Z0DCPbRLQLSMPQTp9Qfkwk8ApxbUPu6zy0vcgao6jBVFagKDkFeKuSlOJ9XlsDh3c7tuO9pcQaqwDYQ2BYC2vzu61hw02+kREREmpqBbVvRJyaATQeOcF5cMKG+7tw2OJYfdmUxfXVS9Xmfrk1i4iUdjStUWhRN1ZPGq7IM8lMhN8UZpPIOVn2dXPV40NlIoy7eob8LVG2doSqgDbRq6wx4IiIi0ijtSMvn1cU7efSSjnSL9MNudzD8zR/Yl11UfU6Qtxurn7gQq4vFwEqlKdM6TnVQcGpG7HYoTIcjSZCzH3L2wZH9zq+P7Hd2LKyLVzAEdYBW7Z2PQXHOzT8GzPoLWEREpLFZsCWVR+Zs5c8XxjFnYwppeaW8eX08V/eOMro0aaIUnOqg4NSClBypCVE5+yAnqebrgrQTX2dxg8B2VUGqKlAFd4Lgjs51rURERMQwpRU23F0tvLN8D68tSaRzuC9fP3geucXlJB0uonukv1qWyylTcKqDgpMAUFbobEqRvRuydznvocre7dxXWXqCi0zO6X4hnSGkS81jq3Zg0YJ8IiIiDSmnqJwhry6noKySRy/uwKwNKaTmluDj7sLNCTH834iOWMzqzCt1U3Cqg4KT1Mlud95P9ftAlbULMn+DkpzjX2N2dY5MhXSuHar8Y5zdCUVEROSc+GjVfp7/+rfq5yaTs8kvOFuY947xx2wyccd5bXC16N9kOZaCUx0UnOSMOBzOFuuZv0Hmjt897nAuMnw8rl4Q1g3CukNYD+djSBdwdW/Y2kVERJqpSpudy99exc70AnzcXZh/3yB+PZTP/839hXKbvfq8q3tHct/Q9iz5NZ0LO4XQOVw/A4qTglMdFJykXjkczhGqWmHqN+cola3s2PNNFufoVHhVkDoaqjwDG752ERGRZuC3Q/m8umQn9w5pR0LbVgD8tO8wH6zch5uLmSW/ZmCz1/5x9+pekbx8TQ/dCyUKTnVRcJIGYat03i+Vvg3Sf3Fuab+ceLqfb1RNkIroBZG9wSesYWsWERFphuZsTOEv834BoGuEL78eygfg1Wt6cH2/aCNLk0ZAwakOCk5iGIfD2c0v7ZeqQPWz8/FI0vHP94moClG9nI8RvTUyJSIicgY2HTiCl9VCpzBfpn2/m9e/3UXfmADm3TvI6NLEYApOdVBwkkanNA/StztDVNrPcGgLZCeCw37suf4xztGoo0EqPB7c9X0sIiJyqjLySxk4ZRl2B0y7qRdLfs2gqKyS1oGePHpJB3zc1Sm3JVFwqoOCkzQJZYXO6X2HtkDqZudjzt7jn9sqDqL6Vm39nQ0oLC4NW6+IiEgTMmHGBr7fmXnM/vPaBzE6Ppy5Gw9y/4XtGdYxxIDqpCEpONVBwUmarJJcSNv6uzC1FfKSjz3P1cs5KnU0SEX1A+/gBi5WRESk8Vq8PY17/rMZcIalS7qG8vI3Oykut1Wf4+/pytKJQwjythpVpjSAJhec3nnnHV577TXS09OJj4/n7bffpn///ie9btasWdx4441ceeWVLFiw4JTeS8FJmpWibGeISt0IKeshdROU5R97nn8MRFeFqKh+ENoNXNwavl4REZFGoMJm56/ztxPsY+Xh4XG4WMws35nJnZ9sxNViopWXldTcEs6PC6JLhC89Iv0Z1SPc6LLlHGhSwWn27NmMGzeO999/n4SEBKZOncrcuXNJTEwkJOTEw6NJSUmcd955tG3blsDAQAUnEXAu4JudCAc3OLeUDZC1E/jD/+Yu7s77pFoPgNYDIToBPPyNqFhERKTROHC4CA83Cxl5ZVz5ziqOdjE3mWD23QPp30ZNmpqbJhWcEhIS6NevH9OmTQPAbrcTHR3Ngw8+yBNPPHHca2w2GxdccAETJkxg5cqV5ObmKjiJnEhpnnMk6mDVqNTBDVCa+4eTTM57o2IGOoNU6wHgF2VEtSIiIo3CJ2uT+PqXNMor7WxNySXS34NvHj4fXzWPaFaaTHAqLy/H09OTefPmcdVVV1XvHz9+PLm5uXz55ZfHve6ZZ57hl19+Yf78+dx22211BqeysjLKymoWIs3Pzyc6OlrBSVouh8O5xlTKT5C8Fg6sPX7jCb/omhDVeiAEdwKzFgoUEZGWpbCskpH/WElyTjFjekXy97E9ax0vrbBRXG4j0EtT4Jui0wlOhrbeys7OxmazERoaWmt/aGgoO3fuPO41q1at4sMPP2Tr1q2n9B5TpkzhueeeO9tSRZoPkwmC4pxbr1uc+wozIXmdM0glr3WuNZWXAttSYNsc5znu/lUhagC0HuSc6qf7pEREpJnztrrw97HxXPf+WuZvSWVYpxDOax/EluQjrEjM4sutqZRW2Jl7z0Dio/2NLlfOoSbVs7igoIBbb72VDz74gKCgoFO6ZtKkSUycOLH6+dERJxH5He8Q6HKFcwNnO/TUjc7RqOS1NdP7di12bgCuns57o2LPg9jzFaRERKTZ6hMTyAMXxvHWst38+fMtxz3n/R/28t4tfRq4MmlIhganoKAgLBYLGRkZtfZnZGQQFhZ2zPl79+4lKSmJ0aNHV++z252LhLq4uJCYmEi7du1qXWO1WrFa1UZS5LRYvaHtUOcGYKtwriuVvA4OrHGGqeLDsG+5cwMFKRERadYevLA9q/dks+nAEQDaBHmR0CaQ+Gh/Jn2xjSW/ppOSU0xUgAff/pbBnsxC7jy/DVYXi8GVS31pFM0h+vfvz9tvvw04g1Dr1q154IEHjmkOUVpayp49e2rt++tf/0pBQQH/+Mc/6NChA25udf+gpuYQIvXAbnd260taBUkr4cBqZ5D6PQUpERFpZkorbBw8UkykvycebjWB6NYPf2Ll7mwu6BCMw+Fg5e5sAB4eHsfDwzsYVa6cgiZzjxPAxIkTGT9+PH379qV///5MnTqVoqIibr/9dgDGjRtHZGQkU6ZMwd3dnW7dutW63t/fH+CY/SJyDpnNENrFuSXcfeIgdaIRqbZDIbwnWAz/K0hEROSUubtaaB/ic8z+Cee1YeXubH7clQWA2QR2B/zzh33c1L81Ib7u1eem5BRjdTUT4uN+zOtI42b4Ty1jx44lKyuLyZMnk56eTs+ePVm8eHF1w4jk5GTM6uQl0ridbpD6/gWw+jpHotoOgTZDILijs3GFiIhIEzO0QzB/viiOlJxiIv09GNM7ksfm/syW5Fzu+mQjXSP9uLFfa8ptNm7810+4WEy8d0sfhnQINrp0OQ2GT9VraJqqJ2KA6iC1Evb/6Hwszat9jndYTYhqO0TrSImISJO26cARrnlvTfVzq4sZb6sLh4vKAXAxm3j35t5c0vXY+/ql4TSZdZyMoOAk0gjYbZC2Ffb9APtWONeUqiytfU6r9lUhaqhzep+nVmsXEZGmZfnOTBIzCliz93D1NL64EG86hfvyv58PEejlxvePDsHfU/cAG0XBqQ4KTiKNUEWpMzztrwpSh7aAw/67E0wQHl/V6W+Ic0FeVw+DihURETk9NruDN75NZENSDq9eG09UgAej3lrJroxCxvSKZFT3cCwWEx1CfYj0179vDUnBqQ4KTiJNQEmu876ofSuco1LZibWPu7hDzCBodyG0uwhCOuv+KBERaVI2JOVw3ftrj9l/z5B2PHFZJwMqapkUnOqg4CTSBOWnVY1GVY1IFRyqfdwnwhmi2l8IbYdpWp+IiDQJU77ZwcyfkokO8MRmd5CYUQDAuzf3ZmT3cIOraxkUnOqg4CTSxDkckJUIe5fBnmXOkala90eZILK3cySq/UUQ2Vdtz0VEpEl4ZfFO3luxFx+rC49c3IFr+0bh6+5qdFnNmoJTHRScRJqZihI4sAb2fu8MUlk7ah+3+kHbC2qClH9rY+oUERE5iUqbnZv+/RPr9+cA4Ofhyr1D2+FldcEEXNM7im2peTw5fxt3n9+W6/tFG1twM6DgVAcFJ5FmLi/VuVbUnmXOx5IjtY+3inMGqHYXQexgcPMypk4REZHjKK2wMW/TQaav3s/erKJax3pG+7M/u4i8kgqCfayseeJCcorKMZtMBPtYDaq4aVNwqoOCk0gLYrfBoa010/oObgCHrea4xc3Z6jzuEufWqp1hpYqIiPyeze5g3qYUPl+fgp+HK5uTj1BQWlnrnCcu68S07/fg7mpm2cSh+HlqWt/pUnCqg4KTSAtWkutcgHfvMtjzPeQl1z4e2BbiRkDcxRAzGFzdDSlTRETkj347lM9dn2zEy2qhd+sAZm1IqXX8zxe2Z+IlHcnIL2V7ah6RAR50CtPPuiej4FQHBScRAZxNJrJ3w+4lsPtbOLAW7BU1x109nQvwxl3sHI3y1zxyERExls3u/LE99UgJF7y2HACL2YTN7sDb6kKfmAB+qFpo193VzLJHh2pdqJNQcKqDgpOIHFdpvrPl+e5vYfd3UJBW+3hwZ2eI6jACohPAoukQIiJinLs/2ci3v2Xw6jU9mLEmid/S8gEwm8DLzYWCskqu7hXJm2N7GltoI6fgVAcFJxE5KYcDMrY7Q9Sub+HgenDYa45bfaHdMOdIVPvh4BNmXK0iItIiFZVVkppbQodQH9bsyWbCxxvoExPAC1d2o7CskiumrcZkgleu7kHHMB/io/2NLrlRUnCqg4KTiJy24hxnu/Pd38Ge76D4cO3j4fHQ4VLnaFR4LzCbjalTRERarEqbHRdLzb8/f/58C1/9XLNgvBbVPT4FpzooOInIWbHb4NCWqil93zq//j3vMOhwCXS4DNoOBTdPQ8oUEZGWLTO/lKe/3M6ezEL2ZhXRIdSbRX8+ny0puQBEBXjg5+GKh6sFk8lkbLEGUnCqg4KTiNSrwkznSNSub2DvcigvrDnm4u5sMNHxUueIlG+EcXWKiEiLlFdSwXmvfE9BaSVdI3z59VB+reNdI3x57+Y+tG7VMn/Rp+BUBwUnETlnKssgaRXsWgyJi49tdx7WAzpe5gxR4T01pU9ERBrEm98m8tb3ewBwtZgI83PnUG5pdZe+Vl5uzLi9P92j/Iws0xAKTnVQcBKRBuFwQOZvkPiNM0gd3Aj87q9b7zDnPVEdL3OOSmlKn4iInCO5xeVc8vcfKS638c9b+zC4fRAOh4NDeaX86dONbE/Np2OoD4sfPr/FTdtTcKqDgpOIGKIwq6pLn6b0iYhIw8stLsfNxYynm0ut/XnFFQx+5XsKyyr56La+XNgptNZxu92B2dx8w5SCUx0UnETEcJVlkLQSdi05/pS+8Hhnc4mOl0JYvKb0iYjIOTVl0Q7++eM+uoT7Eu7nTkFZJQPatmLl7ix2phXwxGWdGDcwplmORik41UHBSUQalZNN6fMJr5rSN9I5KuXqblipIiLSPGXkl3L+K8spt9lPeM7QjsGc1z6IUT3CCffzaMDqzi0FpzooOIlIo1bXlD5XL2h/IXQc5QxTnoHG1SkiIs3Ka0t28v4P+7i2dxRdInxZvz+HTmE+WCwm3vh2V3UjCQ9XC/cNbcd1faMJ86v9y7xKmx2L2dSkRqYUnOqg4CQiTcbRKX2J38DORVBQs5AhJjO0Hugcieo0EgLbGleniIg0eQ6HA7sDLMe5n+m3Q/l8vzODpTsy2Vq1DhRAfLQ/N/aL5qpekTgccN0/13CkqIJ3b+5NfLR/wxV/FhSc6qDgJCJNksMBaVudASpxEWRsr308uLMzQHUcCRG9dV+UiIjUO4fDwZdbD/Hx2iS2puRyNEX0jQmgX5tA3luxFwB3VzPv3dKHYR1DDKz21Cg41UHBSUSahSMHnCNRiQshaTU4bDXHvMOcjSU6joI2F+i+KBERqXfZhWV8sfkgb3+/h4LSyur9HUK92ZVRSJC3lR/+byheVpc6XsV4Ck51UHASkWan5Ajs/s45ErV7KZQX1BzTfVEiInIOrdydxW3TN2CzOxjYthUfT+jP8Dd/IDmnmEcv7sCDF8UZXWKdFJzqoOAkIs3a0fuidi5yjkjVui/K4rwvqtNI58K7ui9KRETqwYItqcxcn8zLV3enbbA3X/18iD9/vgVvqwtPjuxMr9b+dA6v+bnb4XCQU1ROK2+rgVU7KTjVQcFJRFoMhwMObama0lfXfVGjIKKX7osSEZF6Ybc7uOKdVWxPza/e1y82gBv7tybC34O3lu3m4JESlk4cgpuLsf/2KDjVQcFJRFqsI0lVHfoWwoE1x78vqtPlzvuiXIz/LaCIiDRdmQWlfPDjPnamF7B272Eq7bUjh5vFzGd3JdAv1tgp5ApOdVBwEhEBinNgz1JniNqztPZ6UW7e0P6iqvuiLgGPAOPqFBGRJi8jv5TP1yfzv58PceBwMWN6RfLQ8DiiAjyNLk3BqS4KTiIif1BZBvtXOjv0JX4DBWk1x0wWiB3sDFGdRoJ/a+PqFBGRJs3hcOBwgPk4a0UZRcGpDgpOIiJ1sNshbYuzucTOhZC1o/bxsO5VIWqU8+smtDq8iIjIHyk41UHBSUTkNBze62wssXMRpKwDh73mmF9rZ3e+TiMhZjBYXI2rU0RE5AwoONVBwUlE5AwVZcOuJc6RqL3fQ2VJzTF3P4gb4QxR7YeD1ce4OkVERE6RglMdFJxEROpBeTHsW+EMUbu+geLDNccsbtBmiHM6X8fLwCfMsDJFRETqouBUBwUnEZF6ZrdBynrY+bVzWl/OvtrHI/s6Q1SnURDUQfdFiYhIo6HgVAcFJxGRc8jhgKzEmhCVuqn28cB2zul8nS6HqH5gthhTp4iICApOdVJwEhFpQPlpzql8OxfC/h/BVl5zzDOoZtHdtkPB1cOwMkVEpGVScKqDgpOIiEFK82Hvsqr7or6FsryaY66e0O5C53S+uBHg1cq4OkVEpMVQcKqDgpOISCNgq4ADq50hauciyD9Yc8xkhtYDoeNI57S+wLbG1SkiIs2aglMdFJxERBoZhwPSf6kJURnbah8P6VIVokZBRC81lxARkXqj4FQHBScRkUbuyAFI/MbZYOLAGnDYao75RFQtujsKYs8HFzfj6hQRkSZPwakOCk4iIk1IcQ7s/s4ZovYsg4qimmNWX+diu51GQdzFzkV4RUREToOCUx0UnEREmqiKUmdnvp1fO0ekijJrjpldIfa8qkV3R4JfpHF1iohIk6HgVAcFJxGRZsBud64RdXS9qOxdtY+H93S2Oe800nmPlO6LEhGR41BwqoOCk4hIM5S929lcInERpKwHfvdPm39MTYiKHgAWF8PKFBGRxkXBqQ4KTiIizVxhpnMqX+Ii2LscbGU1xzwCocMI55S+dheCm5dxdYqIiOEUnOqg4CQi0oKUFcLe750hatdiKDlSc8zFHdoOdYaoDpeBd7BhZYqIiDFOJxuYG6imOr3zzjvExsbi7u5OQkIC69evP+G5H3zwAeeffz4BAQEEBAQwfPjwOs8XEZEWzOoNXa6AMe/DY3vgtoUw4D7n9L3KUmeY+upBeD0OPrwEVv8DsvcYXbWIiDRCho84zZ49m3HjxvH++++TkJDA1KlTmTt3LomJiYSEhBxz/s0338zgwYMZNGgQ7u7uvPLKK8yfP59ff/2VyMiTd1HSiJOIiOBwQOZvVYvuLoS0rbWPB3WoWnT3cojsA+ZG8XtGERGpZ01qql5CQgL9+vVj2rRpANjtdqKjo3nwwQd54oknTnq9zWYjICCAadOmMW7cuJOer+AkIiLHyDtYtejuQkhaCfbKmmPeodDhUmeIanMBuLobV6eIiNSr08kGhrYWKi8vZ9OmTUyaNKl6n9lsZvjw4axdu/aUXqO4uJiKigoCAwOPe7ysrIyyspobg/Pz88+uaBERaX78oqD/Xc6tNK9q0d2FzsfCDNj8sXNz9YL2FzlDVIdLwCPA6MpFRKSBGBqcsrOzsdlshIaG1tofGhrKzp07T+k1Hn/8cSIiIhg+fPhxj0+ZMoXnnnvurGsVEZEWwt0Pul/r3CrLnSNQOxc6R6QKDsGOr5ybyQIxg2panfu3NrpyERE5h5r0YhYvv/wys2bNYsWKFbi7H3/qxKRJk5g4cWL18/z8fKKjoxuqRBERacpc3JwjTO0vgpGvQ9oW2LnI2aUv8zdnqEpaCYsfh9Duzg59nUZCWA8tuisi0swYGpyCgoKwWCxkZGTU2p+RkUFYWFid177++uu8/PLLLF26lB49epzwPKvVitVqrZd6RUSkBTObnY0iIvvARU9Dzr6aEJW8FjK2ObcfXga/6KrmEiMhZjBYXI2uXkREzpKhbYLc3Nzo06cPy5Ytq95nt9tZtmwZAwcOPOF1r776Ki+88AKLFy+mb9++DVGqiIhIbYFtYdADcPsiZ6vzK991Tttz8YC8FFj/T/jkSnitHfz3Lvh1PpQVGF21iIicIcO76s2ePZvx48fzz3/+k/79+zN16lTmzJnDzp07CQ0NZdy4cURGRjJlyhQAXnnlFSZPnszMmTMZPHhw9et4e3vj7e190vdTVz0RETmnyoth3wpIXAiJi6E4u+aYxc3Zma/jZRA3Avw1dVxExEhNpqsewNixY8nKymLy5Mmkp6fTs2dPFi9eXN0wIjk5GfPv1s947733KC8v59prr631Os888wzPPvtsQ5YuIiJyLDdP5xS9TiPBboOU9c4QtXOhc3rfnqXOjUchpKuzO1+HSyGqH5gtRlcvIiInYPiIU0PTiJOIiBjC4YCsRGeI2vUtHFwPDnvNcY8AaH8xdBgB7S4Ez+MvsyEiIvWnSS2A29AUnEREpFEoznGOPO1aAnu+c64fdZTJAtEJNaNRwZ3UpU9E5BxQcKqDgpOIiDQ6tkrnCNSuJc4ta0ft436tnSNRHUZA7PngevwlOERE5PQoONVBwUlERBq9Iwdg97fOELX/R7CV1Rxz8YC2Q52jUXEjwC/SsDJFRJo6Bac6KDiJiEiTUl7kDE9HR6MKDtU+Htq9ZkpfZB81mBAROQ0KTnVQcBIRkSbL4YCM7TUh6uAG4Hf/jHsEOBtLtL8Y2l8E3iGGlSoi0hQoONVBwUlERJqNosNVDSYWw95ltRtMAITHQ/vhziAV1Q8shq9CIiLSqCg41UHBSUREmiVbJaRudAap3d9B2tbax61+0G5oVZAaDr4RRlQpItKoKDjVQcFJRERahMIs5yjUnqWwZxmU5NQ+HtIV4qpCVPQAcHEzpk4REQMpONVBwUlERFocuw0ObXWuF7VnKRzcSK17o9y8oc2QmiDl39qoSkVEGpSCUx0UnEREpMUrzoG931eNRi2Foqzax4M6QNthzkYTsYPB6mNMnSIi55iCUx0UnERERH7Hbof0X2pCVMp6cNhqjptdIKq/M0S1GwYRvdTyXESaDQWnOig4iYiI1KEkF5JWwt7lzlGpI/trH3f3hzYX1ASpgFgDihQRqR8KTnVQcBIRETkNOfth33JnkNr/w7EtzwPa1ISo2PPBw9+QMkVEzoSCUx0UnERERM6QrdLZ5nzv984gdXA92CtrjpssENnHGaLaXej82uJqWLkiIiej4FQHBScREZF6UlYASatqgtTh3bWPu/lAzCDn1L42F0BoNzCbjalVROQ4FJzqoOAkIiJyjuSmVE3r+x72rYCSI7WPewQ4p/O1ucDZ/jwoDkwmQ0oVEQEFpzopOImIiDQAuw3St8H+H53bgTVQUVT7HO+wqhBVFabUaEJEGpiCUx0UnERERAxgq4BDW5wNJvb/CMk/ga2s9jn+rWtGo2LPB99wY2oVkRZDwakOCk4iIiKNQEWps7nE0RGp1E21G02AcyHeo/dHxZ4PnoHG1CoizZaCUx0UnERERBqhskJIXlczIpX2M/CHH1FCu0HMYGfDiZhB4B1iSKki0nwoONVBwUlERKQJKDkCSatrRqSydhx7Tqu4qhBVFab8oxu+ThFp0hSc6qDgJCIi0gQVZkLSSjiw1tloIvPXY8/xa10zGhUzGFq1U9c+EamTglMdFJxERESageIcSPkJDqx2BqlDW8Fhq32OV0jtEamQLlpHSkRqUXCqg4KTiIhIM1RW6Gw2cWCNczu48diufe7+0HoARPeH6AEQ0QvcPA0pV0QaBwWnOig4iYiItAAVpXBoc82IVPJPx64jZXaB8HiITqjZ1AJdpEVRcKqDgpOIiEgLZKt0dupL+QlS1jmDVGH6sef5t64dpEK7gtnS8PWKSINQcKqDgpOIiIjgcEBuMqSsdwaplJ8g41dw2Guf5+YNUX2dISqqP0T21npSIs2IglMdFJxERETkuErznQvxVoepDVBecOx5ge0gso8zUEX2gbDu4GJt+HpF5KwpONVBwUlEREROid0GmTuqpvf95Gw4kbP32PMsbs7wFNkHIvs6A1VgW7VCF2kCFJzqoOAkIiIiZ6w4B1I3O0emUjc6w1RJzrHnuftXBanfjUx5BTV4uSJSNwWnOig4iYiISL1xOOBIkjNIHdzofEz7+dhW6AC+kRDeEyJ61jx6hzRouSJSm4JTHRScRERE5JyqLIeM7VWjUlWB6vDu45/rE+Fsif77MOUT1oDFirRsCk51UHASERGRBleaD+m/OEejDm2FtK2QvRs4zo9h3mFVQSreGabC48E3QvdMiZwDCk51UHASERGRRqGsANK3/SFM7Tq2JTqARwCEdnNuYVWPwZ3A1b2hqxZpVhSc6qDgJCIiIo1WedGxYSorERy2Y881WSAoripQdXV29gvt5pzqp9EpkVOi4FQHBScRERFpUipKIWun876pjF+dwSpjO5QcOf75nq2cQSq0u/MxuBMEdwSrd8PWLdIEKDjVQcFJREREmjyHA/IPOYNUxjZIrwpVh3cff6ofgF90TYgK6fy7QOXTsLWLNCIKTnVQcBIREZFmq6LEuWhvxnZnmMraAZk7oSjzxNf4RkFIp6og1akmULnr5yRp/hSc6qDgJCIiIi1OcY5zul/WTmeQOvp1YcaJr/GJgKD20Ko9tIpzPga1B7/WYHFpuNpFziEFpzooOImIiIhUKc5xNp/I2lk7WBWmn/gasysEtq0JUr8PVl5BakwhTYqCUx0UnEREREROouQIZO9x3jN1eI9zzanDeyFnL1SWnvg6qx8ExkJAGwiIhcCqx4A24BupkSppdBSc6qDgJCIiInKG7HbIP1gTpI4Gq8N7IDeF4y7oe5TZxdmg4vdhKiC2aosBd7+G+Qwiv3M62UCxX0REREROjdkM/q2dW/uLah+rKIGc/XAkqWqr+jpnP+QeAFt51b79x39tqy/4RTnDlV+Uc/NvXfO1TziYLef4A4qcmIKTiIiIiJw9Vw8I7eLc/shuh4K02mHq9wGr+DCU5UPmb87teEwW53Q//6pg5RsJvhHOBX99qh69QzUdUM4ZfWeJiIiIyLllNoNfpHOLPe/Y4+VFkJcKecmQd9C55aZUfZ0C+algr6w6nlzHG5nAO8Q5OuUTDr7hNV/7hFeFqxDnIsEavZLTpOAkIiIiIsZy84LgDs7teOw2Z+v0vIOQWxWu8lOdo1j5aVCQ7uwEaK90nleYAWlb63hDkzM8eYc4OwF6BYPX778Orn3MzetcfGppYhpFcHrnnXd47bXXSE9PJz4+nrfffpv+/fuf8Py5c+fy9NNPk5SURFxcHK+88gojR45swIpFREREpMGYLc5peb4REH2CnxHtdijOhvxDziBVUPVY/TzNuRXnAA7nucXZp/b+Lh7gEVCzeQbUfu4RAB6Bx+5z9VB79mbE8OA0e/ZsJk6cyPvvv09CQgJTp05lxIgRJCYmEhIScsz5a9as4cYbb2TKlClcfvnlzJw5k6uuuorNmzfTrVs3Az6BiIiIiBjObHaOEnkf+/NjLbZKKMmBwkwoyoKibCg6+nXV88LMmv2VpVBZAgUlzjB2OkwWsPqAu6+z+YXVp2r7w9fuv3vu6lm1eTgf3X733MVdQcxAhrcjT0hIoF+/fkybNg0Au91OdHQ0Dz74IE888cQx548dO5aioiK+/vrr6n0DBgygZ8+evP/++yd9P7UjFxEREZFT4nBAeaFzlKrkiDNwlRxxbsVHar6utb/qa4ftHBRkqglU1eGqKlC5uIHlOFv1fteqR2vV167OYGcyO0f0TKaa59X7zFX7TDX7HA5w2AFHzdfVj0f324+zH+dUyt9vA+53BkMDNZl25OXl5WzatIlJkyZV7zObzQwfPpy1a9ce95q1a9cyceLEWvtGjBjBggULjnt+WVkZZWVl1c/z8/PPvnARERERaf5MppqRoICYU7/O4YCyAmfoKiuA0nxn18Cygt9t+X94rDqvogQqin+3lThbuTtfuGZ/c9B7vOHB6XQYGpyys7Ox2WyEhobW2h8aGsrOnTuPe016evpxz09PTz/u+VOmTOG5556rn4JFRERERE7GZHJOv3Ovp9lNtsqaEFXrsRjKi8FWBrYKqCxzhixbRdXjH7eq/ZVVz6tHhuzOETKHw9mIo9a+qq/tVc9NZsBUMwpV/Xh0v/kP+00155tdnQshm12co1cWt/r582kght/jdK5NmjSp1ghVfn4+0dHRBlYkIiIiInIaLC5gqccgJmfE0OAUFBSExWIhIyOj1v6MjAzCwsKOe01YWNhpnW+1WrFarfVTsIiIiIiItEhmI9/czc2NPn36sGzZsup9drudZcuWMXDgwONeM3DgwFrnA3z33XcnPF9ERERERORsGT5Vb+LEiYwfP56+ffvSv39/pk6dSlFREbfffjsA48aNIzIykilTpgDw0EMPMWTIEN544w1GjRrFrFmz2LhxI//617+M/BgiIiIiItKMGR6cxo4dS1ZWFpMnTyY9PZ2ePXuyePHi6gYQycnJmM01A2ODBg1i5syZ/PWvf+XJJ58kLi6OBQsWaA0nERERERE5Zwxfx6mhaR0nERERERGB08sGht7jJCIiIiIi0hQoOImIiIiIiJyEgpOIiIiIiMhJKDiJiIiIiIichIKTiIiIiIjISSg4iYiIiIiInISCk4iIiIiIyEkoOImIiIiIiJyEgpOIiIiIiMhJKDiJiIiIiIichIKTiIiIiIjISSg4iYiIiIiInISCk4iIiIiIyEm4GF1AQ3M4HADk5+cbXImIiIiIiBjpaCY4mhHq0uKCU0FBAQDR0dEGVyIiIiIiIo1BQUEBfn5+dZ5jcpxKvGpG7HY7hw4dwsfHB5PJZHQ55OfnEx0dTUpKCr6+vkaXIwLo+1IaL31vSmOl701prPS9WTeHw0FBQQERERGYzXXfxdTiRpzMZjNRUVFGl3EMX19ffTNLo6PvS2ms9L0pjZW+N6Wx0vfmiZ1spOkoNYcQERERERE5CQUnERERERGRk1BwMpjVauWZZ57BarUaXYpINX1fyv+3d/8xUdd/HMCfHzru+OUJiNxhjqTJSDJYgtLNtpYwgZhTo7XcraG1MfJw2K9lLn+02mC12bLZ1fqhbTVpuGHmxLrAzmmAeIJciqw/LF14kjEUL/l5r+8fzs++n3BdP+Q+cDwf22e7e7/fnK/39txtr33u83ayYjZpsmI2abJiNu+caXc4BBERERER0T/FO05ERERERERBsHEiIiIiIiIKgo0TERERERFREGyciIiIiIiIgmDjpKNdu3Zh3rx5iIqKQl5eHk6cOKF3SRTmjh49ihUrVmDOnDlQFAX79+/XzIsItm7dipSUFERHR6OgoAA//fSTZk1fXx/sdjvMZjPi4+Px7LPP4vr16yHcBYWb6upqLF68GDNmzEBycjJWrVqF7u5uzZrBwUE4HA7MmjULcXFxKC0txeXLlzVrLly4gJKSEsTExCA5ORkvv/wyRkdHQ7kVCjNOpxNZWVnqfxxqs9nQ0NCgzjOXNFnU1NRAURRs3LhRHWM+7zw2Tjr58ssv8cILL2Dbtm04deoUsrOzUVhYiN7eXr1LozDm9/uRnZ2NXbt23Xb+rbfews6dO/HBBx+gtbUVsbGxKCwsxODgoLrGbrfjzJkzcLlcOHjwII4ePYry8vJQbYHCkNvthsPhQEtLC1wuF0ZGRrB8+XL4/X51zfPPP4+vv/4adXV1cLvd6OnpweOPP67Oj42NoaSkBMPDw/jhhx/w2WefYc+ePdi6daseW6IwMXfuXNTU1MDj8eDkyZNYtmwZVq5ciTNnzgBgLmlyaGtrw4cffoisrCzNOPM5AYR0sWTJEnE4HOr7sbExmTNnjlRXV+tYFU0nAKS+vl59HwgExGq1yttvv62O9ff3i8lkkr1794qIyNmzZwWAtLW1qWsaGhpEURT59ddfQ1Y7hbfe3l4BIG63W0Ru5jAyMlLq6urUNV1dXQJAmpubRUTk0KFDEhERIT6fT13jdDrFbDbL0NBQaDdAYS0hIUE+/vhj5pImhYGBAUlPTxeXyyWPPPKIVFVViQi/NycK7zjpYHh4GB6PBwUFBepYREQECgoK0NzcrGNlNJ2dP38ePp9Pk8uZM2ciLy9PzWVzczPi4+ORm5urrikoKEBERARaW1tDXjOFp6tXrwIAEhMTAQAejwcjIyOabN53331ITU3VZPOBBx6AxWJR1xQWFuLatWvq3QGi/2JsbAy1tbXw+/2w2WzMJU0KDocDJSUlmhwC/N6cKAa9C5iOrly5grGxMU1QAcBiseDcuXM6VUXTnc/nA4Db5vLWnM/nQ3JysmbeYDAgMTFRXUP0XwQCAWzcuBFLly7FwoULAdzMndFoRHx8vGbtn7N5u+zemiP6t7xeL2w2GwYHBxEXF4f6+npkZmaio6ODuSRd1dbW4tSpU2hraxs3x+/NicHGiYiIJg2Hw4Eff/wRx44d07sUIgBARkYGOjo6cPXqVezbtw9lZWVwu916l0XT3MWLF1FVVQWXy4WoqCi9y5k2+FM9HSQlJeGuu+4ad7LJ5cuXYbVadaqKprtb2furXFqt1nEHmIyOjqKvr4/Zpf+ssrISBw8exJEjRzB37lx13Gq1Ynh4GP39/Zr1f87m7bJ7a47o3zIajZg/fz5ycnJQXV2N7OxsvPvuu8wl6crj8aC3txeLFi2CwWCAwWCA2+3Gzp07YTAYYLFYmM8JwMZJB0ajETk5OWhsbFTHAoEAGhsbYbPZdKyMprO0tDRYrVZNLq9du4bW1lY1lzabDf39/fB4POqapqYmBAIB5OXlhbxmCg8igsrKStTX16OpqQlpaWma+ZycHERGRmqy2d3djQsXLmiy6fV6NY29y+WC2WxGZmZmaDZC00IgEMDQ0BBzSbrKz8+H1+tFR0eHeuXm5sJut6uvmc8JoPfpFNNVbW2tmEwm2bNnj5w9e1bKy8slPj5ec7IJ0Z02MDAg7e3t0t7eLgBkx44d0t7eLr/88ouIiNTU1Eh8fLx89dVX0tnZKStXrpS0tDS5ceOG+hlFRUXy4IMPSmtrqxw7dkzS09NlzZo1em2JwsBzzz0nM2fOlO+//14uXbqkXn/88Ye6pqKiQlJTU6WpqUlOnjwpNptNbDabOj86OioLFy6U5cuXS0dHhxw+fFhmz54tr776qh5bojCxadMmcbvdcv78eens7JRNmzaJoijy7bffighzSZPL/5+qJ8J8TgQ2Tjp67733JDU1VYxGoyxZskRaWlr0LonC3JEjRwTAuKusrExEbh5JvmXLFrFYLGIymSQ/P1+6u7s1n/H777/LmjVrJC4uTsxms6xbt04GBgZ02A2Fi9tlEoDs3r1bXXPjxg1Zv369JCQkSExMjKxevVouXbqk+Zyff/5ZiouLJTo6WpKSkuTFF1+UkZGREO+Gwskzzzwj99xzjxiNRpk9e7bk5+erTZMIc0mTy58bJ+bzzlNERPS510VERERERDQ18BknIiIiIiKiINg4ERERERERBcHGiYiIiIiIKAg2TkREREREREGwcSIiIiIiIgqCjRMREREREVEQbJyIiIiIiIiCYONEREREREQUBBsnIiKif0BRFOzfv1/vMoiIKMTYOBER0ZSxdu1aKIoy7ioqKtK7NCIiCnMGvQsgIiL6J4qKirB7927NmMlk0qkaIiKaLnjHiYiIphSTyQSr1aq5EhISANz8GZ3T6URxcTGio6Nx7733Yt++fZq/93q9WLZsGaKjozFr1iyUl5fj+vXrmjWffvop7r//fphMJqSkpKCyslIzf+XKFaxevRoxMTFIT0/HgQMHJnbTRESkOzZOREQUVrZs2YLS0lKcPn0adrsdTz31FLq6ugAAfr8fhYWFSEhIQFtbG+rq6vDdd99pGiOn0wmHw4Hy8nJ4vV4cOHAA8+fP1/wbr7/+Op588kl0dnbiscceg91uR19fX0j3SUREoaWIiOhdBBER0d+xdu1afP7554iKitKMb968GZs3b4aiKKioqIDT6VTnHnroISxatAjvv/8+PvroI7zyyiu4ePEiYmNjAQCHDh3CihUr0NPTA4vFgrvvvhvr1q3Dm2++edsaFEXBa6+9hjfeeAPAzWYsLi4ODQ0NfNaKiCiM8RknIiKaUh599FFNYwQAiYmJ6mubzaaZs9ls6OjoAAB0dXUhOztbbZoAYOnSpQgEAuju7oaiKOjp6UF+fv5f1pCVlaW+jo2NhdlsRm9v77/dEhERTQFsnIiIaEqJjY0d99O5OyU6OvpvrYuMjNS8VxQFgUBgIkoiIqJJgs84ERFRWGlpaRn3fsGCBQCABQsW4PTp0/D7/er88ePHERERgYyMDMyYMQPz5s1DY2NjSGsmIqLJj3eciIhoShkaGoLP59OMGQwGJCUlAQDq6uqQm5uLhx9+GF988QVOnDiBTz75BABgt9uxbds2lJWVYfv27fjtt9+wYcMGPP3007BYLACA7du3o6KiAsnJySguLsbAwACOHz+ODRs2hHajREQ0qbBxIiKiKeXw4cNISUnRjGVkZODcuXMAbp54V1tbi/Xr1yMlJQV79+5FZmYmACAmJgbffPMNqqqqsHjxYsTExKC0tBQ7duxQP6usrAyDg4N455138NJLLyEpKQlPPPFE6DZIRESTEk/VIyKisKEoCurr67Fq1Sq9SyEiojDDZ5yIiIiIiIiCYONEREREREQUBJ9xIiKisMFfnxMR0UThHSciIiIiIqIg2DgREREREREFwcaJiIiIiIgoCDZOREREREREQbBxIiIiIiIiCoKNExERERERURBsnIiIiIiIiIJg40RERERERBTE/wCBoc45Z0JJlgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the MAE\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print('Mean Absolute Error:', mae)\n",
        "\n",
        "# Calculate variance of transformed y_test\n",
        "y_test_transformed_var = np.var(y_test)\n",
        "print('Variance of y_test_transformed:', y_test_transformed_var)\n",
        "\n",
        "# Calculate the RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"rmse: {rmse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzy7Hf65MK4u",
        "outputId": "89f7aa11-d422-44cc-9f0f-cd5481f1c52f"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 9ms/step\n",
            "Mean Absolute Error: 1950.6016750589292\n",
            "Variance of y_test_transformed: 1921730.9282181421\n",
            "rmse: 2118.9025393215984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an index based on the length of y_test\n",
        "index = range(len(y_test))\n",
        "\n",
        "# Plot the actual and predicted prices using the index as the x-axis\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(index, y_test, label='Actual')\n",
        "plt.plot(index, y_pred, label='Predicted')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Actual vs. Predicted Prices')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZUIto_UpKOfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pmdarima\n",
        "\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from pmdarima.arima import auto_arima\n",
        "\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "X = df.drop(\"prices\", axis=1) # drop prices column to create X\n",
        "y = df[\"prices\"] # select prices column to create y\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "stepwise_model = auto_arima(y_train, start_p=1, start_q=1,\n",
        "                           max_p=3, max_q=3, m=12,\n",
        "                           start_P=0, seasonal=True,\n",
        "                           d=1, D=1, trace=True,\n",
        "                           error_action='ignore',  \n",
        "                           suppress_warnings=True, \n",
        "                           stepwise=True)\n",
        "\n",
        "print(stepwise_model.order)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jSuJyBmCHWc",
        "outputId": "c310da5f-cea8-49cf-8288-4082f5e4bc6d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pmdarima\n",
            "  Downloading pmdarima-2.0.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (0.29.34)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (1.5.3)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (67.6.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (1.2.0)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (0.13.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from pmdarima) (1.26.15)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.19->pmdarima) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22->pmdarima) (3.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.9/dist-packages (from statsmodels>=0.13.2->pmdarima) (23.1)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.9/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy>=0.5.2->statsmodels>=0.13.2->pmdarima) (1.16.0)\n",
            "Installing collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.3\n",
            "Performing stepwise search to minimize aic\n",
            " ARIMA(1,1,1)(0,1,1)[12]             : AIC=7548.534, Time=0.53 sec\n",
            " ARIMA(0,1,0)(0,1,0)[12]             : AIC=7885.289, Time=0.04 sec\n",
            " ARIMA(1,1,0)(1,1,0)[12]             : AIC=7674.864, Time=0.22 sec\n",
            " ARIMA(0,1,1)(0,1,1)[12]             : AIC=inf, Time=1.37 sec\n",
            " ARIMA(1,1,1)(0,1,0)[12]             : AIC=7683.612, Time=0.43 sec\n",
            " ARIMA(1,1,1)(1,1,1)[12]             : AIC=7548.275, Time=1.32 sec\n",
            " ARIMA(1,1,1)(1,1,0)[12]             : AIC=7604.800, Time=0.96 sec\n",
            " ARIMA(1,1,1)(2,1,1)[12]             : AIC=7549.380, Time=1.86 sec\n",
            " ARIMA(1,1,1)(1,1,2)[12]             : AIC=7548.206, Time=2.10 sec\n",
            " ARIMA(1,1,1)(0,1,2)[12]             : AIC=7547.809, Time=1.82 sec\n",
            " ARIMA(0,1,1)(0,1,2)[12]             : AIC=7548.577, Time=1.28 sec\n",
            " ARIMA(1,1,0)(0,1,2)[12]             : AIC=7608.185, Time=1.18 sec\n",
            " ARIMA(2,1,1)(0,1,2)[12]             : AIC=7544.185, Time=2.16 sec\n",
            " ARIMA(2,1,1)(0,1,1)[12]             : AIC=7544.523, Time=0.90 sec\n",
            " ARIMA(2,1,1)(1,1,2)[12]             : AIC=7544.632, Time=5.40 sec\n",
            " ARIMA(2,1,1)(1,1,1)[12]             : AIC=7544.587, Time=1.05 sec\n",
            " ARIMA(2,1,0)(0,1,2)[12]             : AIC=7583.896, Time=1.38 sec\n",
            " ARIMA(3,1,1)(0,1,2)[12]             : AIC=7546.160, Time=2.71 sec\n",
            " ARIMA(2,1,2)(0,1,2)[12]             : AIC=7538.309, Time=2.64 sec\n",
            " ARIMA(2,1,2)(0,1,1)[12]             : AIC=inf, Time=3.22 sec\n",
            " ARIMA(2,1,2)(1,1,2)[12]             : AIC=7538.713, Time=3.98 sec\n",
            " ARIMA(2,1,2)(1,1,1)[12]             : AIC=inf, Time=2.78 sec\n",
            " ARIMA(1,1,2)(0,1,2)[12]             : AIC=7537.057, Time=3.21 sec\n",
            " ARIMA(1,1,2)(0,1,1)[12]             : AIC=7537.773, Time=0.61 sec\n",
            " ARIMA(1,1,2)(1,1,2)[12]             : AIC=7537.212, Time=2.50 sec\n",
            " ARIMA(1,1,2)(1,1,1)[12]             : AIC=7537.558, Time=1.96 sec\n",
            " ARIMA(0,1,2)(0,1,2)[12]             : AIC=7544.992, Time=3.25 sec\n",
            " ARIMA(1,1,3)(0,1,2)[12]             : AIC=7536.133, Time=3.20 sec\n",
            " ARIMA(1,1,3)(0,1,1)[12]             : AIC=7536.639, Time=1.22 sec\n",
            " ARIMA(1,1,3)(1,1,2)[12]             : AIC=7536.367, Time=4.42 sec\n",
            " ARIMA(1,1,3)(1,1,1)[12]             : AIC=7536.578, Time=2.03 sec\n",
            " ARIMA(0,1,3)(0,1,2)[12]             : AIC=7542.185, Time=3.52 sec\n",
            " ARIMA(2,1,3)(0,1,2)[12]             : AIC=7534.546, Time=5.15 sec\n",
            " ARIMA(2,1,3)(0,1,1)[12]             : AIC=7534.558, Time=2.01 sec\n",
            " ARIMA(2,1,3)(1,1,2)[12]             : AIC=7534.418, Time=8.48 sec\n",
            " ARIMA(2,1,3)(1,1,1)[12]             : AIC=7535.019, Time=2.66 sec\n",
            " ARIMA(2,1,3)(2,1,2)[12]             : AIC=7538.000, Time=7.17 sec\n",
            " ARIMA(2,1,3)(2,1,1)[12]             : AIC=7535.361, Time=6.84 sec\n",
            " ARIMA(3,1,3)(1,1,2)[12]             : AIC=inf, Time=6.96 sec\n",
            " ARIMA(3,1,2)(1,1,2)[12]             : AIC=7539.913, Time=6.43 sec\n",
            " ARIMA(2,1,3)(1,1,2)[12] intercept   : AIC=7539.245, Time=7.06 sec\n",
            "\n",
            "Best model:  ARIMA(2,1,3)(1,1,2)[12]          \n",
            "Total fit time: 118.073 seconds\n",
            "(2, 1, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit an ARIMA model to the time series data\n",
        "(p,d,q) = (2, 1, 3)\n",
        "model_ARIMA = ARIMA(y_train, order=(p, d, q))\n",
        "model_ARIMA_fit = model.fit()\n",
        "\n"
      ],
      "metadata": {
        "id": "UeIRPJ3-K9nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c66e578-012f-4224-bfdd-9d310469d8fc"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.9/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.9/dist-packages/statsmodels/tsa/base/tsa_model.py:471: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
            "  self._init_dates(dates, freq)\n",
            "/usr/local/lib/python3.9/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        }
      ]
    }
  ]
}